{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fa41b2-e26b-4b52-815c-0775b8ac2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==15.0.2 in /opt/conda/lib/python3.11/site-packages (15.0.2)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /opt/conda/lib/python3.11/site-packages (from pyarrow==15.0.2) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.11/site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from fastparquet) (2025.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pyarrow==15.0.2 fastparquet pandas imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0592bba-41de-496c-8ebe-d369b8409989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Preprocessing finished.\n",
      "Saved to: /home/jovyan/next_state_prediction\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, sys, re, json, subprocess, urllib.parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= 경로 =========\n",
    "INPUT_PARQUET = \"/home/jovyan/processed_user_behavior.sorted.parquet\"\n",
    "OUTPUT_DIR    = \"/home/jovyan/next_state_prediction\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========= 원격 config 확보(없으면 기본) =========\n",
    "CONFIG_URL  = \"https://raw.githubusercontent.com/kakaocloud-edu/tutorial/main/DataAnalyzeCourse/src/day1/Lab01/traffic_generator/config.py\"\n",
    "CONFIG_DIR  = \"/home/jovyan/next_state_prediction\"\n",
    "CONFIG_FILE = os.path.join(CONFIG_DIR, \"config.py\")\n",
    "\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)  # 폴더 없으면 생성\n",
    "\n",
    "def _download_config():\n",
    "    try:\n",
    "        subprocess.run([\"wget\", \"-q\", \"-O\", CONFIG_FILE, CONFIG_URL], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.run([\"curl\", \"-sSL\", \"-o\", CONFIG_FILE, CONFIG_URL], check=True)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "if not os.path.exists(CONFIG_FILE):\n",
    "    _ = _download_config()\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "try:\n",
    "    import config as CFG\n",
    "    _HAS_CFG = True\n",
    "except Exception:\n",
    "    CFG = None\n",
    "    _HAS_CFG = False\n",
    "\n",
    "# ========= Parquet 엔진 =========\n",
    "parquet_engine = None\n",
    "try:\n",
    "    import pyarrow; parquet_engine = \"pyarrow\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import fastparquet; parquet_engine = \"fastparquet\"\n",
    "    except Exception:\n",
    "        parquet_engine = None\n",
    "\n",
    "# ========= 설정값 =========\n",
    "AGE_YOUNG  = getattr(CFG, \"AGE_THRESHOLD_YOUNG\", 25) if _HAS_CFG else 25\n",
    "AGE_MIDDLE = getattr(CFG, \"AGE_THRESHOLD_MIDDLE\", 50) if _HAS_CFG else 50\n",
    "CATEGORY_PREFERENCE = getattr(CFG, \"CATEGORY_PREFERENCE\", {\n",
    "    \"F\": {\"young\": [\"Fashion\", \"Books\"], \"middle\": [\"Fashion\", \"Home\", \"Books\"], \"old\": [\"Home\", \"Books\"]},\n",
    "    \"M\": {\"young\": [\"Electronics\", \"Gaming\"], \"middle\": [\"Electronics\", \"Home\", \"Gaming\"], \"old\": [\"Books\"]},\n",
    "})\n",
    "BIAS_THRESHOLDS = getattr(CFG, \"BIAS_THRESHOLDS\", {\n",
    "    \"cart_has_items_min\": 1, \"heavy_cart_min\": 3, \"deep_page_min\": 6,\n",
    "    \"long_session_sec\": 60, \"idle_slow_sec\": 5,\n",
    "})\n",
    "CATEGORY_KEYWORDS = getattr(CFG, \"CATEGORY_KEYWORDS\", {\n",
    "    \"Electronics\": [\"mouse\", \"earbuds\", \"speaker\", \"laptop\", \"phone\"],\n",
    "    \"Fashion\": [\"sneakers\", \"dress\", \"bag\"],\n",
    "    \"Home\": [\"mug\", \"fryer\", \"coffee\"],\n",
    "    \"Gaming\": [\"keyboard\", \"console\", \"headset\"],\n",
    "    \"Books\": [\"book\", \"novel\"],\n",
    "})\n",
    "CANONICAL_KEYWORD_MAP = getattr(CFG, \"CANONICAL_KEYWORD_MAP\", {\n",
    "    \"blu tooth\": \"bluetooth\",\n",
    "    \"blu%20tooth\": \"bluetooth\",\n",
    "    \"cofee\": \"coffee\",\n",
    "    \"iphon\": \"iphone\",\n",
    "    \"labtop\": \"laptop\",\n",
    "    \"rayban\": \"sunglasses\",\n",
    "})\n",
    "\n",
    "CATEGORY_KEYWORDS_LC = {k.strip().lower(): set(w.strip().lower() for w in v) for k, v in CATEGORY_KEYWORDS.items()}\n",
    "GLOBAL_ALLOWED = set().union(*CATEGORY_KEYWORDS_LC.values()) if CATEGORY_KEYWORDS_LC else set()\n",
    "\n",
    "# ========= 유틸 =========\n",
    "def norm_keyword(x: str) -> str:\n",
    "    if not isinstance(x, str): return \"\"\n",
    "    x = urllib.parse.unquote(x).strip().lower()\n",
    "    x = \" \".join(x.split())\n",
    "    return CANONICAL_KEYWORD_MAP.get(x, x)\n",
    "\n",
    "def norm_category(x: str) -> str:\n",
    "    if not isinstance(x, str): return \"\"\n",
    "    return x.strip()\n",
    "\n",
    "def norm_gender(x: str) -> str:\n",
    "    if pd.isna(x) or not isinstance(x, str) or x.strip()==\"\":\n",
    "        return \"Unknown\"\n",
    "    xl = x.strip().lower()\n",
    "    if xl in {\"m\",\"male\",\"man\"}: return \"M\"\n",
    "    if xl in {\"f\",\"female\",\"woman\"}: return \"F\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def age_to_bucket(age: float) -> str:\n",
    "    if pd.isna(age): return \"unknown\"\n",
    "    try: a = float(age)\n",
    "    except Exception: return \"unknown\"\n",
    "    if a <= AGE_YOUNG: return \"young\"\n",
    "    if a <= AGE_MIDDLE: return \"middle\"\n",
    "    return \"old\"\n",
    "\n",
    "def filter_keyword_by_category(kw: str, cat: str) -> str:\n",
    "    if not kw: return \"\"\n",
    "    cat_key = (cat or \"\").strip().lower()\n",
    "    allowed = CATEGORY_KEYWORDS_LC.get(cat_key, GLOBAL_ALLOWED)\n",
    "    return kw if kw in allowed else \"\"\n",
    "\n",
    "def is_preferred_cat(g: str, age_b: str, cat: str) -> int:\n",
    "    g = g if g in (\"M\",\"F\") else None\n",
    "    if not g or age_b not in (\"young\",\"middle\",\"old\"): return 0\n",
    "    return int(cat in CATEGORY_PREFERENCE.get(g, {}).get(age_b, []))\n",
    "\n",
    "# ========= 로드/정렬 =========\n",
    "read_engine = \"pyarrow\" if parquet_engine==\"pyarrow\" else None\n",
    "df = pd.read_parquet(INPUT_PARQUET, engine=read_engine)\n",
    "\n",
    "required = [\"session_id\",\"timestamp\",\"current_state\",\"next_state\",\n",
    "            \"search_count\",\"cart_item_count\",\"page_depth\",\"product_id\",\"search_keyword\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values([\"session_id\",\"timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "for c in [\"search_count\",\"cart_item_count\",\"page_depth\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(\"int32\")\n",
    "\n",
    "cat_col = \"category_keyword\" if \"category_keyword\" in df.columns else (\"category_name\" if \"category_name\" in df.columns else None)\n",
    "\n",
    "# ========= 성별/연령 파생 =========\n",
    "gender_std = (df[\"gender\"].map(norm_gender).fillna(\"Unknown\") if \"gender\" in df.columns\n",
    "              else pd.Series([\"Unknown\"]*len(df), index=df.index))\n",
    "age_bucket = (df[\"age\"].map(age_to_bucket) if \"age\" in df.columns\n",
    "              else pd.Series([\"unknown\"]*len(df), index=df.index))\n",
    "\n",
    "# ========= 키워드/카테고리 정규화 + 허용 필터 =========\n",
    "df[\"search_keyword_norm\"] = df[\"search_keyword\"].fillna(\"\").map(norm_keyword)\n",
    "if cat_col:\n",
    "    cat_norm = df[cat_col].fillna(\"\").map(norm_category)\n",
    "    df[\"category_norm\"] = cat_norm.replace(\"\", np.nan)\n",
    "else:\n",
    "    df[\"category_norm\"] = np.nan\n",
    "\n",
    "df[\"search_keyword_norm\"] = [\n",
    "    filter_keyword_by_category(kw, (cat if isinstance(cat,str) else \"\"))\n",
    "    for kw, cat in zip(df[\"search_keyword_norm\"], df[\"category_norm\"].fillna(\"\"))\n",
    "]\n",
    "_kw_series = df[\"search_keyword_norm\"].replace(\"\", np.nan)\n",
    "\n",
    "# ========= 플래그 =========\n",
    "bt = BIAS_THRESHOLDS\n",
    "flag_is_pref_category = pd.Series(\n",
    "    [is_preferred_cat(g,a,(c if isinstance(c,str) else \"\")) for g,a,c in\n",
    "     zip(gender_std.tolist(), age_bucket.tolist(), df[\"category_norm\"].fillna(\"\").tolist())],\n",
    "    name=\"flag_is_pref_category\", dtype=\"int8\"\n",
    ")\n",
    "flag_has_cart_items = (df[\"cart_item_count\"]>=bt.get(\"cart_has_items_min\",1)).astype(\"int8\").rename(\"flag_has_cart_items\")\n",
    "flag_heavy_cart     = (df[\"cart_item_count\"]>=bt.get(\"heavy_cart_min\",3)).astype(\"int8\").rename(\"flag_heavy_cart\")\n",
    "flag_deep_page      = (df[\"page_depth\"]     >=bt.get(\"deep_page_min\",6)).astype(\"int8\").rename(\"flag_deep_page\")\n",
    "\n",
    "last_elapsed = pd.to_numeric(df.get(\"last_action_elapsed\",0), errors=\"coerce\").fillna(0.0)\n",
    "session_dur  = pd.to_numeric(df.get(\"session_duration\",0),  errors=\"coerce\").fillna(0.0)\n",
    "flag_long_session = (session_dur>=bt.get(\"long_session_sec\",60)).astype(\"int8\").rename(\"flag_long_session\")\n",
    "flag_idle_slow    = (last_elapsed>=bt.get(\"idle_slow_sec\",5)).astype(\"int8\").rename(\"flag_idle_slow\")\n",
    "\n",
    "goal_tokens = [\"/cart/add\",\"/cart/view\",\"/checkout\"]\n",
    "pattern = \"|\".join(map(re.escape, goal_tokens))\n",
    "has_goal_event_now = (\n",
    "    df[\"current_state\"].astype(str).str.contains(pattern) |\n",
    "    (df[\"cart_item_count\"] >= bt.get(\"cart_has_items_min\",1))\n",
    ").astype(\"int8\")\n",
    "shifted = has_goal_event_now.groupby(df[\"session_id\"]).shift(1).fillna(0).astype(\"int8\")\n",
    "flag_with_goal = shifted.groupby(df[\"session_id\"]).cummax().astype(\"int8\").rename(\"flag_with_goal\")\n",
    "flag_no_goal   = (1 - flag_with_goal).astype(\"int8\").rename(\"flag_no_goal\")\n",
    "\n",
    "# ========= 추가: prev1/prev2 + 누적치 =========\n",
    "prev1_state = df.groupby(\"session_id\")[\"current_state\"].shift(1).fillna(\"\")\n",
    "prev2_state = df.groupby(\"session_id\")[\"current_state\"].shift(2).fillna(\"\")\n",
    "prev1_state_ohe = pd.get_dummies(prev1_state, prefix=\"prev1_state\", dtype=\"int8\")\n",
    "prev2_state_ohe = pd.get_dummies(prev2_state, prefix=\"prev2_state\", dtype=\"int8\")\n",
    "\n",
    "# 순서 결합 피처\n",
    "prevpair = (prev1_state.astype(str) + \"→\" + prev2_state.astype(str))\n",
    "prevpair_ohe = pd.get_dummies(prevpair, prefix=\"prevpair_state\", dtype=\"int8\")\n",
    "\n",
    "# 누적(shift 후 cumsum)\n",
    "prev_search = df.groupby(\"session_id\")[\"search_count\"].shift(1).fillna(0).astype(\"int32\")\n",
    "cum_search  = prev_search.groupby(df[\"session_id\"]).cumsum().astype(\"int32\").rename(\"cum_search\")\n",
    "prev_page   = df.groupby(\"session_id\")[\"page_depth\"].shift(1).fillna(0).astype(\"int32\")\n",
    "cum_page    = prev_page.groupby(df[\"session_id\"]).cumsum().astype(\"int32\").rename(\"cum_page\")\n",
    "has_cart_now     = (df[\"cart_item_count\"]>=1).astype(\"int8\")\n",
    "prev_has_cart    = has_cart_now.groupby(df[\"session_id\"]).shift(1).fillna(0).astype(\"int8\")\n",
    "ever_cart_flag   = prev_has_cart.groupby(df[\"session_id\"]).cummax().astype(\"int8\").rename(\"ever_cart_flag\")\n",
    "\n",
    "# ========= OHE =========\n",
    "state_ohe  = pd.get_dummies(df[\"current_state\"].fillna(\"\"), prefix=\"state\", dtype=\"int8\")\n",
    "search_ohe = pd.get_dummies(_kw_series, prefix=\"keyword\", dtype=\"int8\")\n",
    "cat_ohe    = pd.get_dummies(df[\"category_norm\"], prefix=\"category\", dtype=\"int8\") if (\"category_norm\" in df) else pd.DataFrame(index=df.index)\n",
    "gender_ohe = pd.get_dummies(gender_std, prefix=\"gender\", dtype=\"int8\")\n",
    "age_ohe    = pd.get_dummies(age_bucket, prefix=\"age\", dtype=\"int8\")\n",
    "has_product = df[\"product_id\"].notna().astype(\"int8\").rename(\"has_product_id\")\n",
    "counters    = df[[\"search_count\",\"cart_item_count\",\"page_depth\"]]\n",
    "\n",
    "# ========= 결합 =========\n",
    "X_parts = [\n",
    "    state_ohe, has_product, counters, search_ohe, cat_ohe, gender_ohe, age_ohe,\n",
    "    flag_is_pref_category, flag_has_cart_items, flag_heavy_cart, flag_deep_page,\n",
    "    flag_long_session, flag_idle_slow, flag_with_goal, flag_no_goal,\n",
    "    prev1_state_ohe, prev2_state_ohe, prevpair_ohe,\n",
    "    cum_search, cum_page, ever_cart_flag,\n",
    "]\n",
    "X = pd.concat(X_parts, axis=1)\n",
    "\n",
    "# 중복 제거/형 변환\n",
    "if not X.columns.is_unique:\n",
    "    X = X.loc[:, ~X.columns.duplicated(keep=\"first\")]\n",
    "bool_cols = X.select_dtypes(include=[\"bool\"]).columns\n",
    "if len(bool_cols)>0:\n",
    "    X[bool_cols] = X[bool_cols].astype(\"int8\")\n",
    "\n",
    "y = pd.DataFrame({\"next_state\": df[\"next_state\"].astype(str).fillna(\"\")})\n",
    "\n",
    "# ========= 전이가능 맵 저장 =========\n",
    "trans_map = (df[[\"current_state\",\"next_state\"]]\n",
    "             .dropna()\n",
    "             .astype(str)\n",
    "             .groupby(\"current_state\")[\"next_state\"]\n",
    "             .agg(lambda s: sorted(set(s.tolist())))\n",
    "             .to_dict())\n",
    "with open(os.path.join(OUTPUT_DIR, \"transition_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trans_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ========= 저장 =========\n",
    "features_csv = os.path.join(OUTPUT_DIR, \"next_state_features.csv\")\n",
    "labels_csv   = os.path.join(OUTPUT_DIR, \"next_state.csv\")\n",
    "X.to_csv(features_csv, index=False)\n",
    "y.to_csv(labels_csv,   index=False)\n",
    "\n",
    "features_parquet = os.path.join(OUTPUT_DIR, \"next_state_features.parquet\")\n",
    "labels_parquet   = os.path.join(OUTPUT_DIR, \"next_state.parquet\")\n",
    "\n",
    "def _try_save_parquet(df_obj, path, engine):\n",
    "    df_obj.to_parquet(path, index=False, engine=engine)\n",
    "\n",
    "features_parquet_saved = False; labels_parquet_saved = False\n",
    "err_feat = err_label = None\n",
    "if parquet_engine:\n",
    "    try:\n",
    "        _try_save_parquet(X, features_parquet, parquet_engine); features_parquet_saved = True\n",
    "    except Exception as e:\n",
    "        err_feat = f\"[{parquet_engine}] {type(e).__name__}: {e}\"\n",
    "    try:\n",
    "        _try_save_parquet(y, labels_parquet, parquet_engine); labels_parquet_saved = True\n",
    "    except Exception as e:\n",
    "        err_label = f\"[{parquet_engine}] {type(e).__name__}: {e}\"\n",
    "\n",
    "if not features_parquet_saved:\n",
    "    alt = \"fastparquet\" if parquet_engine==\"pyarrow\" else \"pyarrow\"\n",
    "    try:\n",
    "        __import__(alt)\n",
    "        _try_save_parquet(X, features_parquet, alt); features_parquet_saved = True\n",
    "        print(f\"[INFO] features를 {alt}로 폴백 저장 성공\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(\"[Done] Preprocessing finished.\")\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4dcdffe-39f9-4712-b80d-dc346fc649e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Drop rare OHE columns: n=52 (min_count=10)\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.94595\n",
      "[200]\tvalid_0's multi_logloss: 0.785777\n",
      "[300]\tvalid_0's multi_logloss: 0.698777\n",
      "[400]\tvalid_0's multi_logloss: 0.649222\n",
      "[500]\tvalid_0's multi_logloss: 0.617959\n",
      "[600]\tvalid_0's multi_logloss: 0.59887\n",
      "[700]\tvalid_0's multi_logloss: 0.585899\n",
      "[800]\tvalid_0's multi_logloss: 0.576227\n",
      "[900]\tvalid_0's multi_logloss: 0.569838\n",
      "[1000]\tvalid_0's multi_logloss: 0.565749\n",
      "[1100]\tvalid_0's multi_logloss: 0.562047\n",
      "[1200]\tvalid_0's multi_logloss: 0.558827\n",
      "[1300]\tvalid_0's multi_logloss: 0.556594\n",
      "[1400]\tvalid_0's multi_logloss: 0.554719\n",
      "[1500]\tvalid_0's multi_logloss: 0.553609\n",
      "[1600]\tvalid_0's multi_logloss: 0.553223\n",
      "[1700]\tvalid_0's multi_logloss: 0.553055\n",
      "[1800]\tvalid_0's multi_logloss: 0.552367\n",
      "[1900]\tvalid_0's multi_logloss: 0.551988\n",
      "[2000]\tvalid_0's multi_logloss: 0.551769\n",
      "[2100]\tvalid_0's multi_logloss: 0.551657\n",
      "[2200]\tvalid_0's multi_logloss: 0.551564\n",
      "[2300]\tvalid_0's multi_logloss: 0.551557\n",
      "[2400]\tvalid_0's multi_logloss: 0.551558\n",
      "Early stopping, best iteration is:\n",
      "[2265]\tvalid_0's multi_logloss: 0.551555\n",
      "best_iter: 2265\n",
      "{'Top-1': 0.7924, 'Top-3': 0.9269, 'MacroF1': 0.7812}\n",
      "{'Top-1(masked)': 0.7924, 'Top-3(masked)': 0.9271, 'picked@thr_contains_true': 0.9019, 'avg_picks_per_row': 1.531}\n",
      "Saved model artifacts to: /home/jovyan/next_state_prediction/next_state_prediction_model\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, top_k_accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "# 콜백(구버전 호환용): 없으면 AttributeError가 날 수 있어 try/except 처리\n",
    "try:\n",
    "    from lightgbm import early_stopping, log_evaluation\n",
    "    HAS_LGBM_CALLBACKS = True\n",
    "except Exception:\n",
    "    HAS_LGBM_CALLBACKS = False\n",
    "import joblib\n",
    "\n",
    "# ===== 경로/설정 =====\n",
    "BASE_DIR = \"/home/jovyan/datasets/next_state_prediction\"\n",
    "FEAT = os.path.join(BASE_DIR, \"next_state_features.parquet\")\n",
    "LAB  = os.path.join(BASE_DIR, \"next_state.parquet\")\n",
    "TRANSITION = os.path.join(BASE_DIR, \"transition_map.json\")\n",
    "\n",
    "USE_OVERSAMPLING = True          # imblearn 없이 자체 오버샘플링\n",
    "CLASS_WEIGHT_MODE = None         # oversampling 켜면 None 권장\n",
    "RARE_MIN_COUNT = 10              # 희소 OHE 컬럼 드롭 임계치\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "MASK_TOPK = 3\n",
    "MASK_THRESH = 0.15\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "# ===== 유틸: 희소 OHE 열 드롭 =====\n",
    "def drop_rare_ohe(X, min_count=10):\n",
    "    bin_prefixes = (\"prevpair_state_\", \"prev1_state_\", \"prev2_state_\", \"keyword_\", \"category_\")\n",
    "    bin_cols = [c for c in X.columns if c.startswith(bin_prefixes)]\n",
    "    keep, drop = [], []\n",
    "    for c in bin_cols:\n",
    "        cnt = int(X[c].sum())\n",
    "        (keep if cnt >= min_count else drop).append(c)\n",
    "    if drop:\n",
    "        print(f\"[INFO] Drop rare OHE columns: n={len(drop)} (min_count={min_count})\")\n",
    "        X = X.drop(columns=drop)\n",
    "    return X, drop\n",
    "\n",
    "# ===== 유틸: 내장 오버샘플링 =====\n",
    "def simple_random_oversample(X: pd.DataFrame, y: np.ndarray, random_state=42, max_target=None):\n",
    "    \"\"\"\n",
    "    각 클래스 샘플 수를 max_count(또는 max_target)까지 무작위 복제해 균형화.\n",
    "    \"\"\"\n",
    "    rs = np.random.default_rng(random_state)\n",
    "    y = np.asarray(y)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = int(counts.max()) if max_target is None else int(max_target)\n",
    "\n",
    "    parts_X, parts_y = [], []\n",
    "    for cls, cnt in zip(classes, counts):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        if cnt < max_count:\n",
    "            add_n = max_count - cnt\n",
    "            add_idx = rs.choice(idx, size=add_n, replace=True)\n",
    "            take_idx = np.concatenate([idx, add_idx])\n",
    "        else:\n",
    "            take_idx = idx\n",
    "        parts_X.append(X.iloc[take_idx])\n",
    "        parts_y.append(y[take_idx])\n",
    "\n",
    "    X_os = pd.concat(parts_X, axis=0).reset_index(drop=True)\n",
    "    y_os = np.concatenate(parts_y, axis=0)\n",
    "    # 셔플\n",
    "    perm = rs.permutation(len(y_os))\n",
    "    return X_os.iloc[perm].reset_index(drop=True), y_os[perm]\n",
    "\n",
    "# ===== 데이터 로드 =====\n",
    "X = pd.read_parquet(FEAT)\n",
    "y_raw = pd.read_parquet(LAB)[\"next_state\"].astype(str)\n",
    "\n",
    "# 희소 OHE 드롭\n",
    "X, dropped_cols = drop_rare_ohe(X, RARE_MIN_COUNT)\n",
    "\n",
    "# 라벨 인코딩\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "classes = le.classes_\n",
    "num_class = len(classes)\n",
    "\n",
    "# 오버샘플링 or class_weight\n",
    "if USE_OVERSAMPLING:\n",
    "    X_res, y_res = simple_random_oversample(X, y, random_state=RANDOM_STATE, max_target=None)\n",
    "    class_weight = None\n",
    "else:\n",
    "    X_res, y_res = X, y\n",
    "    if CLASS_WEIGHT_MODE == \"balanced\":\n",
    "        class_weight = \"balanced\"\n",
    "    elif CLASS_WEIGHT_MODE == \"custom\":\n",
    "        vals, cnts = np.unique(y_res, return_counts=True)\n",
    "        inv = {int(v): float(1.0/c) for v, c in zip(vals, cnts)}\n",
    "        s = sum(inv.values()); inv = {k: v*s/len(inv) for k, v in inv.items()}\n",
    "        class_weight = inv\n",
    "    else:\n",
    "        class_weight = None\n",
    "\n",
    "# split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_res, y_res, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_res\n",
    ")\n",
    "\n",
    "# ===== 모델 정의 =====\n",
    "clf = LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=4000,        # 콜백으로 조기종료\n",
    "    num_leaves=31,\n",
    "    max_depth=10,\n",
    "    min_data_in_leaf=50,\n",
    "    min_gain_to_split=1e-3,\n",
    "    subsample=0.8,            # bagging_fraction\n",
    "    colsample_bytree=0.8,     # feature_fraction\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 일부 버전은 fit(verbose=..) 미지원 → 파라미터로 끄고 콜백 로그 사용\n",
    "try:\n",
    "    clf.set_params(verbose=-1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ===== 학습 (콜백 기반 ES/로그) =====\n",
    "fit_kwargs = dict(\n",
    "    X=X_tr, y=y_tr,\n",
    "    eval_set=[(X_te, y_te)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    ")\n",
    "if HAS_LGBM_CALLBACKS:\n",
    "    fit_kwargs[\"callbacks\"] = [\n",
    "        early_stopping(stopping_rounds=200, verbose=True),\n",
    "        log_evaluation(period=100),\n",
    "    ]\n",
    "\n",
    "clf.fit(**fit_kwargs)\n",
    "\n",
    "# best_iteration 안전 획득\n",
    "best_iter = getattr(clf, \"best_iteration_\", None)\n",
    "print(\"best_iter:\", best_iter)\n",
    "\n",
    "# ===== 평가 (num_iteration 호환) =====\n",
    "def predict_with_iter(model, X, *, raw_score=False):\n",
    "    kw = {}\n",
    "    if best_iter is not None:\n",
    "        kw[\"num_iteration\"] = best_iter\n",
    "    return model.predict(X, raw_score=raw_score, **kw)\n",
    "\n",
    "def predict_proba_with_iter(model, X):\n",
    "    kw = {}\n",
    "    if best_iter is not None:\n",
    "        kw[\"num_iteration\"] = best_iter\n",
    "    return model.predict_proba(X, **kw)\n",
    "\n",
    "proba = predict_proba_with_iter(clf, X_te)\n",
    "y_pred = predict_with_iter(clf, X_te)\n",
    "\n",
    "top1 = top_k_accuracy_score(y_te, proba, k=1, labels=list(range(num_class)))\n",
    "top3 = top_k_accuracy_score(y_te, proba, k=min(3, num_class), labels=list(range(num_class)))\n",
    "macro_f1 = f1_score(y_te, y_pred, average=\"macro\")\n",
    "print({\"Top-1\": round(top1,4), \"Top-3\": round(top3,4), \"MacroF1\": round(macro_f1,4)})\n",
    "\n",
    "# ===== 전이 마스킹 평가 =====\n",
    "with open(TRANSITION, \"r\", encoding=\"utf-8\") as f:\n",
    "    trans_map = json.load(f)\n",
    "\n",
    "# state_*은 “드롭 이후” 사용 컬럼에서 추출\n",
    "state_cols = [c for c in X_tr.columns if c.startswith(\"state_\")]\n",
    "\n",
    "def states_from_ohe(Xpart):\n",
    "    idx = Xpart[state_cols].values.argmax(axis=1)\n",
    "    return [state_cols[i].replace(\"state_\",\"\") for i in idx]\n",
    "\n",
    "curr_states_te = states_from_ohe(X_te)\n",
    "\n",
    "def masked_topk_metrics(model, X_eval, y_eval, curr_states, trans_map, classes, K=3, thr=0.15):\n",
    "    logits = predict_with_iter(model, X_eval, raw_score=True)\n",
    "    logits = np.asarray(logits, dtype=np.float64)\n",
    "\n",
    "    # 불가능 전이 -inf 마스킹\n",
    "    for i, cur in enumerate(curr_states):\n",
    "        allowed = set(trans_map.get(cur, []))\n",
    "        if allowed:\n",
    "            disallowed = [j for j, lab in enumerate(classes) if lab not in allowed]\n",
    "            logits[i, disallowed] = -np.inf\n",
    "\n",
    "    # softmax\n",
    "    logits -= np.max(logits, axis=1, keepdims=True)\n",
    "    proba = np.exp(logits); proba /= proba.sum(axis=1, keepdims=True)\n",
    "\n",
    "    topk_idx = np.argsort(-proba, axis=1)[:, :min(K, proba.shape[1])]\n",
    "    y_pred_top1 = topk_idx[:, 0]\n",
    "    acc1 = (y_pred_top1 == y_eval).mean()\n",
    "    in_topk = (topk_idx == y_eval.reshape(-1,1)).any(axis=1).mean()\n",
    "\n",
    "    picked_ok = 0; picked_total = 0\n",
    "    for i in range(proba.shape[0]):\n",
    "        picks = [(j, proba[i, j]) for j in topk_idx[i] if proba[i, j] >= thr]\n",
    "        picked_total += len(picks)\n",
    "        picked_ok += any(j == y_eval[i] for j,_ in picks)\n",
    "\n",
    "    return {\n",
    "        \"Top-1(masked)\": round(float(acc1),4),\n",
    "        f\"Top-{K}(masked)\": round(float(in_topk),4),\n",
    "        \"picked@thr_contains_true\": round(float(picked_ok/len(y_eval)),4),\n",
    "        \"avg_picks_per_row\": round(float(picked_total/len(y_eval)),3),\n",
    "    }\n",
    "\n",
    "masked_scores = masked_topk_metrics(\n",
    "    clf, X_te, y_te, curr_states_te, trans_map, classes=classes, K=MASK_TOPK, thr=MASK_THRESH\n",
    ")\n",
    "print(masked_scores)\n",
    "\n",
    "# ===== 아티팩트 저장 =====\n",
    "ART_DIR = os.path.join(BASE_DIR, \"next_state_prediction_model\")\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "# 모델/라벨인코더 저장 (확장자 .joblib)\n",
    "joblib.dump(clf, os.path.join(ART_DIR, \"lgbm_next_state.joblib\"))\n",
    "joblib.dump(le,  os.path.join(ART_DIR, \"label_encoder.joblib\"))\n",
    "\n",
    "# 학습 스키마(열 순서) 저장 -> 추론 정렬용\n",
    "with open(os.path.join(ART_DIR, \"feature_columns.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list(X_tr.columns), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(ART_DIR, \"dropped_rare_columns.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dropped_cols, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved model artifacts to:\", ART_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea154b7-d1da-406e-ab5b-b6a524f47f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
