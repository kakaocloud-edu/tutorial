{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b0e95b6-4a74-4f07-8b7b-0e19101cca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1. 공통 설정 & 임포트 (전처리+학습 통합) ===\n",
    "import os, json, warnings, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- 표시 옵션 ----------\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# ---------- 경로/상수 ----------\n",
    "DATASETS_DIR  = Path(\"datasets\") / \"gender\"                 # ✅ ./datasets/gender\n",
    "INPUT_PATH    = \"datasets/processed_user_behavior.joined.csv\"\n",
    "OUTPUT_DIR    = DATASETS_DIR                                # ✅ 산출물 저장 경로\n",
    "ARTIFACT_DIR  = Path(\"models\") / \"gender\"                   # ./models/gender\n",
    "\n",
    "# 폴더 생성(이미 있어도 OK)\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 프리로그인 이벤트가 2개 이상인 세션만 사용\n",
    "MIN_PRELOGIN_EVENTS = 2\n",
    "\n",
    "# 카테고리 5개만 사용 (OTHER 제거). 필요 시 실제 5개로 교체\n",
    "KNOWN_CATS = [\"Books\", \"Electronics\", \"Gaming\", \"Home\", \"Fashion\"]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "PRODUCTION_THRESHOLD = 0.444   # 운영 임계값 (고정)\n",
    "\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"공백 제거 + 유니코드 NFKC 정규화(NA 안전)\"\"\"\n",
    "    s = s.astype(\"string\").str.strip()\n",
    "    return s.apply(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f68f00e3-e876-4d3f-9a78-ce7a364d2e1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/processed_user_behavior.joined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Cell 2. 원본 로드 & 정렬 & 프리로그인 구간 추출 ===\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# timestamp 파싱 & 정렬\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/processed_user_behavior.joined.csv'"
     ]
    }
   ],
   "source": [
    "# === Cell 2. 원본 로드 & 정렬 & 프리로그인 구간 추출 ===\n",
    "df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
    "\n",
    "# timestamp 파싱 & 정렬\n",
    "df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"session_id\", \"ts\"]).copy()\n",
    "\n",
    "sort_cols = [\"session_id\", \"ts\"]\n",
    "if \"event_id\" in df.columns:\n",
    "    sort_cols.append(\"event_id\")  # 있으면 정렬 안정성 ↑\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"원본 행 수:\", len(df))\n",
    "\n",
    "# 로그인 여부 판단\n",
    "uid_str  = df[\"user_id\"].astype(\"string\").str.strip()\n",
    "anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "\n",
    "# 세션 내에서 한 번이라도 user_id가 등장하면 그 시점 이후는 로그인 이후\n",
    "appeared = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "pre = df.loc[~appeared].copy()\n",
    "\n",
    "# 프리로그인 이벤트가 2개 이상인 세션만 유지\n",
    "prelogin_counts = pre.groupby(\"session_id\").size()\n",
    "valid_sessions = prelogin_counts.index[prelogin_counts >= MIN_PRELOGIN_EVENTS]\n",
    "pre = pre[pre[\"session_id\"].isin(valid_sessions)].copy()\n",
    "\n",
    "print(\"프리로그인 필터 후 행 수:\", len(pre))\n",
    "print(\"유지된 프리로그인 세션 수:\", pre[\"session_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08604857-d072-4d74-a70e-e515a0ad6a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교집합 세션 수: 649\n",
      "라벨 분포:\n",
      " gender\n",
      "F    344\n",
      "M    305\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3. 라벨(M/F) 생성(세션 전체) + 프리로그인 수치 집계 ===\n",
    "# 세션 전체에서 첫 유효 gender(M/F)\n",
    "df_gender = df.copy()\n",
    "df_gender[\"gender_norm\"] = (\n",
    "    df_gender[\"gender\"]\n",
    "      .astype(\"string\").str.strip().str.upper()\n",
    "      .replace({\"FEMALE\":\"F\", \"MALE\":\"M\"})\n",
    ")\n",
    "lab_full = (\n",
    "    df_gender[df_gender[\"gender_norm\"].isin([\"M\",\"F\"])]\n",
    "      .groupby(\"session_id\")[\"gender_norm\"].agg(lambda s: s.iloc[0])\n",
    ")\n",
    "\n",
    "# 프리로그인 수치 피처 집계\n",
    "agg_num = pre.groupby(\"session_id\").agg(\n",
    "    n_events=(\"session_id\", \"size\"),\n",
    "    search_count_sum=(\"search_count\", \"sum\"),\n",
    "    cart_item_count_sum=(\"cart_item_count\", \"sum\"),\n",
    "    page_depth_mean=(\"page_depth\", \"mean\"),\n",
    "    last_elapsed_mean=(\"last_action_elapsed\", \"mean\"),\n",
    "    unique_pages=(\"current_state\", \"nunique\"),\n",
    "    unique_categories=(\"resolved_category\", \"nunique\"),\n",
    ").fillna(0.0)\n",
    "\n",
    "# 세션 시작 시간 파생\n",
    "first_ts = pre.groupby(\"session_id\")[\"ts\"].min()\n",
    "agg_num[\"start_hour\"] = first_ts.dt.hour\n",
    "agg_num[\"start_weekday\"] = first_ts.dt.weekday\n",
    "\n",
    "# 라벨과 프리로그인 피처의 교집합 세션만 사용\n",
    "keep_sessions = agg_num.index.intersection(lab_full.index)\n",
    "X_num = agg_num.loc[keep_sessions].copy()\n",
    "y = lab_full.loc[keep_sessions].rename(\"gender\").copy()\n",
    "\n",
    "print(\"교집합 세션 수:\", len(keep_sessions))\n",
    "print(\"라벨 분포:\\n\", y.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3161882-35c7-4f67-b1c8-56a8cd74db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카테고리 카운트 컬럼: ['cat_cnt::Books', 'cat_cnt::Electronics', 'cat_cnt::Gaming', 'cat_cnt::Home', 'cat_cnt::Fashion']\n",
      "카테고리 파생 컬럼(예시 3개씩): ['cat_prop::Books', 'cat_prop::Electronics', 'cat_prop::Gaming', 'cat_log::Books', 'cat_log::Electronics', 'cat_log::Gaming']\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4. 카테고리 5개(OTHER 제거) 카운트/비율/로그 파생 ===\n",
    "# KNOWN_CATS 정규화\n",
    "KNOWN_CATS_NORM = [unicodedata.normalize(\"NFKC\", c.strip()) for c in KNOWN_CATS]\n",
    "norm_to_orig = {unicodedata.normalize(\"NFKC\", c.strip()): c for c in KNOWN_CATS}\n",
    "\n",
    "# 프리로그인 구간 카테고리 정규화\n",
    "pre_cat_norm = norm(pre[\"resolved_category\"])\n",
    "\n",
    "# 5개 목록에 포함된 이벤트만 집계(OTHER 제거)\n",
    "mask = pre_cat_norm.isin(KNOWN_CATS_NORM)\n",
    "pre_kept = pre[mask].copy()\n",
    "pre_kept[\"cat_norm\"] = pre_cat_norm[mask].values\n",
    "pre_kept[\"one\"] = 1\n",
    "\n",
    "# 세션 × 카테고리 카운트\n",
    "cat_cnt = pre_kept.pivot_table(\n",
    "    index=\"session_id\",\n",
    "    columns=\"cat_norm\",\n",
    "    values=\"one\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# 컬럼을 정확히 5개로 강제(없으면 0) + 원래 이름으로 되돌리기\n",
    "cat_cnt = cat_cnt.reindex(columns=KNOWN_CATS_NORM, fill_value=0)\n",
    "cat_cnt.columns = [norm_to_orig[c] for c in cat_cnt.columns]\n",
    "\n",
    "# prefix & 정렬(교집합 세션만)\n",
    "cat_cnt.columns = [f\"cat_cnt::{c}\" for c in cat_cnt.columns]\n",
    "cat_cnt = cat_cnt.reindex(X_num.index).fillna(0).astype(int)\n",
    "\n",
    "# 이후 파생에 사용할 원 컬럼명 목록\n",
    "cat_cnt_cols = list(cat_cnt.columns)\n",
    "\n",
    "# prop = count / n_events (세션 길이 보정)\n",
    "cat_prop = cat_cnt.div(X_num[\"n_events\"].replace(0, 1), axis=0)\n",
    "cat_prop.columns = [c.replace(\"cat_cnt::\", \"cat_prop::\") for c in cat_cnt_cols]\n",
    "\n",
    "# log1p(count) — 선형모델용 강도 완화\n",
    "cat_log = np.log1p(cat_cnt)\n",
    "cat_log.columns = [c.replace(\"cat_cnt::\", \"cat_log::\") for c in cat_cnt_cols]\n",
    "\n",
    "print(\"카테고리 카운트 컬럼:\", list(cat_cnt.columns))\n",
    "print(\"카테고리 파생 컬럼(예시 3개씩):\", list(cat_prop.columns[:3]) + list(cat_log.columns[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd50ab1-7f2a-4dd8-9137-9bda6afc8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (649, 26)\n",
      "                             session_id  n_events  search_count_sum  cart_item_count_sum  page_depth_mean  last_elapsed_mean  unique_pages  unique_categories  start_hour  start_weekday  \\\n",
      "0  0022505a-eba9-4865-994b-2253b3c4444c         4                 0                    0              2.5           1.000000             4                  1          13              2   \n",
      "1  00ba3fe8-1d64-4e4d-9803-1871df07f69b         8                 3                    0              4.5           1.250000             7                  1           6              4   \n",
      "2  01791a54-64a9-4336-93a6-81853d8d4ee8        13                 8                    0              7.0           1.153846             9                  2          13              2   \n",
      "3  017c1695-2c78-4a45-83a5-e11c4ae061b8         4                 0                    0              2.5           2.000000             4                  0          14              2   \n",
      "4  0185ccfb-814a-4ad0-82d8-06877765b8fc        13                24                    0              7.0           1.538462             6                  2          14              2   \n",
      "5  01aeb2a3-a2a0-45ed-b157-9f9897ce9d60        25                16                    0             13.0           1.600000             9                  4          14              2   \n",
      "6  029bf7d9-810e-47b9-bf30-ce30026a64bc         8                 0                    0              4.5           1.375000             6                  1          13              2   \n",
      "7  02e00355-824a-4fef-a98a-7438e6073796        15                27                    0              8.0           1.266667             8                  5          14              2   \n",
      "8  02e42d6f-9218-49f3-9a03-eced520f9a45        12                11                    0              6.5           1.833333             7                  1          14              2   \n",
      "9  02f77c3b-e366-4f8b-a0f9-bb0217347413         6                 0                    0              3.5           1.166667             5                  0          14              2   \n",
      "\n",
      "   cat_cnt::Books  cat_cnt::Electronics  cat_cnt::Gaming  cat_cnt::Home  cat_cnt::Fashion  cat_prop::Books  cat_prop::Electronics  cat_prop::Gaming  cat_prop::Home  cat_prop::Fashion  \\\n",
      "0               1                     0                0              0                 0         0.250000               0.000000          0.000000        0.000000           0.000000   \n",
      "1               0                     0                0              0                 1         0.000000               0.000000          0.000000        0.000000           0.125000   \n",
      "2               0                     2                0              2                 0         0.000000               0.153846          0.000000        0.153846           0.000000   \n",
      "3               0                     0                0              0                 0         0.000000               0.000000          0.000000        0.000000           0.000000   \n",
      "4               2                     0                0              2                 0         0.153846               0.000000          0.000000        0.153846           0.000000   \n",
      "5               3                     1                1              0                 1         0.120000               0.040000          0.040000        0.000000           0.040000   \n",
      "6               1                     0                0              0                 0         0.125000               0.000000          0.000000        0.000000           0.000000   \n",
      "7               1                     2                2              1                 1         0.066667               0.133333          0.133333        0.066667           0.066667   \n",
      "8               0                     1                0              0                 0         0.000000               0.083333          0.000000        0.000000           0.000000   \n",
      "9               0                     0                0              0                 0         0.000000               0.000000          0.000000        0.000000           0.000000   \n",
      "\n",
      "   cat_log::Books  cat_log::Electronics  cat_log::Gaming  cat_log::Home  cat_log::Fashion gender  \n",
      "0        0.693147              0.000000         0.000000       0.000000          0.000000      M  \n",
      "1        0.000000              0.000000         0.000000       0.000000          0.693147      F  \n",
      "2        0.000000              1.098612         0.000000       1.098612          0.000000      M  \n",
      "3        0.000000              0.000000         0.000000       0.000000          0.000000      M  \n",
      "4        1.098612              0.000000         0.000000       1.098612          0.000000      F  \n",
      "5        1.386294              0.693147         0.693147       0.000000          0.693147      F  \n",
      "6        0.693147              0.000000         0.000000       0.000000          0.000000      M  \n",
      "7        0.693147              1.098612         1.098612       0.693147          0.693147      F  \n",
      "8        0.000000              0.693147         0.000000       0.000000          0.000000      M  \n",
      "9        0.000000              0.000000         0.000000       0.000000          0.000000      M  \n",
      "\n",
      "Saved: datasets/gender/prelogin_gender_features.csv\n",
      "Saved: datasets/gender/prelogin_gender_features.parquet\n",
      "Saved (X only): datasets/gender/prelogin_features_only.csv | datasets/gender/prelogin_features_only.parquet\n",
      "Saved (y only): datasets/gender/prelogin_labels_only.csv | datasets/gender/prelogin_labels_only.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5. 최종 결합(X) & 저장(선택) ===\n",
    "# 피처 결합\n",
    "X = (\n",
    "    X_num\n",
    "    .join(cat_cnt, how=\"left\")\n",
    "    .join(cat_prop, how=\"left\")\n",
    "    .join(cat_log, how=\"left\")\n",
    ").fillna(0)\n",
    "\n",
    "# 라벨 결합 및 미리보기\n",
    "X_ = X.copy()\n",
    "if X_.index.name != \"session_id\":\n",
    "    X_.index.name = \"session_id\"\n",
    "\n",
    "dataset = X_.join(y).reset_index()\n",
    "\n",
    "print(\"dataset shape:\", dataset.shape)\n",
    "print(dataset.head(10))\n",
    "\n",
    "# (선택) 중간 산출물 저장\n",
    "FEATURES_CSV = f\"{OUTPUT_DIR}/prelogin_gender_features.csv\"\n",
    "FEATURES_PARQUET = f\"{OUTPUT_DIR}/prelogin_gender_features.parquet\"\n",
    "X_only_csv = f\"{OUTPUT_DIR}/prelogin_features_only.csv\"\n",
    "X_only_parquet = f\"{OUTPUT_DIR}/prelogin_features_only.parquet\"\n",
    "y_only_csv = f\"{OUTPUT_DIR}/prelogin_labels_only.csv\"\n",
    "y_only_parquet = f\"{OUTPUT_DIR}/prelogin_labels_only.parquet\"\n",
    "\n",
    "dataset.to_csv(FEATURES_CSV, index=False)\n",
    "dataset.to_parquet(FEATURES_PARQUET, index=False)\n",
    "X_.reset_index().to_csv(X_only_csv, index=False)\n",
    "X_.reset_index().to_parquet(X_only_parquet, index=False)\n",
    "y.to_frame(\"gender\").reset_index().to_csv(y_only_csv, index=False)\n",
    "y.to_frame(\"gender\").reset_index().to_parquet(y_only_parquet, index=False)\n",
    "\n",
    "print(f\"\\nSaved: {FEATURES_CSV}\")\n",
    "print(f\"Saved: {FEATURES_PARQUET}\")\n",
    "print(\"Saved (X only):\", X_only_csv, \"|\", X_only_parquet)\n",
    "print(\"Saved (y only):\", y_only_csv, \"|\", y_only_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2550a977-e5ee-46f3-86b6-bea59dc46626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (519, 24)  Valid: (130, 24)\n",
      "Label dist (train):\n",
      " gender\n",
      "F    0.529865\n",
      "M    0.470135\n",
      "Name: proportion, dtype: float64\n",
      "Label dist (valid):\n",
      " gender\n",
      "F    0.530769\n",
      "M    0.469231\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6. X/y 분리, 학습/검증 분할 ===\n",
    "# gender 정규화\n",
    "dataset[\"gender\"] = (\n",
    "    dataset[\"gender\"].astype(\"string\").str.strip().str.upper()\n",
    "    .replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    ")\n",
    "\n",
    "# feature/label 분리\n",
    "non_feature = {\"session_id\",\"gender\"}\n",
    "feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "X = dataset[feature_cols].copy()\n",
    "y_bin = dataset[\"gender\"].map({\"F\":0, \"M\":1}).astype(int)\n",
    "\n",
    "# 학습/검증 stratified split (8:2)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "train_idx, valid_idx = next(sss.split(X, y_bin))\n",
    "X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "y_train, y_valid = y_bin.iloc[train_idx], y_bin.iloc[valid_idx]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Valid:\", X_valid.shape)\n",
    "print(\"Label dist (train):\\n\", y_train.value_counts(normalize=True).rename({0:\"F\",1:\"M\"}))\n",
    "print(\"Label dist (valid):\\n\", y_valid.value_counts(normalize=True).rename({0:\"F\",1:\"M\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d561c9a-de12-4204-acd6-0b6801c01809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7. 보조 함수: 임계값 튜닝 & 평가 유틸 ===\n",
    "def tune_threshold(model, X_va, y_va, search=(0.2,0.8,61)):\n",
    "    \"\"\"Macro-F1 최대화 임계값 탐색 (predict_proba 사용 가능한 모델 전제)\"\"\"\n",
    "    lo, hi, n = search\n",
    "    ths = np.linspace(lo, hi, n)\n",
    "    proba = model.predict_proba(X_va)[:,1]\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in ths:\n",
    "        preds = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_va, preds, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1 = float(t), float(f1)\n",
    "    return best_t, best_f1, proba\n",
    "\n",
    "def evaluate(model, X_va, y_va, threshold=0.5, name=\"Model\"):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X_va)[:,1]\n",
    "        y_hat = (proba >= threshold).astype(int)\n",
    "    else:\n",
    "        y_hat = model.predict(X_va)\n",
    "        proba = None\n",
    "\n",
    "    print(f\"\\n=== {name} @ threshold={threshold:.3f} ===\")\n",
    "    print(classification_report(y_va, y_hat, target_names=[\"F\",\"M\"], digits=4))\n",
    "    print(\"Confusion matrix [rows=true F,M | cols=pred F,M]:\\n\", confusion_matrix(y_va, y_hat))\n",
    "    return y_hat, proba\n",
    "\n",
    "def print_scores(name, y_true, proba, t):\n",
    "    y_pred = (proba >= t).astype(int)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = (y_true.values == y_pred).mean()\n",
    "    print(f\"[{name}] Macro-F1 : {macro_f1:.4f} / Accuracy : {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d5300d-ea87-4db5-842b-02c1fe0706f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic best params: {'clf__C': 0.2, 'clf__class_weight': None, 'clf__l1_ratio': 0.5, 'clf__penalty': 'elasticnet', 'scaler': RobustScaler()}  | cv f1_macro: 0.5986\n",
      "[Calibration] done: method=isotonic, cv=5\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8. Logistic 하이퍼파라미터 튜닝 + 확률 보정 ===\n",
    "CALIB_METHOD = \"isotonic\"  # or \"sigmoid\"\n",
    "CALIB_CV = 5\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\", max_iter=5000, random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# penalty 별 그리드 (충돌 회피 위해 분리)\n",
    "log_grid = [\n",
    "    {\n",
    "        \"scaler\": [StandardScaler(with_mean=True, with_std=True),\n",
    "                   RobustScaler(with_centering=True, with_scaling=True)],\n",
    "        \"clf__penalty\": [\"l2\"],\n",
    "        \"clf__C\": [0.2, 0.5, 1.0, 2.0, 5.0],\n",
    "        \"clf__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "    {\n",
    "        \"scaler\": [StandardScaler(with_mean=True, with_std=True),\n",
    "                   RobustScaler(with_centering=True, with_scaling=True)],\n",
    "        \"clf__penalty\": [\"l1\"],\n",
    "        \"clf__C\": [0.2, 0.5, 1.0, 2.0, 5.0],\n",
    "        \"clf__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "    {\n",
    "        \"scaler\": [StandardScaler(with_mean=True, with_std=True),\n",
    "                   RobustScaler(with_centering=True, with_scaling=True)],\n",
    "        \"clf__penalty\": [\"elasticnet\"],\n",
    "        \"clf__l1_ratio\": [0.2, 0.5, 0.8],\n",
    "        \"clf__C\": [0.2, 0.5, 1.0, 2.0, 5.0],\n",
    "        \"clf__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "log_search = GridSearchCV(\n",
    "    estimator=log_pipe,\n",
    "    param_grid=log_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")\n",
    "log_search.fit(X_train, y_train)\n",
    "log_best = log_search.best_estimator_\n",
    "print(\"Logistic best params:\", log_search.best_params_, \" | cv f1_macro:\", round(log_search.best_score_, 4))\n",
    "\n",
    "# ===== 확률 보정 (best 모델 기반) =====\n",
    "try:\n",
    "    cal_log = CalibratedClassifierCV(estimator=log_best, method=CALIB_METHOD, cv=CALIB_CV)\n",
    "except TypeError:  # 구버전 호환\n",
    "    cal_log = CalibratedClassifierCV(base_estimator=log_best, method=CALIB_METHOD, cv=CALIB_CV)\n",
    "\n",
    "cal_log.fit(X_train, y_train)\n",
    "print(f\"[Calibration] done: method={CALIB_METHOD}, cv={CALIB_CV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8899e0b-b0cd-4b57-b12a-db5e0146bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticCal (prod=0.444) @ threshold=0.444 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F     0.6562    0.6087    0.6316        69\n",
      "           M     0.5909    0.6393    0.6142        61\n",
      "\n",
      "    accuracy                         0.6231       130\n",
      "   macro avg     0.6236    0.6240    0.6229       130\n",
      "weighted avg     0.6256    0.6231    0.6234       130\n",
      "\n",
      "Confusion matrix [rows=true F,M | cols=pred F,M]:\n",
      " [[42 27]\n",
      " [22 39]]\n",
      "[LogisticCal (prod@0.444)] Macro-F1 : 0.6229 / Accuracy : 0.6231\n",
      "\n",
      "[Saved] logistic_best.joblib, logistic_calibrated.joblib, logistic_meta.json, logistic_valid_predictions.csv\n",
      "[Info] production_threshold=0.444\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9. 임계값 0.444 평가 & 아티팩트 저장 ===\n",
    "# 보정된 모델(cal_log) 기준으로 prod=0.444만 출력\n",
    "yhat_prod, _ = evaluate(\n",
    "    cal_log, X_valid, y_valid,\n",
    "    threshold=PRODUCTION_THRESHOLD,\n",
    "    name=f\"LogisticCal (prod={PRODUCTION_THRESHOLD:.3f})\"\n",
    ")\n",
    "proba_v = cal_log.predict_proba(X_valid)[:,1]\n",
    "print_scores(f\"LogisticCal (prod@{PRODUCTION_THRESHOLD:.3f})\", y_valid, proba_v, PRODUCTION_THRESHOLD)\n",
    "\n",
    "# ===== 아티팩트 저장 =====\n",
    "dump(log_best, os.path.join(ARTIFACT_DIR, \"logistic_best.joblib\"))\n",
    "dump(cal_log,  os.path.join(ARTIFACT_DIR, \"logistic_calibrated.joblib\"))\n",
    "\n",
    "# JSON 직렬화 보조: 객체 → {name, params}\n",
    "def _to_jsonable(v):\n",
    "    if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "        return v\n",
    "    if hasattr(v, \"get_params\"):\n",
    "        return {\"name\": v.__class__.__name__, \"params\": v.get_params(deep=False)}\n",
    "    return str(v)\n",
    "\n",
    "best_params_jsonable = {k: _to_jsonable(v) for k, v in log_search.best_params_.items()}\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"logistic_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"best_params\": best_params_jsonable,\n",
    "        \"best_cv_f1_macro\": float(log_search.best_score_),\n",
    "        \"production_threshold\": float(PRODUCTION_THRESHOLD),\n",
    "        \"calibration\": {\"method\": CALIB_METHOD, \"cv\": CALIB_CV}\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 검증 예측 저장\n",
    "pred_col = f\"y_pred@{PRODUCTION_THRESHOLD:.3f}\"\n",
    "out_df = pd.DataFrame({\n",
    "    \"session_id\": dataset.iloc[valid_idx][\"session_id\"].values if \"session_id\" in dataset.columns else np.arange(len(valid_idx)),\n",
    "    \"y_true\": y_valid.values,\n",
    "    pred_col: (proba_v >= PRODUCTION_THRESHOLD).astype(int),\n",
    "    \"proba_cal\": proba_v\n",
    "})\n",
    "out_df.to_csv(os.path.join(ARTIFACT_DIR, \"logistic_valid_predictions.csv\"), index=False)\n",
    "\n",
    "print(\"\\n[Saved] logistic_best.joblib, logistic_calibrated.joblib, logistic_meta.json, logistic_valid_predictions.csv\")\n",
    "print(f\"[Info] production_threshold={PRODUCTION_THRESHOLD:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83d505-2480-4e61-9296-7b1c1d76f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176aebff-c5dc-44f6-bc2f-337daf34ac15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250b922-3a62-48e5-9a03-795c1e597b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
