{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a6144-ed4b-4303-a59f-ca40a8042c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1 패키지 설치\n",
    "!pip install -U pyarrow==15.0.2 fastparquet pandas torch optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240a47-e321-4be8-9e1c-22a064a86179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2 경로/시드/하이퍼 파라미터\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "import torch\n",
    "\n",
    "# ==== 경로 ====\n",
    "DATA_DIR = Path(\"/home/jovyan/datasets/next_state_pre\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_PARQUET = Path(\"/home/jovyan/datasets/processed_user_behavior.sorted.parquet\")  # 필요시 수정\n",
    "MAP_JSON    = DATA_DIR / \"state_mapping.json\"\n",
    "SPLIT_JSON  = DATA_DIR / \"train_val_sessions.json\"\n",
    "PAIR_NPY    = DATA_DIR / \"observed_prev_pairs.npy\"\n",
    "BEST_PATH   = DATA_DIR / \"best_model.pt\"\n",
    "\n",
    "# ==== 시드/분할 ====\n",
    "SEED = 17\n",
    "VAL_FRAC = 0.2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ==== 하이퍼파라미터 (수동 설정 기본값) ====\n",
    "HYPERPARAMS = {\n",
    "    \"emb_dim\": 192,\n",
    "    \"pair_dim\": 16,\n",
    "    \"hid\": 384,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 3,\n",
    "    \"use_amp\": True,           # CUDA 사용시 AMP 켜기\n",
    "    \"grad_clip_norm\": 1.0,\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DATA_DIR:\", DATA_DIR, \"| device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e2f3c-6834-4164-a925-c9e114524cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3 데이터 로드 & 필수 컬럼 확인\n",
    "df = pd.read_parquet(RAW_PARQUET)\n",
    "need_cols = [\"session_id\", \"current_state\", \"next_state\"]\n",
    "miss = [c for c in need_cols if c not in df.columns]\n",
    "assert not miss, f\"필수 컬럼 없음: {miss}\"\n",
    "print(df[need_cols].head(3))\n",
    "print(\"[INFO] rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d27c6a-caea-44a0-afe8-1c541207c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4 세션 통계 & 짧은 세션(2이하) 필터링\n",
    "MIN_LEN = 3  # 권장: >=3\n",
    "\n",
    "sess_len = df.groupby(\"session_id\").size().rename(\"sess_len\")\n",
    "desc = sess_len.describe(percentiles=[0.25,0.5,0.75,0.9,0.95]).to_dict()\n",
    "\n",
    "print(\"[INFO] session length stats:\")\n",
    "for k in [\"count\",\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"90%\",\"95%\",\"max\"]:\n",
    "    if k in desc: print(f\"  {k:>4}: {desc[k]}\")\n",
    "\n",
    "short_ratio = (sess_len < MIN_LEN).mean()\n",
    "print(f\"[INFO] short sessions (<{MIN_LEN}) ratio: {short_ratio:.2%}  ({(sess_len < MIN_LEN).sum()} / {len(sess_len)})\")\n",
    "\n",
    "valid_sessions = sess_len[sess_len >= MIN_LEN].index\n",
    "n_before = len(df)\n",
    "df = df[df[\"session_id\"].isin(valid_sessions)].reset_index(drop=True)\n",
    "n_after = len(df)\n",
    "print(f\"[FILTER] kept sessions >= {MIN_LEN}: {len(valid_sessions)} / {len(sess_len)} rows: {n_before} -> {n_after} (-{n_before - n_after})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670a46c-761e-4653-a1ee-3027963d53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5 상태 인덱싱(PAD/UNK) 및 저장\n",
    "PAD_ID, UNK_ID = 0, 1\n",
    "\n",
    "states_all = pd.Index(df[\"current_state\"].astype(str).unique()).union(\n",
    "    df[\"next_state\"].astype(str).unique()\n",
    ")\n",
    "states_sorted = pd.Index(sorted(states_all))\n",
    "\n",
    "state2idx = {s: i+2 for i, s in enumerate(states_sorted)}  # 2부터 시작\n",
    "idx2state = {i: s for s, i in state2idx.items()}\n",
    "num_states = len(state2idx) + 2  # PAD, UNK 포함\n",
    "\n",
    "with open(MAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"PAD_ID\": PAD_ID, \"UNK_ID\": UNK_ID, \"state2idx\": state2idx}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[SAVE] state mapping ->\", MAP_JSON)\n",
    "print(\"[INFO] num_states (PAD/UNK 포함):\", num_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbb7bf-b7c3-4c3f-942e-321fc2c96c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6 인덱싱 적용\n",
    "with open(MAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    mp = json.load(f)\n",
    "state2idx = mp[\"state2idx\"]; PAD_ID = mp[\"PAD_ID\"]; UNK_ID = mp[\"UNK_ID\"]\n",
    "\n",
    "def map_state(s):\n",
    "    return state2idx.get(str(s), UNK_ID)\n",
    "\n",
    "df[\"current_state_idx\"] = df[\"current_state\"].astype(str).map(map_state).astype(\"int32\")\n",
    "df[\"next_state_idx\"]    = df[\"next_state\"].astype(str).map(map_state).astype(\"int32\")\n",
    "print(df[[\"current_state\",\"current_state_idx\",\"next_state\",\"next_state_idx\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c68fc6-577b-4599-adc4-d0e2b1f5bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7 prev1/prev2 생성 & 관측 쌍 저장\n",
    "g = df.groupby(\"session_id\")[\"current_state_idx\"]\n",
    "df[\"prev1_idx\"] = g.shift(1).fillna(PAD_ID).astype(\"int32\")\n",
    "df[\"prev2_idx\"] = g.shift(2).fillna(PAD_ID).astype(\"int32\")\n",
    "\n",
    "pairs = df[[\"prev2_idx\",\"prev1_idx\"]].drop_duplicates().to_numpy(dtype=np.int32)\n",
    "np.save(PAIR_NPY, pairs)\n",
    "print(\"[SAVE] observed prev pairs ->\", PAIR_NPY, pairs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afe78a-3447-4323-9345-2465c73c82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8 세션 기반 Train/Val 분할\n",
    "sess = df[\"session_id\"].drop_duplicates().to_numpy()\n",
    "np.random.shuffle(sess)\n",
    "n_val = int(len(sess)*VAL_FRAC)\n",
    "val_sessions = set(sess[:n_val]); train_sessions = set(sess[n_val:])\n",
    "\n",
    "with open(SPLIT_JSON, \"w\") as f:\n",
    "    json.dump({\"seed\": SEED, \"val_frac\": VAL_FRAC,\n",
    "               \"train_sessions\": list(train_sessions),\n",
    "               \"val_sessions\": list(val_sessions)}, f, indent=2)\n",
    "print(\"[SAVE] split ->\", SPLIT_JSON, \"| #train:\", len(train_sessions), \" #val:\", len(val_sessions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80415df-3071-42a9-9710-15aff0cdb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9 탭형 피처 저장\n",
    "TAB_FEATS = [\"current_state_idx\",\"prev1_idx\",\"prev2_idx\"]\n",
    "LABEL = \"next_state_idx\"\n",
    "\n",
    "X_train = df[df[\"session_id\"].isin(train_sessions)][TAB_FEATS].reset_index(drop=True)\n",
    "y_train = df[df[\"session_id\"].isin(train_sessions)][LABEL].reset_index(drop=True)\n",
    "X_val   = df[df[\"session_id\"].isin(val_sessions)][TAB_FEATS].reset_index(drop=True)\n",
    "y_val   = df[df[\"session_id\"].isin(val_sessions)][LABEL].reset_index(drop=True)\n",
    "\n",
    "# Parquet\n",
    "X_train.astype(\"int32\").to_parquet(DATA_DIR/\"X_train.parquet\", index=False)\n",
    "y_train.to_frame(name=\"y\").astype(\"int32\").to_parquet(DATA_DIR/\"y_train.parquet\", index=False)\n",
    "X_val.astype(\"int32\").to_parquet(DATA_DIR/\"X_val.parquet\", index=False)\n",
    "y_val.to_frame(name=\"y\").astype(\"int32\").to_parquet(DATA_DIR/\"y_val.parquet\", index=False)\n",
    "\n",
    "# CSV (옵션)\n",
    "X_train.astype(\"int32\").to_csv(DATA_DIR/\"X_train.csv\", index=False)\n",
    "y_train.to_frame(name=\"y\").astype(\"int32\").to_csv(DATA_DIR/\"y_train.csv\", index=False)\n",
    "X_val.astype(\"int32\").to_csv(DATA_DIR/\"X_val.csv\", index=False)\n",
    "y_val.to_frame(name=\"y\").astype(\"int32\").to_csv(DATA_DIR/\"y_val.csv\", index=False)\n",
    "\n",
    "print(\"[SAVE] parquet & csv ->\", DATA_DIR)\n",
    "print(\"  X_train:\", X_train.shape, \" | y_train:\", y_train.shape)\n",
    "print(\"  X_val  :\", X_val.shape,   \" | y_val  :\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce70d5c-91d5-4dcb-97be-3305abacbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10 시퀀스 빌드\n",
    "def build_session_sequences(frame):\n",
    "    seqs_in, seqs_prevpair, seqs_y = [], [], []\n",
    "    for sid, sub in frame.sort_values([\"session_id\"]).groupby(\"session_id\"):\n",
    "        x = sub[\"current_state_idx\"].to_numpy(dtype=np.int32)\n",
    "        y = sub[\"next_state_idx\"].to_numpy(dtype=np.int32)\n",
    "        p1 = sub[\"prev1_idx\"].to_numpy(dtype=np.int32)\n",
    "        p2 = sub[\"prev2_idx\"].to_numpy(dtype=np.int32)\n",
    "        if len(x) < 2: \n",
    "            continue\n",
    "        seqs_in.append(x[:-1])\n",
    "        seqs_y.append(y[:-1])\n",
    "        seqs_prevpair.append(np.stack([p2[:-1], p1[:-1]], axis=1))  # [T-1,2]\n",
    "    return seqs_in, seqs_prevpair, seqs_y\n",
    "\n",
    "train_in, train_prevpair, train_y = build_session_sequences(df[df[\"session_id\"].isin(train_sessions)])\n",
    "val_in,   val_prevpair,   val_y   = build_session_sequences(df[df[\"session_id\"].isin(val_sessions)])\n",
    "\n",
    "print(\"[INFO] #train seq:\", len(train_in), \" | #val seq:\", len(val_in))\n",
    "print(\"[INFO] sample lens:\", [len(train_in[i]) for i in range(min(3, len(train_in)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cb75c-f60f-42f1-96ae-4404ce59219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11.1 Dataset/DataLoader & 배치 확인\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class NextStateSeqDataset(Dataset):\n",
    "    def __init__(self, xs, pairs, ys):\n",
    "        self.xs = xs; self.pairs = pairs; self.ys = ys\n",
    "    def __len__(self): return len(self.xs)\n",
    "    def __getitem__(self, i): return self.xs[i], self.pairs[i], self.ys[i]\n",
    "\n",
    "def pad_collate(batch, pad_id=PAD_ID):\n",
    "    xs, ps, ys = zip(*batch)\n",
    "    L = max(len(x) for x in xs)\n",
    "    x_pad = np.full((len(xs), L), pad_id, dtype=np.int64)\n",
    "    y_pad = np.full((len(xs), L), pad_id, dtype=np.int64)\n",
    "    mask  = np.zeros((len(xs), L), dtype=np.bool_)\n",
    "    pair_pad = np.full((len(xs), L, 2), pad_id, dtype=np.int64)\n",
    "    for i,(x,p,y) in enumerate(zip(xs,ps,ys)):\n",
    "        l = len(x)\n",
    "        x_pad[i,:l] = x; y_pad[i,:l] = y; mask[i,:l] = True; pair_pad[i,:l,:] = p\n",
    "    return (torch.from_numpy(x_pad), torch.from_numpy(pair_pad),\n",
    "            torch.from_numpy(y_pad), torch.from_numpy(mask))\n",
    "\n",
    "train_ds = NextStateSeqDataset(train_in, train_prevpair, train_y)\n",
    "val_ds   = NextStateSeqDataset(val_in,   val_prevpair,   val_y)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=HYPERPARAMS[\"batch_size\"], shuffle=True,  collate_fn=pad_collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=HYPERPARAMS[\"batch_size\"], shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "NUM_CLASSES = num_states  # 중요: vocab_size와 동일\n",
    "xb, pb, yb, mb = next(iter(train_loader))\n",
    "print(\"[INFO] batch shapes:\", xb.shape, pb.shape, yb.shape, mb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ec57f-ff05-4e37-9b58-1937edd190a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11.2 prev1/prev2 csv로 저장\n",
    "# === CSV로 prev2/prev1 확인 저장 ===\n",
    "import pandas as pd\n",
    "\n",
    "# 배치에서 numpy로 변환\n",
    "pb_np = pb.numpy()    # (B, L, 2)\n",
    "yb_np = yb.numpy()    # (B, L)\n",
    "mask_np = mb.numpy()  # (B, L)\n",
    "\n",
    "rows = []\n",
    "for i in range(pb_np.shape[0]):       # 배치 크기\n",
    "    for j in range(pb_np.shape[1]):   # 시퀀스 길이\n",
    "        if mask_np[i, j]:  # 유효 토큰만\n",
    "            prev2, prev1 = pb_np[i, j]\n",
    "            rows.append({\n",
    "                \"sample_id\": i,\n",
    "                \"step\": j,\n",
    "                \"prev2\": int(prev2),\n",
    "                \"prev1\": int(prev1),\n",
    "                \"target\": int(yb_np[i, j])\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "out_path = \"/home/jovyan/datasets/next_state_pre/pre_pairs_sample.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"[SAVE] prev2/prev1/target 샘플 -> {out_path} (rows={len(df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daaca1-2973-4c58-a5e2-28a02ae34169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12 간단 LSTMClassifier 구현\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=192, pair_dim=16, hid=384, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_classes = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
    "        self.pair_proj = nn.Linear(2, pair_dim)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim + pair_dim,\n",
    "                            hidden_size=hid, num_layers=num_layers,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0,\n",
    "                            batch_first=True, bidirectional=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(hid, vocab_size)\n",
    "\n",
    "    def forward(self, x, pair, mask):\n",
    "        xe = self.emb(x)                  # [B,T,E]\n",
    "        pe = self.pair_proj(pair.float()) # [B,T,P]\n",
    "        inp = torch.cat([xe, pe], dim=-1) # [B,T,E+P]\n",
    "        out, _ = self.lstm(inp)           # [B,T,H]\n",
    "        out = self.drop(out)\n",
    "        logits = self.head(out)           # [B,T,C]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a7659-ffea-4793-ba1f-ec4b818f0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13 클래스 가중치 계산\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# y_train은 탭형용이라도 라벨 분포 확인에 그대로 사용 가능\n",
    "label_counts = Counter(y_train.tolist())\n",
    "total = sum(label_counts.values())\n",
    "\n",
    "weights = np.ones(NUM_CLASSES, dtype=np.float32)\n",
    "for k, v in label_counts.items():\n",
    "    weights[k] = total / (len(label_counts) * v)  # inverse frequency\n",
    "\n",
    "class_weights = torch.tensor(weights, device=device)\n",
    "print(\"[INFO] class_weights sample:\", {k: float(class_weights[k]) for k in list(label_counts.keys())[:8]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32651ac7-0480-45a2-87c0-e22e23502273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14 디바이스/모델/옵티마이저 준비 & 디버그 포워드\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=NUM_CLASSES,\n",
    "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
    "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
    "    hid=HYPERPARAMS[\"hid\"],\n",
    "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
    "    dropout=HYPERPARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=HYPERPARAMS[\"lr\"], weight_decay=HYPERPARAMS[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "scaler    = GradScaler(\"cuda\", enabled=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]))\n",
    "\n",
    "# 디버그 포워드\n",
    "xb, pb, yb, mb = next(iter(train_loader))\n",
    "xb, pb, yb, mb = xb.to(device), pb.to(device), yb.to(device), mb.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(xb, pb, mb)\n",
    "    print(\"[DEBUG] logits shape:\", tuple(logits.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99957c-063d-4261-82de-fc4422463bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 15 학습/평가 함수\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, num_classes, class_weights=None, use_amp=True, grad_clip_norm=1.0, scaler=None):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
    "    correct = 0; total = 0; last_loss = 0.0\n",
    "\n",
    "    for x, pair, y, mask in loader:\n",
    "        x, pair, y, mask = x.to(device), pair.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if use_amp and scaler is not None:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                logits = model(x, pair, mask)\n",
    "                loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip_norm:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(x, pair, mask)\n",
    "            loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "            loss.backward()\n",
    "            if grad_clip_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        last_loss = float(loss.item())\n",
    "        with torch.no_grad():\n",
    "            pred  = logits.argmax(-1)\n",
    "            valid = (mask & (y != PAD_ID))\n",
    "            correct += (pred[valid] == y[valid]).sum().item()\n",
    "            total   += valid.sum().item()\n",
    "\n",
    "    return {\"loss\": last_loss, \"acc\": correct / max(total, 1)}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, num_classes, class_weights=None):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
    "    total_valid = 0; top1_correct = 0; top3_correct = 0\n",
    "    sum_loss = 0.0; n_batches = 0\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    for x, pair, y, mask in loader:\n",
    "        x, pair, y, mask = x.to(device), pair.to(device), y.to(device), mask.to(device)\n",
    "        logits = model(x, pair, mask)\n",
    "        loss   = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "        sum_loss += float(loss.item()); n_batches += 1\n",
    "\n",
    "        prob = logits.softmax(-1); pred = prob.argmax(-1)\n",
    "        valid = (mask & (y != PAD_ID))\n",
    "        top1_correct += (pred[valid] == y[valid]).sum().item()\n",
    "        total_valid  += valid.sum().item()\n",
    "\n",
    "        _, topk_idx = prob.topk(3, dim=-1)\n",
    "        top3_correct += (topk_idx[valid].eq(y[valid].unsqueeze(-1))).any(dim=-1).sum().item()\n",
    "\n",
    "        all_preds.extend(pred[valid].detach().cpu().tolist())\n",
    "        all_trues.extend(y[valid].detach().cpu().tolist())\n",
    "\n",
    "    macro_f1 = f1_score(all_trues, all_preds, average=\"macro\") if all_trues else 0.0\n",
    "    return {\n",
    "        \"loss\": (sum_loss / max(n_batches, 1)),\n",
    "        \"top1\": (top1_correct / max(total_valid, 1)),\n",
    "        \"top3\": (top3_correct / max(total_valid, 1)),\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4672c-224c-44ee-a62d-7a4236133e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16 학습 루프\n",
    "best, patience, bad = None, HYPERPARAMS[\"patience\"], 0\n",
    "\n",
    "for epoch in range(HYPERPARAMS[\"epochs\"]):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         num_classes=NUM_CLASSES, class_weights=class_weights,\n",
    "                         use_amp=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]),\n",
    "                         grad_clip_norm=HYPERPARAMS[\"grad_clip_norm\"], scaler=scaler)\n",
    "    ev = eval_epoch(model,  val_loader,   device,\n",
    "                    num_classes=NUM_CLASSES, class_weights=class_weights)\n",
    "\n",
    "    # 스케줄러(plateau) — 최대화 기준: top1\n",
    "    scheduler.step(ev[\"top1\"])\n",
    "\n",
    "    print(f\"[E{epoch:02d}] \"\n",
    "          f\"train loss={tr['loss']:.4f} acc={tr['acc']:.4f} | \"\n",
    "          f\"val loss={ev['loss']:.4f} top1={ev['top1']:.4f} \"\n",
    "          f\"top3={ev['top3']:.4f} macroF1={ev['macro_f1']:.4f}\")\n",
    "\n",
    "    if not best or ev[\"top1\"] > best[\"top1\"]:\n",
    "        best, bad = {\"epoch\": epoch, **ev}, 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"metrics\": ev,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"pad_id\": PAD_ID,\n",
    "            \"hparams\": HYPERPARAMS,\n",
    "        }, str(BEST_PATH))\n",
    "        print(f\"[SAVE] best -> {BEST_PATH}  (epoch={epoch}, top1={ev['top1']:.4f})\")\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "print(\"[BEST]\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6e5c1-10cf-49f1-a2af-871e74ecc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 17 Optuna 하이퍼 파라미터 탐색\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 탐색 공간\n",
    "    emb_dim    = trial.suggest_int(\"emb_dim\", 96, 256, step=32)\n",
    "    pair_dim   = trial.suggest_int(\"pair_dim\", 8, 32, step=8)\n",
    "    hid        = trial.suggest_int(\"hid\", 128, 512, step=64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout    = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr         = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    wd         = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    m = LSTMClassifier(\n",
    "        vocab_size=NUM_CLASSES, emb_dim=emb_dim, pair_dim=pair_dim,\n",
    "        hid=hid, num_layers=num_layers, dropout=dropout\n",
    "    ).to(device)\n",
    "    opt = AdamW(m.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    scaler_tmp = GradScaler(\"cuda\", enabled=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]))\n",
    "\n",
    "    # 짧게 예열\n",
    "    for _ in range(3):\n",
    "        train_one_epoch(m, train_loader, opt, device, NUM_CLASSES, class_weights=class_weights,\n",
    "                        use_amp=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]),\n",
    "                        grad_clip_norm=HYPERPARAMS[\"grad_clip_norm\"], scaler=scaler_tmp)\n",
    "    ev = eval_epoch(m, val_loader, device, NUM_CLASSES, class_weights=class_weights)\n",
    "    return ev[\"top1\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"next_state_lstm_top1\")\n",
    "study.optimize(objective, n_trials=15, show_progress_bar=True)\n",
    "\n",
    "print(\"[BEST VALUE]\", study.best_value)\n",
    "print(\"[BEST PARAMS]\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984943f-42ef-4df5-9ba2-861158e781e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18 최종 모델 학습\n",
    "# === Cell 18: Apply Optuna best -> update HYPERPARAMS -> retrain final model ===\n",
    "import json, torch\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# ---- 안전 체크 (Cell 17을 먼저 실행했는지 등) ----\n",
    "assert 'study' in globals(), \"Optuna study가 없습니다. 먼저 Cell 17(Optuna)을 실행하세요.\"\n",
    "assert 'NUM_CLASSES' in globals(), \"NUM_CLASSES가 필요합니다.\"\n",
    "assert 'PAD_ID' in globals(), \"PAD_ID가 필요합니다.\"\n",
    "assert 'HYPERPARAMS' in globals(), \"HYPERPARAMS 딕셔너리가 필요합니다.\"\n",
    "assert 'train_loader' in globals() and 'val_loader' in globals(), \"train/val DataLoader가 필요합니다.\"\n",
    "assert 'train_one_epoch' in globals() and 'eval_epoch' in globals(), \"train_one_epoch / eval_epoch 함수가 필요합니다.\"\n",
    "assert 'LSTMClassifier' in globals(), \"LSTMClassifier 클래스가 필요합니다.\"\n",
    "device = globals().get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BEST_PATH = globals().get('BEST_PATH', './best_model.pt')\n",
    "class_weights = globals().get('class_weights', None)\n",
    "\n",
    "# ---- 1) Optuna best 가져와서 HYPERPARAMS 업데이트 ----\n",
    "best_params = study.best_trial.params\n",
    "HYPERPARAMS.update({\n",
    "    \"emb_dim\":      best_params[\"emb_dim\"],\n",
    "    \"pair_dim\":     best_params[\"pair_dim\"],\n",
    "    \"hid\":          best_params[\"hid\"],\n",
    "    \"num_layers\":   best_params[\"num_layers\"],\n",
    "    \"dropout\":      best_params[\"dropout\"],\n",
    "    \"lr\":           best_params[\"lr\"],\n",
    "    \"weight_decay\": best_params[\"weight_decay\"],\n",
    "})\n",
    "print(\"[INFO] HYPERPARAMS updated with Optuna best:\", HYPERPARAMS)\n",
    "\n",
    "# (옵션) 베스트 파라미터를 JSON으로 남기기\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    BEST_JSON = (Path(BEST_PATH).parent / \"best_hparams.json\")\n",
    "    with open(BEST_JSON, \"w\") as f:\n",
    "        json.dump({\"optuna_best\": best_params, \"hparams\": HYPERPARAMS}, f, indent=2)\n",
    "    print(f\"[SAVE] best_hparams -> {BEST_JSON}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] best_hparams 저장 생략:\", e)\n",
    "\n",
    "# ---- 2) 베스트 HYPERPARAMS로 모델/옵티마이저/스케줄러 준비 ----\n",
    "USE_AMP   = (device == \"cuda\") and bool(HYPERPARAMS.get(\"use_amp\", True))\n",
    "EPOCHS    = int(HYPERPARAMS.get(\"epochs\", 50))\n",
    "PATIENCE  = int(HYPERPARAMS.get(\"patience\", 3))\n",
    "GRAD_CLIP = float(HYPERPARAMS.get(\"grad_clip_norm\", 1.0))\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=NUM_CLASSES,\n",
    "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
    "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
    "    hid=HYPERPARAMS[\"hid\"],\n",
    "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
    "    dropout=HYPERPARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=HYPERPARAMS[\"lr\"], weight_decay=HYPERPARAMS[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "scaler    = GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "# ---- 3) 본 학습 루프 (조기 종료 & 체크포인트 저장) ----\n",
    "best, bad = None, 0\n",
    "for epoch in range(EPOCHS):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         num_classes=NUM_CLASSES, class_weights=class_weights,\n",
    "                         use_amp=USE_AMP, grad_clip_norm=GRAD_CLIP, scaler=scaler)\n",
    "    ev = eval_epoch(model,  val_loader,   device,\n",
    "                    num_classes=NUM_CLASSES, class_weights=class_weights)\n",
    "\n",
    "    # 스케줄러(Plateau) 기준: top1 최대화\n",
    "    scheduler.step(ev[\"top1\"])\n",
    "\n",
    "    print(f\"[E{epoch:02d}] train loss={tr['loss']:.4f} acc={tr['acc']:.4f} | \"\n",
    "          f\"val loss={ev['loss']:.4f} top1={ev['top1']:.4f} top3={ev['top3']:.4f} macroF1={ev['macro_f1']:.4f}\")\n",
    "\n",
    "    if not best or ev[\"top1\"] > best[\"top1\"]:\n",
    "        best, bad = {\"epoch\": epoch, **ev}, 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"metrics\": ev,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"pad_id\": PAD_ID,\n",
    "            \"hparams\": HYPERPARAMS,\n",
    "        }, str(BEST_PATH))\n",
    "        print(f\"[SAVE] best -> {BEST_PATH}  (epoch={epoch}, top1={ev['top1']:.4f})\")\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= PATIENCE:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "print(\"\\n[BEST FINAL]\", best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
