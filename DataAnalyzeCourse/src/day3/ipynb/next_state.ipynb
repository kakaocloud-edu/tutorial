{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next-State Prediction (LSTM) — Clean Notebook\n",
        "\n",
        "이 노트북은 사용자 세션 로그로 **다음 상태(next_state)** 를 예측하는 LSTM 파이프라인을 재현 가능하게 제공합니다.  \n",
        "전처리 → 상태 인덱싱 → 시퀀스/탭형 피처 생성 → 학습/검증 분할 → LSTM 학습 → Optuna 탐색 → 최종 재학습 순서로 구성됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) 런타임/환경\n",
        "\n",
        "- Python 3.11+\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1) 의존성 설치 (런타임 최초 1회)\n",
        "!pip -q install -U pyarrow==15.0.2 fastparquet pandas torch optuna scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 경로/시드/하이퍼파라미터\n",
        "\n",
        "- 학습 산출물은 `DATA_DIR`에 저장됩니다.\n",
        "- `HYPERPARAMS`는 기본값이며, Optuna 탐색 후 자동으로 갱신됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json, pandas as pd, numpy as np\n",
        "import torch\n",
        "\n",
        "# ==== 경로 ====\n",
        "DATA_DIR = Path(\"/home/jovyan/datasets/next_state_pre\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_PARQUET = Path(\"/home/jovyan/datasets/processed_user_behavior.sorted.parquet\")  # 필요시 수정\n",
        "MAP_JSON    = DATA_DIR / \"state_mapping.json\"\n",
        "SPLIT_JSON  = DATA_DIR / \"train_val_sessions.json\"\n",
        "PAIR_NPY    = DATA_DIR / \"observed_prev_pairs.npy\"\n",
        "BEST_PATH   = DATA_DIR / \"best_model.pt\"\n",
        "\n",
        "# ==== 시드/분할 ====\n",
        "SEED = 17\n",
        "VAL_FRAC = 0.2\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ==== 하이퍼파라미터 (수동 설정 기본값) ====\n",
        "HYPERPARAMS = {\n",
        "    \"emb_dim\": 192,\n",
        "    \"pair_dim\": 16,\n",
        "    \"hid\": 384,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 3e-4,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"patience\": 3,\n",
        "    \"use_amp\": True,           # CUDA 사용시 AMP 켜기\n",
        "    \"grad_clip_norm\": 1.0,\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DATA_DIR:\", DATA_DIR, \"| device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 데이터 로드 & 필수 컬럼 검증\n",
        "\n",
        "필수 컬럼: `session_id`, `current_state`, `next_state`  \n",
        "누락 시 즉시 실패하도록 `assert`로 방어합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df = pd.read_parquet(RAW_PARQUET)\n",
        "need_cols = [\"session_id\", \"current_state\", \"next_state\"]\n",
        "miss = [c for c in need_cols if c not in df.columns]\n",
        "assert not miss, f\"필수 컬럼 없음: {miss}\"\n",
        "print(df[need_cols].head(3))\n",
        "print(\"[INFO] rows:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 세션 통계 & 짧은 세션 필터링\n",
        "\n",
        "- 최소 길이 `MIN_LEN` 미만의 세션은 학습에 불리하므로 제거합니다.\n",
        "- 통계를 함께 출력해 데이터 분포를 빠르게 파악합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "MIN_LEN = 3  # 권장: >=3\n",
        "\n",
        "sess_len = df.groupby(\"session_id\").size().rename(\"sess_len\")\n",
        "desc = sess_len.describe(percentiles=[0.25,0.5,0.75,0.9,0.95]).to_dict()\n",
        "\n",
        "print(\"[INFO] session length stats:\")\n",
        "for k in [\"count\",\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"90%\",\"95%\",\"max\"]:\n",
        "    if k in desc: print(f\"  {k:>4}: {desc[k]}\")\n",
        "\n",
        "short_ratio = (sess_len < MIN_LEN).mean()\n",
        "print(f\"[INFO] short sessions (<{MIN_LEN}) ratio: {short_ratio:.2%}  ({(sess_len < MIN_LEN).sum()} / {len(sess_len)})\")\n",
        "\n",
        "valid_sessions = sess_len[sess_len >= MIN_LEN].index\n",
        "n_before = len(df)\n",
        "df = df[df[\"session_id\"].isin(valid_sessions)].reset_index(drop=True)\n",
        "n_after = len(df)\n",
        "print(f\"[FILTER] kept sessions >= {MIN_LEN}: {len(valid_sessions)} / {len(sess_len)} rows: {n_before} -> {n_after} (-{n_before - n_after})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 상태 인덱싱 (PAD/UNK 포함) & 저장\n",
        "\n",
        "- `PAD_ID=0`, `UNK_ID=1`로 예약.\n",
        "- 상태 사전을 JSON으로 저장하여 재현성과 추후 추론 일관성을 보장합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "PAD_ID, UNK_ID = 0, 1\n",
        "\n",
        "states_all = pd.Index(df[\"current_state\"].astype(str).unique()).union(\n",
        "    df[\"next_state\"].astype(str).unique()\n",
        ")\n",
        "states_sorted = pd.Index(sorted(states_all))\n",
        "\n",
        "state2idx = {s: i+2 for i, s in enumerate(states_sorted)}  # 2부터 시작\n",
        "idx2state = {i: s for s, i in state2idx.items()}\n",
        "num_states = len(state2idx) + 2  # PAD, UNK 포함\n",
        "\n",
        "with open(MAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"PAD_ID\": PAD_ID, \"UNK_ID\": UNK_ID, \"state2idx\": state2idx}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"[SAVE] state mapping ->\", MAP_JSON)\n",
        "print(\"[INFO] num_states (PAD/UNK 포함):\", num_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) 인덱싱 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(MAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    mp = json.load(f)\n",
        "state2idx = mp[\"state2idx\"]; PAD_ID = mp[\"PAD_ID\"]; UNK_ID = mp[\"UNK_ID\"]\n",
        "\n",
        "def map_state(s):\n",
        "    return state2idx.get(str(s), UNK_ID)\n",
        "\n",
        "df[\"current_state_idx\"] = df[\"current_state\"].astype(str).map(map_state).astype(\"int32\")\n",
        "df[\"next_state_idx\"]    = df[\"next_state\"].astype(str).map(map_state).astype(\"int32\")\n",
        "print(df[[\"current_state\",\"current_state_idx\",\"next_state\",\"next_state_idx\"]].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) prev1/prev2 생성 & 관측 쌍 저장\n",
        "\n",
        "- 시퀀스 맥락을 반영하기 위해 `prev1`, `prev2`를 만듭니다.\n",
        "- 관측된 `(prev2, prev1)` 조합을 별도 저장해 분석 및 규칙 기반 마스킹에 활용 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "g = df.groupby(\"session_id\")[\"current_state_idx\"]\n",
        "df[\"prev1_idx\"] = g.shift(1).fillna(PAD_ID).astype(\"int32\")\n",
        "df[\"prev2_idx\"] = g.shift(2).fillna(PAD_ID).astype(\"int32\")\n",
        "\n",
        "pairs = df[[\"prev2_idx\",\"prev1_idx\"]].drop_duplicates().to_numpy(dtype=np.int32)\n",
        "np.save(PAIR_NPY, pairs)\n",
        "print(\"[SAVE] observed prev pairs ->\", PAIR_NPY, pairs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) 세션 단위 Train/Val 분할\n",
        "\n",
        "- 세션 누수 방지를 위해 세션 기준으로 분할합니다.\n",
        "- 사용한 시드/분할 결과를 JSON으로 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sess = df[\"session_id\"].drop_duplicates().to_numpy()\n",
        "np.random.shuffle(sess)\n",
        "n_val = int(len(sess)*VAL_FRAC)\n",
        "val_sessions = set(sess[:n_val]); train_sessions = set(sess[n_val:])\n",
        "\n",
        "with open(SPLIT_JSON, \"w\") as f:\n",
        "    json.dump({\"seed\": SEED, \"val_frac\": VAL_FRAC,\n",
        "               \"train_sessions\": list(train_sessions),\n",
        "               \"val_sessions\": list(val_sessions)}, f, indent=2)\n",
        "print(\"[SAVE] split ->\", SPLIT_JSON, \"| #train:\", len(train_sessions), \" #val:\", len(val_sessions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) 탭형 피처/레이블 저장 (Parquet/CSV)\n",
        "\n",
        "- 경량 모델 실험이나 원시 분포 확인용입니다.\n",
        "- 주 학습은 시퀀스(LSTM)로 진행되지만, 탭형 산출물을 함께 보관합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "TAB_FEATS = [\"current_state_idx\",\"prev1_idx\",\"prev2_idx\"]\n",
        "LABEL = \"next_state_idx\"\n",
        "\n",
        "X_train = df[df[\"session_id\"].isin(train_sessions)][TAB_FEATS].reset_index(drop=True)\n",
        "y_train = df[df[\"session_id\"].isin(train_sessions)][LABEL].reset_index(drop=True)\n",
        "X_val   = df[df[\"session_id\"].isin(val_sessions)][TAB_FEATS].reset_index(drop=True)\n",
        "y_val   = df[df[\"session_id\"].isin(val_sessions)][LABEL].reset_index(drop=True)\n",
        "\n",
        "# Parquet\n",
        "X_train.astype(\"int32\").to_parquet(DATA_DIR/\"X_train.parquet\", index=False)\n",
        "y_train.to_frame(name=\"y\").astype(\"int32\").to_parquet(DATA_DIR/\"y_train.parquet\", index=False)\n",
        "X_val.astype(\"int32\").to_parquet(DATA_DIR/\"X_val.parquet\", index=False)\n",
        "y_val.to_frame(name=\"y\").astype(\"int32\").to_parquet(DATA_DIR/\"y_val.parquet\", index=False)\n",
        "\n",
        "# CSV (옵션)\n",
        "X_train.astype(\"int32\").to_csv(DATA_DIR/\"X_train.csv\", index=False)\n",
        "y_train.to_frame(name=\"y\").astype(\"int32\").to_csv(DATA_DIR/\"y_train.csv\", index=False)\n",
        "X_val.astype(\"int32\").to_csv(DATA_DIR/\"X_val.csv\", index=False)\n",
        "y_val.to_frame(name=\"y\").astype(\"int32\").to_csv(DATA_DIR/\"y_val.csv\", index=False)\n",
        "\n",
        "print(\"[SAVE] parquet & csv ->\", DATA_DIR)\n",
        "print(\"  X_train:\", X_train.shape, \" | y_train:\", y_train.shape)\n",
        "print(\"  X_val  :\", X_val.shape,   \" | y_val  :\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) 세션 시퀀스 구성\n",
        "\n",
        "- 각 세션을 시간 순으로 정렬하고, 마지막 스텝을 제외한 구간을 입력/정답으로 매칭합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_session_sequences(frame):\n",
        "    seqs_in, seqs_prevpair, seqs_y = [], [], []\n",
        "    for sid, sub in frame.sort_values([\"session_id\"]).groupby(\"session_id\"):\n",
        "        x = sub[\"current_state_idx\"].to_numpy(dtype=np.int32)\n",
        "        y = sub[\"next_state_idx\"].to_numpy(dtype=np.int32)\n",
        "        p1 = sub[\"prev1_idx\"].to_numpy(dtype=np.int32)\n",
        "        p2 = sub[\"prev2_idx\"].to_numpy(dtype=np.int32)\n",
        "        if len(x) < 2:\n",
        "            continue\n",
        "        seqs_in.append(x[:-1])\n",
        "        seqs_y.append(y[:-1])\n",
        "        seqs_prevpair.append(np.stack([p2[:-1], p1[:-1]], axis=1))  # [T-1,2]\n",
        "    return seqs_in, seqs_prevpair, seqs_y\n",
        "\n",
        "train_in, train_prevpair, train_y = build_session_sequences(df[df[\"session_id\"].isin(train_sessions)])\n",
        "val_in,   val_prevpair,   val_y   = build_session_sequences(df[df[\"session_id\"].isin(val_sessions)])\n",
        "\n",
        "print(\"[INFO] #train seq:\", len(train_in), \" | #val seq:\", len(val_in))\n",
        "print(\"[INFO] sample lens:\", [len(train_in[i]) for i in range(min(3, len(train_in)))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Dataset / DataLoader\n",
        "\n",
        "- 패딩/마스크로 가변 길이 시퀀스를 처리합니다.\n",
        "- 배치 샘플을 CSV로 떨어뜨려 디버깅하기 쉽게 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class NextStateSeqDataset(Dataset):\n",
        "    def __init__(self, xs, pairs, ys):\n",
        "        self.xs = xs; self.pairs = pairs; self.ys = ys\n",
        "    def __len__(self): return len(self.xs)\n",
        "    def __getitem__(self, i): return self.xs[i], self.pairs[i], self.ys[i]\n",
        "\n",
        "def pad_collate(batch, pad_id=PAD_ID):\n",
        "    xs, ps, ys = zip(*batch)\n",
        "    L = max(len(x) for x in xs)\n",
        "    x_pad = np.full((len(xs), L), pad_id, dtype=np.int64)\n",
        "    y_pad = np.full((len(xs), L), pad_id, dtype=np.int64)\n",
        "    mask  = np.zeros((len(xs), L), dtype=np.bool_)\n",
        "    pair_pad = np.full((len(xs), L, 2), pad_id, dtype=np.int64)\n",
        "    for i,(x,p,y) in enumerate(zip(xs,ps,ys)):\n",
        "        l = len(x)\n",
        "        x_pad[i,:l] = x; y_pad[i,:l] = y; mask[i,:l] = True; pair_pad[i,:l,:] = p\n",
        "    return (torch.from_numpy(x_pad), torch.from_numpy(pair_pad),\n",
        "            torch.from_numpy(y_pad), torch.from_numpy(mask))\n",
        "\n",
        "train_ds = NextStateSeqDataset(train_in, train_prevpair, train_y)\n",
        "val_ds   = NextStateSeqDataset(val_in,   val_prevpair,   val_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=HYPERPARAMS[\"batch_size\"], shuffle=True,  collate_fn=pad_collate)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=HYPERPARAMS[\"batch_size\"], shuffle=False, collate_fn=pad_collate)\n",
        "\n",
        "NUM_CLASSES = num_states  # 중요: vocab_size와 동일\n",
        "xb, pb, yb, mb = next(iter(train_loader))\n",
        "print(\"[INFO] batch shapes:\", xb.shape, pb.shape, yb.shape, mb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.5) prev2/prev1 샘플 CSV 저장\n",
        "\n",
        "학습 배치에서 샘플을 떼서 빠르게 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === CSV로 prev2/prev1 확인 저장 ===\n",
        "import pandas as pd\n",
        "\n",
        "pb_np = pb.numpy()    # (B, L, 2)\n",
        "yb_np = yb.numpy()    # (B, L)\n",
        "mask_np = mb.numpy()  # (B, L)\n",
        "\n",
        "rows = []\n",
        "for i in range(pb_np.shape[0]):       # 배치 크기\n",
        "    for j in range(pb_np.shape[1]):   # 시퀀스 길이\n",
        "        if mask_np[i, j]:  # 유효 토큰만\n",
        "            prev2, prev1 = pb_np[i, j]\n",
        "            rows.append({\n",
        "                \"sample_id\": i,\n",
        "                \"step\": j,\n",
        "                \"prev2\": int(prev2),\n",
        "                \"prev1\": int(prev1),\n",
        "                \"target\": int(yb_np[i, j])\n",
        "            })\n",
        "\n",
        "df_sample = pd.DataFrame(rows)\n",
        "out_path = str(DATA_DIR / \"pre_pairs_sample.csv\")\n",
        "df_sample.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"[SAVE] prev2/prev1/target 샘플 -> {out_path} (rows={len(df_sample)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) 모델 정의 (LSTMClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=192, pair_dim=16, hid=384, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.num_classes = vocab_size\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
        "        self.pair_proj = nn.Linear(2, pair_dim)\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim + pair_dim,\n",
        "                            hidden_size=hid, num_layers=num_layers,\n",
        "                            dropout=dropout if num_layers > 1 else 0.0,\n",
        "                            batch_first=True, bidirectional=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.head = nn.Linear(hid, vocab_size)\n",
        "\n",
        "    def forward(self, x, pair, mask):\n",
        "        xe = self.emb(x)                  # [B,T,E]\n",
        "        pe = self.pair_proj(pair.float()) # [B,T,P]\n",
        "        inp = torch.cat([xe, pe], dim=-1) # [B,T,E+P]\n",
        "        out, _ = self.lstm(inp)           # [B,T,H]\n",
        "        out = self.drop(out)\n",
        "        logits = self.head(out)           # [B,T,C]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) 클래스 불균형 가중치\n",
        "\n",
        "- 학습 세트 라벨 분포 기반 inverse-frequency 가중치를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "label_counts = Counter(pd.Series(y_train).tolist())\n",
        "total = sum(label_counts.values())\n",
        "\n",
        "weights = np.ones(NUM_CLASSES, dtype=np.float32)\n",
        "for k, v in label_counts.items():\n",
        "    weights[k] = total / (len(label_counts) * v)  # inverse frequency\n",
        "\n",
        "class_weights = torch.tensor(weights, device=device)\n",
        "print(\"[INFO] class_weights sample:\", {k: float(class_weights[k]) for k in list(label_counts.keys())[:8]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) 모델 초기화 & 드라이런\n",
        "\n",
        "- AMP(CUDA) 활성화 여부는 자동 판단합니다.\n",
        "- 전방 전달만 수행하여 입출력 텐서 형태를 검증합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "model = LSTMClassifier(\n",
        "    vocab_size=NUM_CLASSES,\n",
        "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
        "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
        "    hid=HYPERPARAMS[\"hid\"],\n",
        "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
        "    dropout=HYPERPARAMS[\"dropout\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=HYPERPARAMS[\"lr\"], weight_decay=HYPERPARAMS[\"weight_decay\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "scaler    = GradScaler(\"cuda\", enabled=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]))\n",
        "\n",
        "xb, pb, yb, mb = next(iter(train_loader))\n",
        "xb, pb, yb, mb = xb.to(device), pb.to(device), yb.to(device), mb.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(xb, pb, mb)\n",
        "    print(\"[DEBUG] logits shape:\", tuple(logits.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) 학습/평가 루틴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, num_classes, class_weights=None, use_amp=True, grad_clip_norm=1.0, scaler=None):\n",
        "    model.train()\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
        "    correct = 0; total = 0; last_loss = 0.0\n",
        "\n",
        "    for x, pair, y, mask in loader:\n",
        "        x, pair, y, mask = x.to(device), pair.to(device), y.to(device), mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if use_amp and scaler is not None:\n",
        "            with torch.amp.autocast(device_type=\"cuda\"):\n",
        "                logits = model(x, pair, mask)\n",
        "                loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
        "            scaler.scale(loss).backward()\n",
        "            if grad_clip_norm:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(x, pair, mask)\n",
        "            loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
        "            loss.backward()\n",
        "            if grad_clip_norm:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        last_loss = float(loss.item())\n",
        "        with torch.no_grad():\n",
        "            pred  = logits.argmax(-1)\n",
        "            valid = (mask & (y != PAD_ID))\n",
        "            correct += (pred[valid] == y[valid]).sum().item()\n",
        "            total   += valid.sum().item()\n",
        "\n",
        "    return {\"loss\": last_loss, \"acc\": correct / max(total, 1)}\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, device, num_classes, class_weights=None):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
        "    total_valid = 0; top1_correct = 0; top3_correct = 0\n",
        "    sum_loss = 0.0; n_batches = 0\n",
        "    all_preds, all_trues = [], []\n",
        "\n",
        "    for x, pair, y, mask in loader:\n",
        "        x, pair, y, mask = x.to(device), pair.to(device), y.to(device), mask.to(device)\n",
        "        logits = model(x, pair, mask)\n",
        "        loss   = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
        "        sum_loss += float(loss.item()); n_batches += 1\n",
        "\n",
        "        prob = logits.softmax(-1); pred = prob.argmax(-1)\n",
        "        valid = (mask & (y != PAD_ID))\n",
        "        top1_correct += (pred[valid] == y[valid]).sum().item()\n",
        "        total_valid  += valid.sum().item()\n",
        "\n",
        "        _, topk_idx = prob.topk(3, dim=-1)\n",
        "        top3_correct += (topk_idx[valid].eq(y[valid].unsqueeze(-1))).any(dim=-1).sum().item()\n",
        "\n",
        "        all_preds.extend(pred[valid].detach().cpu().tolist())\n",
        "        all_trues.extend(y[valid].detach().cpu().tolist())\n",
        "\n",
        "    macro_f1 = f1_score(all_trues, all_preds, average=\"macro\") if all_trues else 0.0\n",
        "    return {\n",
        "        \"loss\": (sum_loss / max(n_batches, 1)),\n",
        "        \"top1\": (top1_correct / max(total_valid, 1)),\n",
        "        \"top3\": (top3_correct / max(total_valid, 1)),\n",
        "        \"macro_f1\": macro_f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) 기본 학습 루프 (조기 종료/체크포인트)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "best, patience, bad = None, HYPERPARAMS[\"patience\"], 0\n",
        "\n",
        "for epoch in range(HYPERPARAMS[\"epochs\"]):\n",
        "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
        "                         num_classes=NUM_CLASSES, class_weights=class_weights,\n",
        "                         use_amp=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]),\n",
        "                         grad_clip_norm=HYPERPARAMS[\"grad_clip_norm\"], scaler=scaler)\n",
        "    ev = eval_epoch(model,  val_loader,   device,\n",
        "                    num_classes=NUM_CLASSES, class_weights=class_weights)\n",
        "\n",
        "    # 스케줄러(plateau) — 최대화 기준: top1\n",
        "    scheduler.step(ev[\"top1\"])\n",
        "\n",
        "    print(f\"[E{epoch:02d}] \"\n",
        "          f\"train loss={tr['loss']:.4f} acc={tr['acc']:.4f} | \"\n",
        "          f\"val loss={ev['loss']:.4f} top1={ev['top1']:.4f} \"\n",
        "          f\"top3={ev['top3']:.4f} macroF1={ev['macro_f1']:.4f}\")\n",
        "\n",
        "    if not best or ev[\"top1\"] > best[\"top1\"]:\n",
        "        best, bad = {\"epoch\": epoch, **ev}, 0\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"metrics\": ev,\n",
        "            \"num_classes\": NUM_CLASSES,\n",
        "            \"pad_id\": PAD_ID,\n",
        "            \"hparams\": HYPERPARAMS,\n",
        "        }, str(BEST_PATH))\n",
        "        print(f\"[SAVE] best -> {BEST_PATH}  (epoch={epoch}, top1={ev['top1']:.4f})\")\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping at epoch\", epoch)\n",
        "            break\n",
        "\n",
        "print(\"[BEST]\", best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Optuna 하이퍼파라미터 탐색 (Top-1 최대화)\n",
        "\n",
        "- 3 epoch 짧은 예열 후 검증 성능으로 비교합니다.\n",
        "- 탐색 결과는 바로 다음 셀에서 재학습에 반영됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import optuna\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "def objective(trial):\n",
        "    # 탐색 공간\n",
        "    emb_dim    = trial.suggest_int(\"emb_dim\", 96, 256, step=32)\n",
        "    pair_dim   = trial.suggest_int(\"pair_dim\", 8, 32, step=8)\n",
        "    hid        = trial.suggest_int(\"hid\", 128, 512, step=64)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    dropout    = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    lr         = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
        "    wd         = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    m = LSTMClassifier(\n",
        "        vocab_size=NUM_CLASSES, emb_dim=emb_dim, pair_dim=pair_dim,\n",
        "        hid=hid, num_layers=num_layers, dropout=dropout\n",
        "    ).to(device)\n",
        "    opt = AdamW(m.parameters(), lr=lr, weight_decay=wd)\n",
        "    scaler_tmp = GradScaler(\"cuda\", enabled=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]))\n",
        "\n",
        "    # 짧게 예열\n",
        "    for _ in range(3):\n",
        "        train_one_epoch(m, train_loader, opt, device, NUM_CLASSES, class_weights=class_weights,\n",
        "                        use_amp=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]),\n",
        "                        grad_clip_norm=HYPERPARAMS[\"grad_clip_norm\"], scaler=scaler_tmp)\n",
        "    ev = eval_epoch(m, val_loader, device, NUM_CLASSES, class_weights=class_weights)\n",
        "    return ev[\"top1\"]\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=\"next_state_lstm_top1\")\n",
        "study.optimize(objective, n_trials=15, show_progress_bar=True)\n",
        "\n",
        "print(\"[BEST VALUE]\", study.best_value)\n",
        "print(\"[BEST PARAMS]\")\n",
        "for k, v in study.best_trial.params.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17) 최종 재학습 (Optuna 베스트 적용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, torch\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import GradScaler\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'study' in globals(), \"Optuna study가 없습니다. 먼저 상단 셀(Optuna)을 실행하세요.\"\n",
        "assert 'NUM_CLASSES' in globals(), \"NUM_CLASSES가 필요합니다.\"\n",
        "assert 'PAD_ID' in globals(), \"PAD_ID가 필요합니다.\"\n",
        "assert 'HYPERPARAMS' in globals(), \"HYPERPARAMS 딕셔너리가 필요합니다.\"\n",
        "assert 'train_loader' in globals() and 'val_loader' in globals(), \"train/val DataLoader가 필요합니다.\"\n",
        "assert 'train_one_epoch' in globals() and 'eval_epoch' in globals(), \"train_one_epoch / eval_epoch 함수가 필요합니다.\"\n",
        "assert 'LSTMClassifier' in globals(), \"LSTMClassifier 클래스가 필요합니다.\"\n",
        "\n",
        "device = globals().get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BEST_PATH = globals().get('BEST_PATH', './best_model.pt')\n",
        "class_weights = globals().get('class_weights', None)\n",
        "\n",
        "# 1) Optuna best 적용\n",
        "best_params = study.best_trial.params\n",
        "HYPERPARAMS.update({\n",
        "    \"emb_dim\":      best_params[\"emb_dim\"],\n",
        "    \"pair_dim\":     best_params[\"pair_dim\"],\n",
        "    \"hid\":          best_params[\"hid\"],\n",
        "    \"num_layers\":   best_params[\"num_layers\"],\n",
        "    \"dropout\":      best_params[\"dropout\"],\n",
        "    \"lr\":           best_params[\"lr\"],\n",
        "    \"weight_decay\": best_params[\"weight_decay\"],\n",
        "})\n",
        "print(\"[INFO] HYPERPARAMS updated with Optuna best:\", HYPERPARAMS)\n",
        "\n",
        "# (옵션) 저장\n",
        "BEST_JSON = (Path(BEST_PATH).parent / \"best_hparams.json\")\n",
        "with open(BEST_JSON, \"w\") as f:\n",
        "    json.dump({\"optuna_best\": best_params, \"hparams\": HYPERPARAMS}, f, indent=2)\n",
        "print(f\"[SAVE] best_hparams -> {BEST_JSON}\")\n",
        "\n",
        "# 2) 모델/옵티마이저/스케줄러 준비\n",
        "USE_AMP   = (device == \"cuda\") and bool(HYPERPARAMS.get(\"use_amp\", True))\n",
        "EPOCHS    = int(HYPERPARAMS.get(\"epochs\", 50))\n",
        "PATIENCE  = int(HYPERPARAMS.get(\"patience\", 3))\n",
        "GRAD_CLIP = float(HYPERPARAMS.get(\"grad_clip_norm\", 1.0))\n",
        "\n",
        "model = LSTMClassifier(\n",
        "    vocab_size=NUM_CLASSES,\n",
        "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
        "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
        "    hid=HYPERPARAMS[\"hid\"],\n",
        "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
        "    dropout=HYPERPARAMS[\"dropout\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=HYPERPARAMS[\"lr\"], weight_decay=HYPERPARAMS[\"weight_decay\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "scaler    = GradScaler(\"cuda\", enabled=USE_AMP)\n",
        "\n",
        "# 3) 본 학습\n",
        "best, bad = None, 0\n",
        "for epoch in range(EPOCHS):\n",
        "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
        "                         num_classes=NUM_CLASSES, class_weights=class_weights,\n",
        "                         use_amp=USE_AMP, grad_clip_norm=GRAD_CLIP, scaler=scaler)\n",
        "    ev = eval_epoch(model,  val_loader,   device,\n",
        "                    num_classes=NUM_CLASSES, class_weights=class_weights)\n",
        "\n",
        "    scheduler.step(ev[\"top1\"])\n",
        "\n",
        "    print(f\"[E{epoch:02d}] train loss={tr['loss']:.4f} acc={tr['acc']:.4f} | \"\n",
        "          f\"val loss={ev['loss']:.4f} top1={ev['top1']:.4f} top3={ev['top3']:.4f} macroF1={ev['macro_f1']:.4f}\")\n",
        "\n",
        "    if not best or ev[\"top1\"] > best[\"top1\"]:\n",
        "        best, bad = {\"epoch\": epoch, **ev}, 0\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"metrics\": ev,\n",
        "            \"num_classes\": NUM_CLASSES,\n",
        "            \"pad_id\": PAD_ID,\n",
        "            \"hparams\": HYPERPARAMS,\n",
        "        }, str(BEST_PATH))\n",
        "        print(f\"[SAVE] best -> {BEST_PATH}  (epoch={epoch}, top1={ev['top1']:.4f})\")\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"Early stopping at epoch\", epoch)\n",
        "            break\n",
        "\n",
        "print(\"\\n[BEST FINAL]\", best)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
