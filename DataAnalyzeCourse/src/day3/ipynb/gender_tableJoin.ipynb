{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category 테이블 Mapping 및 Table join\n",
    "\n",
    "성별 추론 실습 전 필요한 Mapping 테이블을 생성하여 원본 테이블과 Join하는 예제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0af520-2cb9-4855-b108-25b923beb4c7",
   "metadata": {},
   "source": [
    "## 1) 공통 셋업 & 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b3035-850d-4152-a091-3c8ea707bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, typing as T, re, json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BASE_URL = \"http://61.109.236.101\"\n",
    "\n",
    "OUT_DIR = Path(\"datasets\") / \"gender\"; OUT_DIR.mkdir(exist_ok=True)\n",
    "PRODUCT_MAP_PARQUET = OUT_DIR / \"product_category_map.parquet\"\n",
    "PRODUCT_MAP_CSV     = OUT_DIR / \"product_category_map.csv\"\n",
    "KEYWORD_MAP_CSV     = OUT_DIR / \"keyword_to_category_map.csv\"\n",
    "KEYWORD_MAP_PARQUET = OUT_DIR / \"keyword_to_category_map.parquet\"\n",
    "DATASET_PATH = \"datasets/processed_user_behavior.sorted.csv\"\n",
    "JOIN_OUT_CSV = OUT_DIR / \"processed_user_behavior.joined.csv\"\n",
    "JOIN_OUT_PQ  = OUT_DIR / \"processed_user_behavior.joined.parquet\"\n",
    "\n",
    "TIMEOUT = 5\n",
    "SLEEP   = 0.15\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "def make_sess() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    r = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.4,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"],\n",
    "    )\n",
    "    a = HTTPAdapter(max_retries=r, pool_connections=10, pool_maxsize=32)\n",
    "    s.mount(\"http://\", a); s.mount(\"https://\", a)\n",
    "    s.headers.update({\"User-Agent\": \"cat-mapper/1.0\", \"Accept\": \"text/html,*/*;q=0.8\"})\n",
    "    return s\n",
    "\n",
    "S = make_sess()\n",
    "\n",
    "def bs_text(html: str) -> str:\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "\n",
    "def parse_category_from_html(html: str) -> T.Optional[str]:\n",
    "    text = bs_text(html)\n",
    "    m = re.search(r\"Category\\s*:\\s*(.+)\", text, flags=re.I)\n",
    "    if m:\n",
    "        cat = m.group(1)\n",
    "        for stop in [\"Average Rating:\", \"Reviews:\", \"Price:\", \"ID:\", \"Name:\", \"[Home]\"]:\n",
    "            if stop in cat:\n",
    "                cat = cat.split(stop)[0]\n",
    "        cat = cat.splitlines()[0].strip()\n",
    "        return cat or None\n",
    "    for ln in text.splitlines():\n",
    "        if ln.lower().startswith(\"category:\"):\n",
    "            return ln.split(\":\", 1)[1].strip() or None\n",
    "    return None\n",
    "\n",
    "def product_category(pid: T.Union[int, str]) -> T.Optional[str]:\n",
    "    \"\"\"GET /product?id=<pid> → category 문자열 또는 None\"\"\"\n",
    "    url = f\"{BASE_URL}/product\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            time.sleep(SLEEP if attempt else 0)\n",
    "            r = S.get(url, params={\"id\": pid}, timeout=TIMEOUT)\n",
    "            if not r.encoding:\n",
    "                r.encoding = \"utf-8\"\n",
    "            if r.status_code == 404:\n",
    "                return None\n",
    "            r.raise_for_status()\n",
    "            return parse_category_from_html(r.text)\n",
    "        except Exception:\n",
    "            if attempt < 2:\n",
    "                time.sleep(0.5 * (2 ** attempt))  # 0.5s → 1s\n",
    "                continue\n",
    "            return None\n",
    "\n",
    "PROD_RE = re.compile(r\"/product\\?id=(\\d+)\", re.I)\n",
    "\n",
    "def search_ids(q: str, max_products: int = 5) -> list[int]:\n",
    "    \"\"\"GET /search?query=q → product id 리스트(최대 max_products)\"\"\"\n",
    "    time.sleep(SLEEP)\n",
    "    r = S.get(f\"{BASE_URL}/search\", params={\"query\": q}, timeout=TIMEOUT)\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "    ids: list[int] = []\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        m = PROD_RE.search(a[\"href\"])\n",
    "        if m:\n",
    "            ids.append(int(m.group(1)))\n",
    "    if not ids:\n",
    "        ids = [int(x) for x in PROD_RE.findall(r.text)] \n",
    "    ids = list(dict.fromkeys(ids))\n",
    "    return ids[:max_products]\n",
    "\n",
    "def clean_cat(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return re.sub(r\"</?[^>]+>\", \"\", str(x)).strip()\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21902077-c9ff-472d-917d-3cd56a860897",
   "metadata": {},
   "source": [
    "## 2) Product -> Category Mapping 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5622aa0-19a7-4f07-a794-7579fc94aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ids = [str(i) for i in range(101, 125)]\n",
    "\n",
    "if PRODUCT_MAP_PARQUET.exists():\n",
    "    existing = pd.read_parquet(PRODUCT_MAP_PARQUET)\n",
    "    if \"category_name\" in existing.columns and \"category\" not in existing.columns:\n",
    "        existing = existing.rename(columns={\"category_name\": \"category\"})\n",
    "    existing = existing[[\"product_id\", \"category\"]]\n",
    "else:\n",
    "    existing = pd.DataFrame(columns=[\"product_id\", \"category\"])\n",
    "\n",
    "have = set(existing[\"product_id\"].astype(str))\n",
    "todo = [pid for pid in product_ids if pid not in have]\n",
    "print(f\"[plan] total={len(product_ids)} have={len(have)} to_fetch={len(todo)}\")\n",
    "\n",
    "rows = []\n",
    "if todo:\n",
    "    def fetch_one(pid: str) -> dict:\n",
    "        cat = product_category(pid)\n",
    "        return {\"product_id\": pid, \"category\": cat}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(fetch_one, pid): pid for pid in todo}\n",
    "        for fut in as_completed(futs):\n",
    "            rows.append(fut.result())\n",
    "\n",
    "new_df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=[\"product_id\", \"category\"])\n",
    "\n",
    "merged_all = pd.concat([existing, new_df], ignore_index=True)\n",
    "if not merged_all.empty:\n",
    "    merged_all[\"is_valid\"] = merged_all[\"category\"].notna().astype(int)\n",
    "    product_map_df = (\n",
    "        merged_all.sort_values([\"product_id\", \"is_valid\"], ascending=[True, False])\n",
    "        .drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "        .drop(columns=[\"is_valid\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    product_map_df = merged_all\n",
    "\n",
    "product_map_df.to_parquet(PRODUCT_MAP_PARQUET, index=False)\n",
    "product_map_df.to_csv(PRODUCT_MAP_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"[done] saved:\\n  - {PRODUCT_MAP_PARQUET} (rows={len(product_map_df)})\\n  - {PRODUCT_MAP_CSV}\")\n",
    "\n",
    "product_map_df.head(24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561f1dc-b4b5-4e5e-80e4-86b519286621",
   "metadata": {},
   "source": [
    "## 3) Keyword -> Category Mapping 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ab1ce-17bc-47cf-ab7e-b43d6eda7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLENC_RE = re.compile(r\"%[0-9A-Fa-f]{2}\")\n",
    "df_src = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "if \"search_keyword\" in df_src.columns:\n",
    "    raw = df_src[\"search_keyword\"].dropna().astype(str).str.strip()\n",
    "    raw = raw[(raw != \"\") & (~raw.str.contains(URLENC_RE))]\n",
    "else:\n",
    "    raw = pd.Series(dtype=str)\n",
    "\n",
    "canon = raw.str.lower()\n",
    "freq = (\n",
    "    pd.DataFrame({\"canon\": canon, \"orig\": raw})\n",
    "    .groupby([\"canon\", \"orig\"]).size()\n",
    "    .reset_index(name=\"cnt\")\n",
    "    .sort_values([\"canon\", \"cnt\"], ascending=[True, False])\n",
    ")\n",
    "variants_map = freq.groupby(\"canon\")[\"orig\"].apply(list).to_dict()\n",
    "canons = sorted(variants_map.keys())\n",
    "print(\"키워드 수:\", len(canons))\n",
    "\n",
    "MAX_PRODUCTS_PER_KEYWORD = 5\n",
    "\n",
    "def infer_for_canon(canon_kw: str):\n",
    "    tried = []\n",
    "    tried += variants_map.get(canon_kw, [])\n",
    "    for q in [canon_kw.title(), canon_kw, canon_kw.upper()]:\n",
    "        if q not in tried:\n",
    "            tried.append(q)\n",
    "\n",
    "    picked_ids = []\n",
    "    for q in tried:\n",
    "        picked_ids = search_ids(q, max_products=MAX_PRODUCTS_PER_KEYWORD)\n",
    "        if picked_ids:\n",
    "            break\n",
    "\n",
    "    counts = Counter(); checked = 0\n",
    "    for pid in picked_ids:\n",
    "        cat = product_category(pid)\n",
    "        if cat:\n",
    "            counts[cat] += 1\n",
    "            checked += 1\n",
    "\n",
    "    if checked == 0:\n",
    "        return {\"keyword\": canon_kw, \"top_category\": None}\n",
    "\n",
    "    top_cat, _ = counts.most_common(1)[0]\n",
    "    return {\"keyword\": canon_kw, \"top_category\": top_cat}\n",
    "\n",
    "rows = []\n",
    "for i, ckw in enumerate(canons, 1):\n",
    "    rows.append(infer_for_canon(ckw))\n",
    "    if i % 20 == 0 or i == len(canons):\n",
    "        print(f\"[{i}/{len(canons)}] 진행 중…\")\n",
    "\n",
    "kw_map_df = (\n",
    "    pd.DataFrame(rows)[[\"keyword\", \"top_category\"]]\n",
    "    .sort_values([\"top_category\", \"keyword\"], na_position=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "kw_map_df.to_csv(KEYWORD_MAP_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "kw_map_df.to_parquet(KEYWORD_MAP_PARQUET, index=False)\n",
    "print(f\"[done] keyword map saved:\\n  - {KEYWORD_MAP_CSV}\\n  - {KEYWORD_MAP_PARQUET}\")\n",
    "\n",
    "kw_map_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefc1a6-d863-4ac0-8409-50eecaf53ad8",
   "metadata": {},
   "source": [
    "## 4) Mapping 테이블들과 원본 테이블 Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aa1aa-388b-4154-9291-d016c9d45970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "kw_map = pd.read_csv(KEYWORD_MAP_CSV).rename(columns={\"top_category\": \"kw_category\"})\n",
    "kw_map[\"keyword\"] = kw_map[\"keyword\"].astype(str).str.strip().str.lower()\n",
    "kw_series = kw_map.drop_duplicates(subset=[\"keyword\"]).set_index(\"keyword\")[\"kw_category\"]\n",
    "\n",
    "prod_map = pd.read_csv(PRODUCT_MAP_CSV)\n",
    "if \"category\" in prod_map.columns:\n",
    "    prod_map = prod_map.rename(columns={\"category\": \"prod_category\"})\n",
    "elif \"category_name\" in prod_map.columns:\n",
    "    prod_map = prod_map.rename(columns={\"category_name\": \"prod_category\"})\n",
    "elif \"top_category\" in prod_map.columns:\n",
    "    prod_map = prod_map.rename(columns={\"top_category\": \"prod_category\"})\n",
    "else:\n",
    "    raise ValueError(\"product_category_map.csv 에 category/category_name/top_category 중 하나가 필요합니다.\")\n",
    "\n",
    "prod_map[\"product_id\"] = pd.to_numeric(prod_map[\"product_id\"], errors=\"coerce\")\n",
    "prod_map = prod_map.dropna(subset=[\"product_id\"]).drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "prod_series = prod_map.set_index(\"product_id\")[\"prod_category\"]\n",
    "\n",
    "if \"product_id\" in df.columns:\n",
    "    pid_key = pd.to_numeric(df[\"product_id\"], errors=\"coerce\")\n",
    "    prod_hit = pid_key.map(prod_series)\n",
    "else:\n",
    "    prod_hit = pd.Series([pd.NA] * len(df), index=df.index)\n",
    "\n",
    "if \"search_keyword\" in df.columns:\n",
    "    kw_key = df[\"search_keyword\"].astype(str).str.strip().str.lower()\n",
    "    kw_hit = kw_key.map(kw_series)\n",
    "else:\n",
    "    kw_hit = pd.Series([pd.NA] * len(df), index=df.index)\n",
    "\n",
    "resolved = prod_hit.combine_first(kw_hit)\n",
    "if \"category_name\" in df.columns:\n",
    "    resolved = resolved.combine_first(df[\"category_name\"])\n",
    "df[\"resolved_category\"] = resolved.map(clean_cat)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "if \"product_id\" in cols:\n",
    "    insert_at = cols.index(\"product_id\") + 1\n",
    "    cols = [c for c in cols if c != \"resolved_category\"]\n",
    "    cols = cols[:insert_at] + [\"resolved_category\"] + cols[insert_at:]\n",
    "    df = df[cols]\n",
    "\n",
    "df.to_csv(JOIN_OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV 저장: {JOIN_OUT_CSV} (rows={len(df)})\")\n",
    "try:\n",
    "    df.to_parquet(JOIN_OUT_PQ, index=False)\n",
    "    print(f\"[OK] Parquet 저장: {JOIN_OUT_PQ}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Parquet 저장 실패:\", e, \"→ pyarrow 또는 fastparquet 설치 필요\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
