{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f310c0-72a0-4ef9-bc09-2570e62f1a84",
   "metadata": {},
   "source": [
    "# 성별 추론 모델 파이프라인\n",
    "### 사용자의 로그인 전 행동 패턴을 보고 사용자의 성별을 추론하는 머신 러닝 파이프라인 실습입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24fcd54-0d9c-41a4-9025-b766172e5115",
   "metadata": {},
   "source": [
    "## 0) 환경 구성\n",
    "- 파이프라인을 가동하기 위한 패키지를 설치하고 컴포넌트와 파이프라인을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605938c-672c-4432-8a3c-4206f474e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0\n",
    "import kfp, sys\n",
    "print(\"KFP:\", kfp.__version__, \"Python:\", sys.version)\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics, Markdown, HTML\n",
    "\n",
    "from typing import get_type_hints\n",
    "\n",
    "def _eval_annotations(fn):\n",
    "    try:\n",
    "        # globalns를 넘겨줘야 Dataset/Model 같은 심볼을 올바르게 해석\n",
    "        fn.__annotations__ = get_type_hints(fn, globalns=fn.__globals__)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fn\n",
    "\n",
    "def component2(**kwargs):\n",
    "    def _decorator(fn):\n",
    "        fn = _eval_annotations(fn)\n",
    "        return dsl.component(**kwargs)(fn)\n",
    "    return _decorator\n",
    "\n",
    "def pipeline2(**kwargs):\n",
    "    def _decorator(fn):\n",
    "        fn = _eval_annotations(fn)\n",
    "        return dsl.pipeline(**kwargs)(fn)\n",
    "    return _decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533209-2e2a-4b3a-b29a-aede31fc39f7",
   "metadata": {},
   "source": [
    "## 1) 공통 파라미터 초기화 컴포넌트\n",
    "- 파이프라인에서 사용하는 공통 파라미터들을 초기화하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abfb17-38e0-4a5c-9b65-fd89aa427611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Init & Constants (OutputPath 버전)\n",
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath  # <- 경로로 직접 받음\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\n",
    "    \"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\",\n",
    "    \"joblib==1.4.2\", \"matplotlib==3.9.0\", \"pyarrow==17.0.0\",\n",
    "]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell1_init(\n",
    "    known_cats_json: str,\n",
    "    production_threshold: float,\n",
    "    min_prelogin_events: int,\n",
    "    datasets_dir: str,\n",
    "    artifact_dir: str,\n",
    "    params_out_path: OutputPath(str),\n",
    "):\n",
    "    import json, unicodedata\n",
    "    from pathlib import Path\n",
    "\n",
    "    KNOWN_CATS = [unicodedata.normalize(\"NFKC\", c.strip()) for c in json.loads(known_cats_json)]\n",
    "    Path(datasets_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(artifact_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    blob = {\n",
    "        \"KNOWN_CATS\": KNOWN_CATS,\n",
    "        \"PRODUCTION_THRESHOLD\": float(production_threshold),\n",
    "        \"MIN_PRELOGIN_EVENTS\": int(min_prelogin_events),\n",
    "        \"DATASETS_DIR\": datasets_dir,\n",
    "        \"ARTIFACT_DIR\": artifact_dir,\n",
    "    }\n",
    "    with open(params_out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(blob, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e276b-fc1c-4d1a-a30a-a812669e5ece",
   "metadata": {},
   "source": [
    "## 2) 로그인 전 구간 필터링 컴포넌트\n",
    "- PVC에 있는 Join된 원본 데이터를 읽고 로그인 전 구간을 필터링 하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc33073-61e1-4785-ae11-077c7ab9db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Read CSV, sort, pre-login slice -> df_out, pre_out, log_txt  (OutputPath 버전)\n",
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"pyarrow==17.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell2_prelogin_filter(\n",
    "    input_path: str,\n",
    "    min_prelogin_events: int,\n",
    "    df_out: OutputPath(\"Parquet\"),\n",
    "    pre_out: OutputPath(\"Parquet\"),\n",
    "    log_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(input_path, low_memory=False)\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"session_id\", \"ts\"]).copy()\n",
    "\n",
    "    sort_cols = [\"session_id\", \"ts\"]\n",
    "    if \"event_id\" in df.columns:\n",
    "        sort_cols.append(\"event_id\")\n",
    "    df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    uid_str = df[\"user_id\"].astype(\"string\").str.strip()\n",
    "    anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "    has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "    appeared = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "    pre = df.loc[~appeared].copy()\n",
    "\n",
    "    pre_cnt = pre.groupby(\"session_id\").size()\n",
    "    valid_sessions = pre_cnt.index[pre_cnt >= int(min_prelogin_events)]\n",
    "    pre = pre[pre[\"session_id\"].isin(valid_sessions)].copy()\n",
    "\n",
    "    df.to_parquet(df_out, index=False)\n",
    "    pre.to_parquet(pre_out, index=False)\n",
    "\n",
    "    with open(log_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"원본 행 수: {len(df)}\\n\")\n",
    "        f.write(f\"로그인 전 로그 필터 후 행 수: {len(pre)}\\n\")\n",
    "        f.write(f\"로그인 전 유일 세션 수: {pre['session_id'].nunique()}\\n\")\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fcd8aa-770b-4930-b5d8-f2d7403477d6",
   "metadata": {},
   "source": [
    "## 3) 피처 집계 & 성별 라벨링 컴포넌트\n",
    "- 로그인 전 로그를 세션 단위로 집계하고 성별 라벨을 붙여 학습용 데이터를 생성하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdd623-535a-4156-9879-47075649c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Aggregate numeric features + labels -> Xnum_out, y_out, keep_sessions_out, counts_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"pyarrow==17.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell3_aggregate_and_label(\n",
    "    df_in: InputPath(\"Parquet\"),\n",
    "    pre_in: InputPath(\"Parquet\"),\n",
    "    Xnum_out: OutputPath(\"Parquet\"),\n",
    "    y_out: OutputPath(\"CSV\"),\n",
    "    keep_sessions_out: OutputPath(str),\n",
    "    counts_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_parquet(df_in)\n",
    "    pre = pd.read_parquet(pre_in)\n",
    "\n",
    "    df_gender = df.copy()\n",
    "    df_gender[\"gender_norm\"] = (\n",
    "        df_gender[\"gender\"].astype(\"string\").str.strip().str.upper().replace({\"FEMALE\":\"F\", \"MALE\":\"M\"})\n",
    "    )\n",
    "    lab_full = (\n",
    "        df_gender[df_gender[\"gender_norm\"].isin([\"M\",\"F\"])]\n",
    "          .groupby(\"session_id\")[\"gender_norm\"].agg(lambda s: s.iloc[0])\n",
    "    )\n",
    "\n",
    "    agg_num = pre.groupby(\"session_id\").agg(\n",
    "        n_events=(\"session_id\", \"size\"),\n",
    "        search_count_sum=(\"search_count\", \"sum\"),\n",
    "        cart_item_count_sum=(\"cart_item_count\", \"sum\"),\n",
    "        page_depth_mean=(\"page_depth\", \"mean\"),\n",
    "        last_elapsed_mean=(\"last_action_elapsed\", \"mean\"),\n",
    "        unique_pages=(\"current_state\", \"nunique\"),\n",
    "        unique_categories=(\"resolved_category\", \"nunique\"),\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    first_ts = pd.to_datetime(pre.groupby(\"session_id\")[\"ts\"].min(), errors=\"coerce\")\n",
    "    agg_num[\"start_hour\"] = first_ts.dt.hour\n",
    "    agg_num[\"start_weekday\"] = first_ts.dt.weekday\n",
    "\n",
    "    keep_sessions = agg_num.index.intersection(lab_full.index)\n",
    "    X_num = agg_num.loc[keep_sessions].copy()\n",
    "    y = lab_full.loc[keep_sessions].rename(\"gender\").copy()\n",
    "\n",
    "    X_num.to_parquet(Xnum_out, index=True)\n",
    "    y.to_frame().to_csv(y_out, index=True, header=True)\n",
    "    pd.Series(keep_sessions, name=\"session_id\").to_csv(keep_sessions_out, index=False, header=False)\n",
    "\n",
    "    with open(counts_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"로그인 완료한 세션 수: {len(keep_sessions)}\\n\")\n",
    "        f.write(\"성별 분포:\\n\\n\")\n",
    "        f.write(y.value_counts(dropna=False).to_string())\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf8278-061f-4df9-b199-0bf174b0dd5e",
   "metadata": {},
   "source": [
    "## 4) 카운트 & 파생 컬럼 생성 컴포넌트\n",
    "- 카테고리 등장 횟수를 저장하는 카운트 컬럼, 횟수를 비율로 치환하는 파생 컬럼을 생성하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bfc2f-5ef2-4331-9868-3c4245bceeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Category count/prop/log features -> cat_cnt_out, cat_prop_out, cat_log_out, cats_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"pyarrow==17.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell4_category_features(\n",
    "    pre_in: InputPath(\"Parquet\"),\n",
    "    Xnum_in: InputPath(\"Parquet\"),\n",
    "    known_cats_json: str,\n",
    "    cat_cnt_out: OutputPath(\"Parquet\"),\n",
    "    cat_prop_out: OutputPath(\"Parquet\"),\n",
    "    cat_log_out: OutputPath(\"Parquet\"),\n",
    "    cats_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd, numpy as np, json, unicodedata\n",
    "\n",
    "    pre = pd.read_parquet(pre_in)\n",
    "    X_num = pd.read_parquet(Xnum_in)\n",
    "\n",
    "    KNOWN_CATS_NORM = [unicodedata.normalize(\"NFKC\", c.strip()) for c in json.loads(known_cats_json)]\n",
    "    norm_to_orig = {unicodedata.normalize(\"NFKC\", c.strip()): c for c in json.loads(known_cats_json)}\n",
    "\n",
    "    pre_cat_norm = pre[\"resolved_category\"].astype(\"string\").str.strip().apply(\n",
    "        lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x\n",
    "    )\n",
    "    mask = pre_cat_norm.isin(KNOWN_CATS_NORM)\n",
    "    pre_kept = pre[mask].copy()\n",
    "    pre_kept[\"cat_norm\"] = pre_cat_norm[mask].values\n",
    "    pre_kept[\"one\"] = 1\n",
    "\n",
    "    cat_cnt = pre_kept.pivot_table(index=\"session_id\", columns=\"cat_norm\", values=\"one\", aggfunc=\"sum\", fill_value=0)\n",
    "    cat_cnt = cat_cnt.reindex(columns=KNOWN_CATS_NORM, fill_value=0)\n",
    "    cat_cnt.columns = [norm_to_orig[c] for c in cat_cnt.columns]\n",
    "    cat_cnt.columns = [f\"cat_cnt::{c}\" for c in cat_cnt.columns]\n",
    "    cat_cnt = cat_cnt.reindex(X_num.index).fillna(0).astype(int)\n",
    "\n",
    "    cat_prop = cat_cnt.div(X_num[\"n_events\"].replace(0, 1), axis=0)\n",
    "    cat_prop.columns = [c.replace(\"cat_cnt::\", \"cat_prop::\") for c in cat_cnt.columns]\n",
    "\n",
    "    cat_log = np.log1p(cat_cnt)\n",
    "    cat_log.columns = [c.replace(\"cat_cnt::\", \"cat_log::\") for c in cat_cnt.columns]\n",
    "\n",
    "    cat_cnt.to_parquet(cat_cnt_out, index=True)\n",
    "    cat_prop.to_parquet(cat_prop_out, index=True)\n",
    "    cat_log.to_parquet(cat_log_out, index=True)\n",
    "\n",
    "    with open(cats_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"카테고리 카운트 컬럼:\\n\")\n",
    "        f.write(\", \".join(list(cat_cnt.columns)) + \"\\n\\n\")\n",
    "        f.write(\"카테고리 파생 컬럼 예시:\\n\")\n",
    "        f.write(\", \".join(list(cat_prop.columns[:3]) + list(cat_log.columns[:3])))\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fcd11-6447-4c41-a63e-208ae4d8ef01",
   "metadata": {},
   "source": [
    "## 5) 최종 학습 데이터 셋 생성 컴포넌트\n",
    "- 카운트 & 파생 컬럼과 라벨을 합쳐 최종 학습용 데이터 셋을 저장하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f0574-ea73-469d-877e-5c841ae28192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Join all features + y -> dataset_csv_out, dataset_pq_out, head_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"pyarrow==17.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell5_build_dataset(\n",
    "    Xnum_in: InputPath(\"Parquet\"),\n",
    "    cat_cnt_in: InputPath(\"Parquet\"),\n",
    "    cat_prop_in: InputPath(\"Parquet\"),\n",
    "    cat_log_in: InputPath(\"Parquet\"),\n",
    "    y_in: InputPath(\"CSV\"),\n",
    "    dataset_csv_out: OutputPath(\"CSV\"),\n",
    "    dataset_pq_out: OutputPath(\"Parquet\"),\n",
    "    head_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    X_num = pd.read_parquet(Xnum_in)\n",
    "    cat_cnt = pd.read_parquet(cat_cnt_in)\n",
    "    cat_prop = pd.read_parquet(cat_prop_in)\n",
    "    cat_log = pd.read_parquet(cat_log_in)\n",
    "    y = pd.read_csv(y_in, index_col=0)[\"gender\"]\n",
    "\n",
    "    X = (X_num.join(cat_cnt, how=\"left\")\n",
    "              .join(cat_prop, how=\"left\")\n",
    "              .join(cat_log,  how=\"left\")).fillna(0)\n",
    "\n",
    "    X_ = X.copy(); X_.index.name = \"session_id\"\n",
    "    dataset = X_.join(y).reset_index()\n",
    "\n",
    "    dataset.to_csv(dataset_csv_out, index=False)\n",
    "    dataset.to_parquet(dataset_pq_out, index=False)\n",
    "\n",
    "    with open(head_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"dataset shape: {dataset.shape}\\n\\n\")\n",
    "        f.write(dataset.head(10).to_string(index=False))\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdec5bd-f19d-4a9f-8a94-5bee44e49145",
   "metadata": {},
   "source": [
    "## 6) 데이터 셋 분할 컴포넌트\n",
    "- 최종 데이터 셋을 학습용 / 검증용 데이터 셋으로 분할하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bab14-300d-4a63-aefe-2867b6334ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Stratified split -> Xtr_out, Xva_out, ytr_out, yva_out, split_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\", \"pyarrow==17.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell6_split(\n",
    "    dataset_pq_in: InputPath(\"Parquet\"),\n",
    "    random_state: int,\n",
    "    test_size: float,\n",
    "    Xtr_out: OutputPath(\"Parquet\"),\n",
    "    Xva_out: OutputPath(\"Parquet\"),\n",
    "    ytr_out: OutputPath(\"CSV\"),\n",
    "    yva_out: OutputPath(\"CSV\"),\n",
    "    split_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    dataset = pd.read_parquet(dataset_pq_in)\n",
    "    dataset[\"gender\"] = (\n",
    "        dataset[\"gender\"].astype(\"string\").str.strip().str.upper().replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    "    )\n",
    "\n",
    "    non_feature = {\"session_id\",\"gender\"}\n",
    "    feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "    X = dataset[feature_cols].copy()\n",
    "    y_bin = dataset[\"gender\"].map({\"F\":0, \"M\":1}).astype(int)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=int(random_state))\n",
    "    train_idx, valid_idx = next(sss.split(X, y_bin))\n",
    "\n",
    "    # ★ 인덱스 보존(후속 셀에서 매핑에 필요)\n",
    "    X.iloc[train_idx].to_parquet(Xtr_out, index=True)\n",
    "    X.iloc[valid_idx].to_parquet(Xva_out, index=True)\n",
    "    y_bin.iloc[train_idx].to_frame(\"y\").to_csv(ytr_out, index=False)\n",
    "    y_bin.iloc[valid_idx].to_frame(\"y\").to_csv(yva_out, index=False)\n",
    "\n",
    "    with open(split_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Train: {X.iloc[train_idx].shape}  Valid: {X.iloc[valid_idx].shape}\\n\")\n",
    "        f.write(\"Label dist (train):\\n\")\n",
    "        f.write(y_bin.iloc[train_idx].value_counts(normalize=True).rename({0:\"F\",1:\"M\"}).to_string() + \"\\n\\n\")\n",
    "        f.write(\"Label dist (valid):\\n\")\n",
    "        f.write(y_bin.iloc[valid_idx].value_counts(normalize=True).rename({0:\"F\",1:\"M\"}).to_string())\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26b6e7-9742-456b-bbe3-aea7d0939995",
   "metadata": {},
   "source": [
    "## 7) Logistic 모델 학습 컴포넌트\n",
    "- Logistic 모델을 학습하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fd680-1be0-4e17-9f04-7001c72140ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Train base Logistic & Calibrate -> cal_model_out, train_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\n",
    "    \"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\",\n",
    "    \"joblib==1.4.2\", \"pyarrow==17.0.0\"\n",
    "]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell7_train_calibrate(\n",
    "    Xtr_in: InputPath(\"Parquet\"),\n",
    "    ytr_in: InputPath(\"CSV\"),\n",
    "    calib_method: str,\n",
    "    calib_cv: int,\n",
    "    cal_model_out: OutputPath(\"Model\"),\n",
    "    train_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from joblib import dump\n",
    "\n",
    "    X_train = pd.read_parquet(Xtr_in)\n",
    "    y_train = pd.read_csv(ytr_in)[\"y\"].astype(int)\n",
    "\n",
    "    log_best = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", LogisticRegression(solver=\"saga\", penalty=\"l2\", C=1.0, max_iter=5000, random_state=42)),\n",
    "    ])\n",
    "    log_best.fit(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        cal_log = CalibratedClassifierCV(estimator=log_best, method=calib_method, cv=int(calib_cv))\n",
    "    except TypeError:\n",
    "        cal_log = CalibratedClassifierCV(base_estimator=log_best, method=calib_method, cv=int(calib_cv))\n",
    "\n",
    "    cal_log.fit(X_train, y_train)\n",
    "    dump(cal_log, cal_model_out)\n",
    "\n",
    "    with open(train_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[Calibration] done: method={calib_method}, cv={calib_cv}\\n\")\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c922c-9f51-407a-bb57-65982f8213ec",
   "metadata": {},
   "source": [
    "## 8) 모델 검증 및 평가 컴포넌트\n",
    "- 학습한 모델을 검증하여 평가하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db07324-3425-4baa-8111-537897dfb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Evaluate @ production threshold + age metrics -> proba_out, pred_out, age_metrics_out, eval_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\n",
    "    \"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\",\n",
    "    \"joblib==1.4.2\", \"pyarrow==17.0.0\"\n",
    "]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell8_eval_prod_threshold(\n",
    "    cal_model_in: InputPath(\"Model\"),\n",
    "    Xva_in: InputPath(\"Parquet\"),\n",
    "    yva_in: InputPath(\"CSV\"),\n",
    "    dataset_csv_in: InputPath(\"CSV\"),\n",
    "    production_threshold: float,\n",
    "    proba_out: OutputPath(\"CSV\"),\n",
    "    pred_out: OutputPath(\"CSV\"),\n",
    "    age_metrics_out: OutputPath(\"CSV\"),\n",
    "    eval_txt: OutputPath(str),\n",
    "):\n",
    "    import pandas as pd, numpy as np\n",
    "    from joblib import load\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "    cal_log = load(cal_model_in)\n",
    "    X_valid = pd.read_parquet(Xva_in)\n",
    "    y_valid = pd.read_csv(yva_in)[\"y\"].astype(int)\n",
    "\n",
    "    proba_v = cal_log.predict_proba(X_valid)[:, 1]\n",
    "    pred_bin = (proba_v >= float(production_threshold)).astype(int)\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_v}).to_csv(proba_out, index=False)\n",
    "    pd.DataFrame({\"pred\":  pred_bin}).to_csv(pred_out, index=False)\n",
    "\n",
    "    macro_f1 = f1_score(y_valid, pred_bin, average=\"macro\")\n",
    "    acc = (y_valid.values == pred_bin).mean()\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"=== LogisticCal (prod={production_threshold:.3f}) ===\")\n",
    "    lines.append(classification_report(y_valid, pred_bin, target_names=[\"F\",\"M\"], digits=4))\n",
    "    lines.append(\"Confusion matrix [rows=true F,M | cols=pred F,M]:\")\n",
    "    lines.append(str(confusion_matrix(y_valid, pred_bin)))\n",
    "    lines.append(f\"[LogisticCal (prod@{production_threshold:.3f})] Macro-F1 : {macro_f1:.4f} / Accuracy : {acc:.4f}\")\n",
    "\n",
    "    dataset = pd.read_csv(dataset_csv_in)\n",
    "    sess_ids_valid = dataset.iloc[X_valid.index][\"session_id\"].values\n",
    "    out_df = pd.DataFrame({\n",
    "        \"session_id\": sess_ids_valid,\n",
    "        \"y_true\": y_valid.values,\n",
    "        f\"y_pred@{production_threshold:.3f}\": pred_bin,\n",
    "        \"proba_cal\": proba_v,\n",
    "    })\n",
    "\n",
    "    extra_cols = [c for c in [\"age\", \"age_group\"] if c in dataset.columns]\n",
    "    if extra_cols:\n",
    "        out_df = out_df.merge(\n",
    "            dataset[[\"session_id\"] + extra_cols].drop_duplicates(\"session_id\"),\n",
    "            on=\"session_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    metrics_df = out_df.copy()\n",
    "    metrics_df[\"age_group\"] = metrics_df.get(\"age_group\", pd.Series([np.nan]*len(metrics_df))).astype(\"string\").fillna(\"unknown\")\n",
    "\n",
    "    rows = []\n",
    "    pred_col = [c for c in metrics_df.columns if c.startswith(\"y_pred@\")][0]\n",
    "    for g, sub in metrics_df.groupby(\"age_group\", dropna=False):\n",
    "        y_t = sub[\"y_true\"].astype(int).to_numpy()\n",
    "        y_h = sub[pred_col].astype(int).to_numpy()\n",
    "        acc_g = float((y_t == y_h).mean())\n",
    "        f1_g  = float(f1_score(y_t, y_h, average=\"macro\")) if len(np.unique(y_t)) > 1 else float(\"nan\")\n",
    "        rows.append({\"age_group\": str(g), \"acc\": acc_g, \"macro_f1\": f1_g})\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(age_metrics_out, index=False)\n",
    "    with open(eval_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cbf6a2-7ecf-4d37-b64f-d9e9c762edb4",
   "metadata": {},
   "source": [
    "## 9) Katib 최적 하이퍼파라미터 로드 컴포넌트\n",
    "- Katib에서 최적 하이퍼파라미터를 로드하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f3757-b77c-4e3a-b9c7-d0d376358bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Fetch Katib best params -> katib_params_json_out, katib_df_csv_out\n",
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_K8S = [\"numpy==1.26.4\", \"pandas==2.2.2\", \"pyarrow==17.0.0\", \"kubernetes==29.0.0\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_K8S)\n",
    "def cell9_fetch_katib(\n",
    "    katib_namespace: str,\n",
    "    katib_experiment: str,\n",
    "    katib_params_json_out: OutputPath(str),\n",
    "    katib_df_csv_out: OutputPath(\"CSV\"),\n",
    "):\n",
    "    import pandas as pd, json\n",
    "    from kubernetes import client, config\n",
    "\n",
    "    try:\n",
    "        config.load_incluster_config()\n",
    "    except Exception:\n",
    "        config.load_kube_config()\n",
    "\n",
    "    api = client.CustomObjectsApi()\n",
    "    obj = api.get_namespaced_custom_object(\n",
    "        group=\"kubeflow.org\", version=\"v1beta1\", plural=\"experiments\",\n",
    "        namespace=katib_namespace, name=katib_experiment\n",
    "    )\n",
    "\n",
    "    status = obj.get(\"status\", {})\n",
    "    trial = status.get(\"currentOptimalTrial\") or status.get(\"optimalTrial\") or status.get(\"bestTrial\") or {}\n",
    "    params = trial.get(\"parameterAssignments\", [])\n",
    "    if not params:\n",
    "        params = []\n",
    "\n",
    "    with open(katib_params_json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(params, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    pd.DataFrame(params).to_csv(katib_df_csv_out, index=False)\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb46a1-b90f-4245-b258-22d868ec2467",
   "metadata": {},
   "source": [
    "## 10) 모델 재학습 컴포넌트\n",
    "- katib에서 찾은 최적 하이퍼파라미터를 적용해 모델을 재학습하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50970a8-73f6-4f0e-a147-77ed0828e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Train with Katib HPs (+optional rebuild) -> cal_model_out, Xva_out, yva_out, dataset_out, train_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\n",
    "    \"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\",\n",
    "    \"joblib==1.4.2\", \"pyarrow==17.0.0\"\n",
    "]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell10_train_with_katib(\n",
    "    katib_params_json_in: str,         \n",
    "    input_path: str,\n",
    "    known_cats_json: str,\n",
    "    base_min_prelogin_events: int,\n",
    "    Xva_prev_in: InputPath(\"Parquet\"),\n",
    "    yva_prev_in: InputPath(\"CSV\"),\n",
    "    dataset_prev_in: InputPath(\"CSV\"),\n",
    "    cal_model_out: OutputPath(\"Model\"),\n",
    "    Xva_out: OutputPath(\"Parquet\"),\n",
    "    yva_out: OutputPath(\"CSV\"),\n",
    "    dataset_out: OutputPath(\"CSV\"),\n",
    "    train_txt: OutputPath(str),\n",
    "):\n",
    "    import json, os, unicodedata, numpy as np, pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    from joblib import dump\n",
    "\n",
    "    # --- Katib 파라미터 파싱 (문자열 or 경로 둘 다 안전 처리) ---\n",
    "    raw = katib_params_json_in or \"[]\"\n",
    "    try:\n",
    "        if raw.strip().startswith(\"[\"):\n",
    "            params = json.loads(raw)\n",
    "        elif os.path.exists(raw):\n",
    "            params = json.load(open(raw, \"r\", encoding=\"utf-8\"))\n",
    "        else:\n",
    "            params = []\n",
    "    except Exception:\n",
    "        params = []\n",
    "    K = {d[\"name\"]: str(d[\"value\"]).strip() for d in params} if params else {}\n",
    "\n",
    "    def _get_str(m, k, default=None):\n",
    "        v = m.get(k, default); return None if v is None else str(v).strip()\n",
    "    def _get_float(m, k, default=None):\n",
    "        v = m.get(k, None)\n",
    "        try: return float(v)\n",
    "        except: return default\n",
    "    def _get_int(m, k, default=None):\n",
    "        v = m.get(k, None)\n",
    "        try: return int(float(v))\n",
    "        except: return default\n",
    "\n",
    "    penalty   = (_get_str(K, \"penalty\", \"l2\") or \"l2\").lower()\n",
    "    C         = _get_float(K, \"C\", 1.0)\n",
    "    l1_ratio  = _get_float(K, \"l1_ratio\", None)\n",
    "    scaler_nm = (_get_str(K, \"scaler\", \"standard\") or \"standard\").lower()\n",
    "    cw_nm     = _get_str(K, \"class_weight\", None)\n",
    "    class_weight = None if (cw_nm is None or cw_nm.lower()==\"none\") else (\"balanced\" if cw_nm.lower()==\"balanced\" else cw_nm)\n",
    "\n",
    "    calib_method = (_get_str(K, \"calib_method\", \"isotonic\") or \"isotonic\").lower()\n",
    "    if calib_method not in {\"isotonic\",\"sigmoid\"}: calib_method = \"isotonic\"\n",
    "    calib_cv   = _get_int(K, \"calib_cv\", 5)\n",
    "\n",
    "    t_lo    = _get_float(K, \"tune_t_lo\", 0.2)\n",
    "    t_hi    = _get_float(K, \"tune_t_hi\", 0.8)\n",
    "    t_steps = _get_int (K, \"tune_t_steps\", 61)\n",
    "    if (t_lo is not None) and (t_hi is not None) and (t_lo > t_hi):\n",
    "        t_lo, t_hi = t_hi, t_lo\n",
    "\n",
    "    min_prelogin_events_k = _get_int(K, \"min_prelogin_events\", None)\n",
    "    need_rebuild = (min_prelogin_events_k is not None) and (int(min_prelogin_events_k) != int(base_min_prelogin_events))\n",
    "\n",
    "    def _norm(series: pd.Series) -> pd.Series:\n",
    "        s = series.astype(\"string\").str.strip()\n",
    "        return s.apply(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "\n",
    "    def _build_dataset_from_raw(input_path: str, min_events: int, known_cats_json: str):\n",
    "        df = pd.read_csv(input_path, low_memory=False)\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"session_id\",\"ts\"]).copy()\n",
    "        sort_cols = [\"session_id\",\"ts\"] + ([\"event_id\"] if \"event_id\" in df.columns else [])\n",
    "        df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        uid_str  = df[\"user_id\"].astype(\"string\").str.strip()\n",
    "        anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "        has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "        appeared = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "        pre = df.loc[~appeared].copy()\n",
    "\n",
    "        pre_cnt = pre.groupby(\"session_id\").size()\n",
    "        valid_sessions = pre_cnt.index[pre_cnt >= int(min_events)]\n",
    "        pre = pre[pre[\"session_id\"].isin(valid_sessions)].copy()\n",
    "\n",
    "        df_gender = df.copy()\n",
    "        df_gender[\"gender_norm\"] = (\n",
    "            df_gender.get(\"gender\", pd.Series(index=df_gender.index, dtype=\"object\")).astype(\"string\").str.strip().str.upper().replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    "        )\n",
    "        lab_full = df_gender[df_gender[\"gender_norm\"].isin([\"M\",\"F\"])].groupby(\"session_id\")[\"gender_norm\"].agg(lambda s: s.iloc[0])\n",
    "\n",
    "        agg_num = pre.groupby(\"session_id\").agg(\n",
    "            n_events=(\"session_id\",\"size\"),\n",
    "            search_count_sum=(\"search_count\",\"sum\") if \"search_count\" in pre.columns else (\"session_id\",\"size\"),\n",
    "            cart_item_count_sum=(\"cart_item_count\",\"sum\") if \"cart_item_count\" in pre.columns else (\"session_id\",\"size\"),\n",
    "            page_depth_mean=(\"page_depth\",\"mean\") if \"page_depth\" in pre.columns else (\"session_id\",\"size\"),\n",
    "            last_elapsed_mean=(\"last_action_elapsed\",\"mean\") if \"last_action_elapsed\" in pre.columns else (\"session_id\",\"size\"),\n",
    "            unique_pages=(\"current_state\",\"nunique\") if \"current_state\" in pre.columns else (\"session_id\",\"size\"),\n",
    "            unique_categories=(\"resolved_category\",\"nunique\") if \"resolved_category\" in pre.columns else (\"session_id\",\"size\"),\n",
    "        ).fillna(0.0)\n",
    "        first_ts = pre.groupby(\"session_id\")[\"ts\"].min()\n",
    "        agg_num[\"start_hour\"] = first_ts.dt.hour\n",
    "        agg_num[\"start_weekday\"] = first_ts.dt.weekday\n",
    "\n",
    "        import json\n",
    "        KNOWN_CATS = [unicodedata.normalize(\"NFKC\", c.strip()) for c in json.loads(known_cats_json)]\n",
    "        norm_to_orig = {unicodedata.normalize(\"NFKC\", c.strip()): c for c in json.loads(known_cats_json)}\n",
    "        pre_cat_norm = _norm(pre.get(\"resolved_category\", pd.Series(index=pre.index, dtype=\"object\")))\n",
    "        mask = pre_cat_norm.isin(KNOWN_CATS)\n",
    "        pre_kept = pre[mask].copy()\n",
    "        pre_kept[\"cat_norm\"] = pre_cat_norm[mask].values\n",
    "        pre_kept[\"one\"] = 1\n",
    "        cat_cnt = pre_kept.pivot_table(index=\"session_id\", columns=\"cat_norm\", values=\"one\", aggfunc=\"sum\", fill_value=0)\n",
    "        cat_cnt = cat_cnt.reindex(columns=KNOWN_CATS, fill_value=0)\n",
    "        cat_cnt.columns = [norm_to_orig[c] for c in cat_cnt.columns]\n",
    "        cat_cnt.columns = [f\"cat_cnt::{c}\" for c in cat_cnt.columns]\n",
    "        cat_cnt = cat_cnt.reindex(agg_num.index).fillna(0).astype(int)\n",
    "\n",
    "        cat_prop = cat_cnt.div(agg_num[\"n_events\"].replace(0,1), axis=0)\n",
    "        cat_prop.columns = [c.replace(\"cat_cnt::\",\"cat_prop::\") for c in cat_cnt.columns]\n",
    "        cat_log = np.log1p(cat_cnt)\n",
    "        cat_log.columns = [c.replace(\"cat_cnt::\",\"cat_log::\") for c in cat_cnt.columns]\n",
    "\n",
    "        X = (agg_num.join(cat_cnt, how=\"left\").join(cat_prop, how=\"left\").join(cat_log, how=\"left\")).fillna(0)\n",
    "        keep_sessions = X.index.intersection(lab_full.index)\n",
    "        X = X.loc[keep_sessions].copy()\n",
    "        y = lab_full.loc[keep_sessions].rename(\"gender\").copy()\n",
    "\n",
    "        X_ = X.copy(); X_.index.name = \"session_id\"\n",
    "        dataset = X_.join(y).reset_index()\n",
    "        dataset[\"gender\"] = dataset[\"gender\"].astype(\"string\").str.strip().str.upper().replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    "        return dataset\n",
    "\n",
    "    if need_rebuild:\n",
    "        dataset = _build_dataset_from_raw(input_path, int(min_prelogin_events_k), known_cats_json)\n",
    "        non_feature = {\"session_id\",\"gender\"}\n",
    "        feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "        X_all = dataset[feature_cols].copy()\n",
    "        y_bin = dataset[\"gender\"].map({\"F\":0,\"M\":1}).astype(int)\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        _, va_idx = next(sss.split(X_all, y_bin))\n",
    "        X_valid, y_valid = X_all.iloc[va_idx], y_bin.iloc[va_idx]\n",
    "    else:\n",
    "        dataset = pd.read_csv(dataset_prev_in)\n",
    "        X_valid = pd.read_parquet(Xva_prev_in)\n",
    "        y_valid = pd.read_csv(yva_prev_in)[\"y\"].astype(int)\n",
    "\n",
    "    ScalerClass  = StandardScaler if scaler_nm == \"standard\" else RobustScaler\n",
    "    clf_kwargs = dict(solver=\"saga\", penalty=penalty, C=float(C), max_iter=5000, random_state=42)\n",
    "    if penalty == \"elasticnet\" and l1_ratio is not None:\n",
    "        clf_kwargs[\"l1_ratio\"] = float(l1_ratio)\n",
    "    if class_weight is not None:\n",
    "        clf_kwargs[\"class_weight\"] = class_weight\n",
    "\n",
    "    non_feature = {\"session_id\",\"gender\"}\n",
    "    feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "    X_all = dataset[feature_cols].copy()\n",
    "    y_all = dataset[\"gender\"].map({\"F\":0,\"M\":1}).astype(int)\n",
    "\n",
    "    log_katib = Pipeline([\n",
    "        (\"scaler\", ScalerClass(with_mean=True, with_std=True) if ScalerClass is StandardScaler else ScalerClass(with_centering=True, with_scaling=True)),\n",
    "        (\"clf\", LogisticRegression(**clf_kwargs)),\n",
    "    ])\n",
    "    log_katib.fit(X_all, y_all)\n",
    "\n",
    "    try:\n",
    "        cal_katib = CalibratedClassifierCV(estimator=log_katib, method=calib_method, cv=int(calib_cv))\n",
    "    except TypeError:\n",
    "        cal_katib = CalibratedClassifierCV(base_estimator=log_katib, method=calib_method, cv=int(calib_cv))\n",
    "    cal_katib.fit(X_all, y_all)\n",
    "\n",
    "    dump(cal_katib, cal_model_out)\n",
    "    X_valid.to_parquet(Xva_out, index=False)\n",
    "    y_valid.to_frame(\"y\").to_csv(yva_out, index=False)\n",
    "    dataset.to_csv(dataset_out, index=False)\n",
    "\n",
    "    with open(train_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"[OK] 모델 학습 완료 → min_prelogin_events={min_prelogin_events_k if need_rebuild else base_min_prelogin_events}, \"\n",
    "            f\"scaler={scaler_nm}, penalty={penalty}, C={C}, class_weight={class_weight}, \"\n",
    "            f\"calibration=({calib_method}, cv={calib_cv})\\n\"\n",
    "        )\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83379d40-9e77-4a69-837d-46f4050ccbef",
   "metadata": {},
   "source": [
    "## 11) 모델 검증 및 평가 컴포넌트\n",
    "- Katib 최적 하이퍼파라미터로 학습한 모델을 검증 및 평가하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959fe58-6f2f-45e9-b4cb-224a3c11575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 — Tune threshold + save meta -> best_t_txt, proba_csv_out, age_metrics_out, meta_json_out, eval_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_COMMON = [\n",
    "    \"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.1\",\n",
    "    \"joblib==1.4.2\", \"pyarrow==17.0.0\"\n",
    "]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_COMMON)\n",
    "def cell11_tune_threshold_and_save(\n",
    "    cal_model_in: InputPath(\"Model\"),\n",
    "    Xva_in: InputPath(\"Parquet\"),\n",
    "    yva_in: InputPath(\"CSV\"),     \n",
    "    dataset_in: InputPath(\"CSV\"), \n",
    "    df_in: InputPath(\"Parquet\"),\n",
    "    t_lo: float,\n",
    "    t_hi: float,\n",
    "    t_steps: int,\n",
    "    artifact_dir_hint: str,\n",
    "    best_t_txt: OutputPath(str),\n",
    "    proba_csv_out: OutputPath(\"CSV\"),\n",
    "    age_metrics_out: OutputPath(\"CSV\"), \n",
    "    meta_json_out: OutputPath(str),\n",
    "    eval_txt: OutputPath(str),\n",
    "):\n",
    "    import os, json, numpy as np, pandas as pd\n",
    "    from joblib import load, dump\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    model = load(cal_model_in)\n",
    "    X_valid = pd.read_parquet(Xva_in)\n",
    "    y_valid = pd.read_csv(yva_in)[\"y\"].astype(int)\n",
    "    dataset = pd.read_csv(dataset_in)\n",
    "    df = pd.read_parquet(df_in)\n",
    "\n",
    "    lo, hi, n = float(t_lo), float(t_hi), int(t_steps)\n",
    "    ths = np.linspace(lo, hi, n)\n",
    "    proba_k = model.predict_proba(X_valid)[:, 1]\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in ths:\n",
    "        f1 = f1_score(y_valid, (proba_k >= t).astype(int), average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1 = float(t), float(f1)\n",
    "\n",
    "    with open(best_t_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{best_t}\")\n",
    "    pd.DataFrame({\"proba\": proba_k}).to_csv(proba_csv_out, index=False)\n",
    "\n",
    "    sess_ids = dataset.loc[X_valid.index, \"session_id\"].to_numpy() if \"session_id\" in dataset.columns else np.arange(len(X_valid))\n",
    "    pred_bin = (proba_k >= best_t).astype(int)\n",
    "    pred_col = f\"y_pred@{best_t:.3f}\"\n",
    "    out_df = pd.DataFrame({\n",
    "        \"session_id\": sess_ids,\n",
    "        \"y_true\": y_valid.values,\n",
    "        pred_col: pred_bin,\n",
    "        \"proba_cal\": proba_k,\n",
    "    })\n",
    "\n",
    "    extra_cols = [c for c in [\"age\", \"age_group\"] if c in dataset.columns]\n",
    "    if extra_cols:\n",
    "        out_df = out_df.merge(\n",
    "            dataset[[\"session_id\"] + extra_cols].drop_duplicates(\"session_id\"),\n",
    "            on=\"session_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    if \"age_group\" not in out_df.columns and \"age\" in df.columns:\n",
    "        uid_str  = df.get(\"user_id\", pd.Series([\"\"] * len(df))).astype(\"string\").str.strip()\n",
    "        anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "        has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "        appeared_local = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "        post = df.loc[appeared_local].copy()\n",
    "        age_post_by_sess = (\n",
    "            pd.to_numeric(post[\"age\"], errors=\"coerce\")\n",
    "              .groupby(post[\"session_id\"])\n",
    "              .apply(lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan)\n",
    "              .rename(\"age\")\n",
    "        )\n",
    "        age_map = age_post_by_sess.to_frame()\n",
    "\n",
    "        def _to_age_bucket(x):\n",
    "            if pd.isna(x): return np.nan\n",
    "            x = float(x)\n",
    "            return \"young\" if x < 25 else (\"middle\" if x < 50 else \"old\")\n",
    "\n",
    "        age_map[\"age_group\"] = age_map[\"age\"].apply(_to_age_bucket)\n",
    "        out_df = out_df.merge(age_map.reset_index(), on=\"session_id\", how=\"left\")\n",
    "\n",
    "    out_df[\"age_group\"] = out_df.get(\"age_group\").astype(\"string\").fillna(\"unknown\")\n",
    "    rows = []\n",
    "    for g, sub in out_df.groupby(\"age_group\", dropna=False):\n",
    "        y_true_g = sub[\"y_true\"].astype(int).to_numpy()\n",
    "        y_pred_g = sub[pred_col].astype(int).to_numpy()\n",
    "        acc = float((y_true_g == y_pred_g).mean())\n",
    "        f1  = float(f1_score(y_true_g, y_pred_g, average=\"macro\")) if len(np.unique(y_true_g)) > 1 else float(\"nan\")\n",
    "        rows.append({\"age_group\": str(g), \"acc\": acc, \"macro_f1\": f1})\n",
    "\n",
    "    pd.DataFrame(rows).sort_values(\"age_group\").to_csv(age_metrics_out, index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"source\": \"katib\",\n",
    "        \"best_params\": None,\n",
    "        \"best_cv_f1_macro\": None,\n",
    "        \"production_threshold\": float(best_t),\n",
    "        \"calibration\": None,\n",
    "        \"threshold_tuning\": {\"lo\": float(lo), \"hi\": float(hi), \"steps\": int(n)},\n",
    "    }\n",
    "    with open(meta_json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(artifact_dir_hint, exist_ok=True)\n",
    "        dump(model, os.path.join(artifact_dir_hint, \"model_calibrated.joblib\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    with open(eval_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[Katib HP] Best macro-F1 in [{lo:.3f},{hi:.3f}] steps={n}: t={best_t:.3f}, F1={best_f1:.4f}\\n\")\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1abb0f-98f8-4e0a-870a-515b4b612e0a",
   "metadata": {},
   "source": [
    "## 12) Object Storage에 모델 저장 컴포넌트\n",
    "- 검증 및 평가된 모델을 Object Storage에 저장하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df37ad-e4e9-4ed9-b89c-5d50f2c8abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Upload trained model to Kakao Object Storage -> uploaded_uri_out, log_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_S3 = [\"boto3==1.34.162\", \"botocore==1.34.162\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_S3)\n",
    "def cell12_upload_model_to_kakao(\n",
    "    # --- 필수(기본값 없음) ---\n",
    "    cal_model_in: InputPath(\"Model\"),\n",
    "    endpoint: str,\n",
    "    region: str,\n",
    "    bucket: str,\n",
    "    access_key: str,           \n",
    "    secret_key: str,              \n",
    "    uploaded_uri_out: OutputPath(str),\n",
    "    log_txt: OutputPath(str),\n",
    "\n",
    "    # --- 선택(기본값 있음) ---\n",
    "    prefix: str = \"gender_predict_pipeline\",\n",
    "    object_name: str = \"model.joblib\",\n",
    "    public_read: bool = False,\n",
    "):\n",
    "    import os, time, hashlib\n",
    "    import boto3\n",
    "\n",
    "    # 1) boto3 클라이언트\n",
    "    ak = access_key or os.getenv(\"AWS_ACCESS_KEY_ID\") or os.getenv(\"KAKAO_S3_ACCESS_KEY_ID\")\n",
    "    sk = secret_key or os.getenv(\"AWS_SECRET_ACCESS_KEY\") or os.getenv(\"KAKAO_S3_SECRET_ACCESS_KEY\")\n",
    "    if not ak or not sk:\n",
    "        raise RuntimeError(\"Missing credentials: provide access_key/secret_key or set env vars.\")\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=endpoint,\n",
    "        region_name=region,\n",
    "        aws_access_key_id=ak,\n",
    "        aws_secret_access_key=sk,\n",
    "    )\n",
    "\n",
    "    # 2) 버킷 존재 보장\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "    except Exception:\n",
    "        try:\n",
    "            s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": region})\n",
    "        except Exception as e2:\n",
    "            code = getattr(e2, \"response\", {}).get(\"Error\", {}).get(\"Code\")\n",
    "            if code not in (\"BucketAlreadyOwnedByYou\", \"BucketAlreadyExists\"):\n",
    "                raise\n",
    "\n",
    "    # 3) 업로드 키 결정\n",
    "    key_prefix = (prefix.strip(\"/\") + \"/\") if prefix and prefix.strip() else \"\"\n",
    "    if not object_name:\n",
    "        object_name = f\"model_calibrated_{time.strftime('%Y%m%d-%H%M%S')}.joblib\"\n",
    "    key = f\"{key_prefix}{object_name}\"\n",
    "\n",
    "    # 4) 업로드\n",
    "    extra_args = {\"ContentType\": \"application/octet-stream\"}\n",
    "    if public_read:\n",
    "        extra_args[\"ACL\"] = \"public-read\"\n",
    "    s3.upload_file(cal_model_in, bucket, key, ExtraArgs=extra_args)\n",
    "\n",
    "    # 5) 무결성/로그\n",
    "    h = hashlib.md5()\n",
    "    with open(cal_model_in, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    md5 = h.hexdigest()\n",
    "    s3_uri = f\"s3://{bucket}/{key}\"\n",
    "\n",
    "    with open(uploaded_uri_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(s3_uri)\n",
    "    with open(log_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            \"=== Kakao Object Storage Upload ===\\n\"\n",
    "            f\"Endpoint: {endpoint}\\nRegion  : {region}\\n\"\n",
    "            f\"Bucket  : {bucket}\\nPrefix  : {key_prefix}\\n\"\n",
    "            f\"Object  : {object_name}\\nMD5     : {md5}\\n\"\n",
    "            f\"URI     : {s3_uri}\\n\"\n",
    "        )\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045b75-cfe4-4d89-9b1e-956ee8b5a8f7",
   "metadata": {},
   "source": [
    "## 13) KServe 모델 배포 컴포넌트\n",
    "- Object Storage에 저장된 모델을 KServe InferenceService로 배포하는 컴포넌트를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90979b4-783f-4957-9e17-298e3cac645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Deploy with KServe (from Kakao Object Storage) -> isvc_name_out, isvc_yaml_out, isvc_url_out, log_txt\n",
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "BASE_IMAGE = \"python:3.11-slim\"\n",
    "PKGS_K8S = [\"kubernetes==29.0.0\", \"pyyaml==6.0.1\"]\n",
    "\n",
    "@component2(base_image=BASE_IMAGE, packages_to_install=PKGS_K8S)\n",
    "def cell13_deploy_kserve(\n",
    "    # === Required params ===\n",
    "    namespace: str,               # 배포 네임스페이스\n",
    "    isvc_name: str,               # InferenceService 이름\n",
    "    bucket: str,                  # S3 버킷명 (예: models)\n",
    "    prefix: str,                  # 경로/폴더 (예: gender_predict_pipeline)\n",
    "    s3_endpoint: str,             # https://objectstorage.kr-central-2.kakaocloud.com\n",
    "    s3_region: str,               # kr-central-2\n",
    "    aws_access_key_id: str,       # 액세스 키\n",
    "    aws_secret_access_key: str,   # 시크릿 키\n",
    "\n",
    "    # === Outputs (기본값 있는 인자보다 먼저 둬야 파이썬 시그니처 오류가 안 남) ===\n",
    "    isvc_name_out: OutputPath(str),\n",
    "    isvc_yaml_out: OutputPath(str),\n",
    "    isvc_url_out: OutputPath(str),\n",
    "    log_txt: OutputPath(str),\n",
    "\n",
    "    # === Optional params ===\n",
    "    min_replicas: int = 1,\n",
    "    max_replicas: int = 1,\n",
    "    use_legacy_spec: bool = False,    # False: predictor.model, True: predictor.sklearn\n",
    "    append_model_filename: bool = False,  # True면 /model.joblib 까지 붙임\n",
    "):\n",
    "    import time, yaml\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client import ApiException\n",
    "\n",
    "    # ---- sanitize & compose storageUri ----\n",
    "    bucket = bucket.strip().strip(\"/\")\n",
    "    prefix = prefix.strip().strip(\"/\")\n",
    "    s3_endpoint = s3_endpoint.strip()\n",
    "    s3_host = s3_endpoint.replace(\"https://\", \"\").replace(\"http://\", \"\").strip(\"/\")\n",
    "    storage_uri = f\"s3://{bucket}/{prefix}\"\n",
    "    if append_model_filename:\n",
    "        storage_uri = storage_uri + \"/model.joblib\"\n",
    "\n",
    "    # ---- K8s 연결 ----\n",
    "    try:\n",
    "        config.load_incluster_config()\n",
    "    except Exception:\n",
    "        config.load_kube_config()\n",
    "    core = client.CoreV1Api()\n",
    "    crd = client.CustomObjectsApi()\n",
    "\n",
    "    # ---- S3 Secret 생성/갱신 (KServe 크레덴셜 인젝터용 어노테이션 포함) ----\n",
    "    secret_name = f\"{isvc_name}-s3\"\n",
    "    secret_body = client.V1Secret(\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=secret_name,\n",
    "            annotations={\n",
    "                \"serving.kserve.io/s3-endpoint\":  s3_host,\n",
    "                \"serving.kserve.io/s3-usehttps\":  \"1\",\n",
    "                \"serving.kserve.io/s3-verifyssl\": \"1\",\n",
    "                \"serving.kserve.io/s3-region\":    s3_region,\n",
    "            },\n",
    "        ),\n",
    "        type=\"Opaque\",\n",
    "        string_data={\n",
    "            \"AWS_ACCESS_KEY_ID\":     aws_access_key_id,\n",
    "            \"AWS_SECRET_ACCESS_KEY\": aws_secret_access_key,\n",
    "        },\n",
    "    )\n",
    "    try:\n",
    "        core.create_namespaced_secret(namespace, secret_body)\n",
    "    except ApiException as e:\n",
    "        if e.status == 409:\n",
    "            core.patch_namespaced_secret(secret_name, namespace, secret_body)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # ---- ServiceAccount 생성/갱신 (Secret 연결) ----\n",
    "    sa_name = f\"{isvc_name}-sa\"\n",
    "    sa_body = client.V1ServiceAccount(\n",
    "        metadata=client.V1ObjectMeta(name=sa_name),\n",
    "        secrets=[client.V1ObjectReference(name=secret_name)],\n",
    "    )\n",
    "    try:\n",
    "        core.create_namespaced_service_account(namespace, sa_body)\n",
    "    except ApiException as e:\n",
    "        if e.status == 409:\n",
    "            core.patch_namespaced_service_account(sa_name, namespace, sa_body)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # ---- InferenceService (v1beta1) 정의 ----\n",
    "    group, version, plural = \"serving.kserve.io\", \"v1beta1\", \"inferenceservices\"\n",
    "    annotations_isvc = {\n",
    "        \"autoscaling.knative.dev/minScale\": str(min_replicas),\n",
    "        \"autoscaling.knative.dev/maxScale\": str(max_replicas),\n",
    "        # 일부 설치에서 메타에 s3 힌트를 같이 주면 더 안정적\n",
    "        \"serving.kserve.io/s3-endpoint\":  s3_host,\n",
    "        \"serving.kserve.io/s3-usehttps\":  \"1\",\n",
    "        \"serving.kserve.io/s3-verifyssl\": \"1\",\n",
    "        \"serving.kserve.io/s3-region\":    s3_region,\n",
    "    }\n",
    "\n",
    "    if use_legacy_spec:\n",
    "        predictor = {\n",
    "            \"serviceAccountName\": sa_name,\n",
    "            \"sklearn\": {\n",
    "                \"storageUri\": storage_uri,\n",
    "                \"resources\": {\n",
    "                    \"requests\": {\"cpu\": \"200m\", \"memory\": \"512Mi\"},\n",
    "                    \"limits\":   {\"cpu\": \"1\",    \"memory\": \"2Gi\"},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    else:\n",
    "        predictor = {\n",
    "            \"serviceAccountName\": sa_name,\n",
    "            \"model\": {\n",
    "                \"modelFormat\": {\"name\": \"sklearn\"},\n",
    "                \"storageUri\": storage_uri,\n",
    "                \"resources\": {\n",
    "                    \"requests\": {\"cpu\": \"200m\", \"memory\": \"512Mi\"},\n",
    "                    \"limits\":   {\"cpu\": \"1\",    \"memory\": \"2Gi\"},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "    isvc_body = {\n",
    "        \"apiVersion\": f\"{group}/{version}\",\n",
    "        \"kind\": \"InferenceService\",\n",
    "        \"metadata\": {\n",
    "            \"name\": isvc_name,\n",
    "            \"namespace\": namespace,\n",
    "            \"annotations\": annotations_isvc,\n",
    "        },\n",
    "        \"spec\": {\"predictor\": predictor},\n",
    "    }\n",
    "\n",
    "    # ---- 생성 또는 패치 ----\n",
    "    created = False\n",
    "    try:\n",
    "        crd.create_namespaced_custom_object(group, version, namespace, plural, isvc_body)\n",
    "        created = True\n",
    "    except ApiException as e:\n",
    "        if e.status == 409:\n",
    "            crd.patch_namespaced_custom_object(group, version, namespace, plural, isvc_name, isvc_body)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # ---- Ready 대기 (최대 3분) ----\n",
    "    url, ready = \"\", \"\"\n",
    "    deadline = time.time() + 180\n",
    "    while time.time() < deadline:\n",
    "        obj = crd.get_namespaced_custom_object(group, version, namespace, plural, isvc_name)\n",
    "        status = obj.get(\"status\", {})\n",
    "        conds = {c.get(\"type\"): c for c in status.get(\"conditions\", [])}\n",
    "        if \"Ready\" in conds:\n",
    "            ready = conds[\"Ready\"].get(\"status\", \"\")\n",
    "        url = status.get(\"url\") or status.get(\"address\", {}).get(\"url\") or \"\"\n",
    "        if ready == \"True\" and url:\n",
    "            break\n",
    "        time.sleep(3)\n",
    "\n",
    "    # ---- 결과 산출물 ----\n",
    "    with open(isvc_name_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(isvc_name)\n",
    "    with open(isvc_yaml_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(isvc_body, f, sort_keys=False, allow_unicode=True)\n",
    "    with open(isvc_url_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(url or \"\")\n",
    "    with open(log_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            \"=== KServe Deploy Result ===\\n\"\n",
    "            f\"namespace : {namespace}\\n\"\n",
    "            f\"isvc_name : {isvc_name}\\n\"\n",
    "            f\"created   : {created}\\n\"\n",
    "            f\"ready     : {ready}\\n\"\n",
    "            f\"url       : {url}\\n\"\n",
    "            f\"storageUri: {storage_uri}\\n\"\n",
    "            f\"sa/secret : {sa_name} / {secret_name}\\n\"\n",
    "        )\n",
    "\n",
    "print(\"[done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71846a97-ec79-4a11-8a22-335826a78389",
   "metadata": {},
   "source": [
    "## 14) 파이프라인 정의\n",
    "- 컴포넌트들을 연결하여 파이프라인을 정의하고 컴파일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf45925-ac52-483b-b4c6-670bf3cbc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Pipeline (no visualization; ensure upload runs AFTER tuning/eval)\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp import kubernetes as k8s\n",
    "\n",
    "# 이미 pipeline2가 있다면 재정의 방지\n",
    "try:\n",
    "    pipeline2\n",
    "except NameError:\n",
    "    from inspect import signature, _empty\n",
    "    def _eval_annotations(fn):\n",
    "        sig = signature(fn)\n",
    "        fixed = {}\n",
    "        for name, param in sig.parameters.items():\n",
    "            ann = param.annotation\n",
    "            if ann is _empty:\n",
    "                continue\n",
    "            if isinstance(ann, str) and ann in {\"str\", \"int\", \"float\", \"bool\"}:\n",
    "                ann = eval(ann)\n",
    "            fixed[name] = ann\n",
    "        if fixed:\n",
    "            fn.__annotations__.update(fixed)\n",
    "        return fn\n",
    "\n",
    "    def pipeline2(**kwargs):\n",
    "        def _decorator(fn):\n",
    "            fn = _eval_annotations(fn)\n",
    "            return dsl.pipeline(**kwargs)(fn)\n",
    "        return _decorator\n",
    "\n",
    "@pipeline2(\n",
    "    name=\"prelogin-gender-pipeline\",\n",
    "    description=\"Build features, train baseline, fetch Katib HPs, retrain, tune threshold, upload to Kakao Object Storage, and deploy on KServe.\",\n",
    ")\n",
    "def prelogin_gender_pipeline(\n",
    "    # 입력 CSV (PVC로 마운트)\n",
    "    input_path: str = \"/home/jovyan/datasets/gender/processed_user_behavior.joined.csv\",\n",
    "\n",
    "    # 공통 파라미터\n",
    "    known_cats_json: str = '[\"Books\",\"Electronics\",\"Gaming\",\"Home\",\"Fashion\"]',\n",
    "    production_threshold: float = 0.5,\n",
    "    min_prelogin_events: int = 2,\n",
    "\n",
    "    # 디렉토리 힌트(로컬 경로, 실제 I/O는 아티팩트가 담당)\n",
    "    datasets_dir: str = \"datasets/gender\",\n",
    "    artifact_dir: str = \"models/gender\",\n",
    "\n",
    "    # 분할/캘리브레이션\n",
    "    random_state: int = 42,\n",
    "    test_size: float = 0.2,\n",
    "    calib_method: str = \"isotonic\",\n",
    "    calib_cv: int = 5,\n",
    "\n",
    "    # Katib\n",
    "    katib_namespace: str = \"kbm-u-kubeflow-tutorial\",\n",
    "    katib_experiment: str = \"gender-logistic-random\",\n",
    "\n",
    "    # 임계값 탐색\n",
    "    tune_t_lo: float = 0.2,\n",
    "    tune_t_hi: float = 0.8,\n",
    "    tune_t_steps: int = 61,\n",
    "\n",
    "    # PVC 이름\n",
    "    pvc_name: str = \"cpu-notebook-workspace\",\n",
    "\n",
    "    # Kakao Object Storage 업로드(파라미터로 키 전달)\n",
    "    kakao_endpoint: str = \"https://objectstorage.kr-central-2.kakaocloud.com\",\n",
    "    kakao_region: str = \"kr-central-2\",\n",
    "    kakao_bucket: str = \"models\",\n",
    "    kakao_prefix: str = \"gender_predict_pipeline\",\n",
    "    kakao_public_read: bool = False,\n",
    "    kakao_access_key: str = \"\",   # ← KFP UI에서 입력\n",
    "    kakao_secret_key: str = \"\",   # ← KFP UI에서 입력\n",
    "):\n",
    "    # 1) 초기화\n",
    "    c1 = cell1_init(\n",
    "        known_cats_json=known_cats_json,\n",
    "        production_threshold=production_threshold,\n",
    "        min_prelogin_events=min_prelogin_events,\n",
    "        datasets_dir=datasets_dir,\n",
    "        artifact_dir=artifact_dir,\n",
    "    )\n",
    "\n",
    "    # 2) CSV 읽기 + prelogin 필터 (PVC 필요)\n",
    "    c2 = cell2_prelogin_filter(\n",
    "        input_path=input_path,\n",
    "        min_prelogin_events=min_prelogin_events,\n",
    "    )\n",
    "    k8s.mount_pvc(task=c2, pvc_name=pvc_name, mount_path=\"/home/jovyan\")\n",
    "\n",
    "    # 3) 숫자 집계 + 라벨 생성\n",
    "    c3 = cell3_aggregate_and_label(\n",
    "        df_in=c2.outputs[\"df_out\"],\n",
    "        pre_in=c2.outputs[\"pre_out\"],\n",
    "    )\n",
    "\n",
    "    # 4) 카테고리 파생\n",
    "    c4 = cell4_category_features(\n",
    "        pre_in=c2.outputs[\"pre_out\"],\n",
    "        Xnum_in=c3.outputs[\"Xnum_out\"],\n",
    "        known_cats_json=known_cats_json,\n",
    "    )\n",
    "\n",
    "    # 5) 데이터셋 조립\n",
    "    c5 = cell5_build_dataset(\n",
    "        Xnum_in=c3.outputs[\"Xnum_out\"],\n",
    "        cat_cnt_in=c4.outputs[\"cat_cnt_out\"],\n",
    "        cat_prop_in=c4.outputs[\"cat_prop_out\"],\n",
    "        cat_log_in=c4.outputs[\"cat_log_out\"],\n",
    "        y_in=c3.outputs[\"y_out\"],\n",
    "    )\n",
    "\n",
    "    # 6) 분할\n",
    "    c6 = cell6_split(\n",
    "        dataset_pq_in=c5.outputs[\"dataset_pq_out\"],\n",
    "        random_state=random_state,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "    # 7) 기본 로지스틱 + 캘리브레이션 (베이스라인)\n",
    "    c7 = cell7_train_calibrate(\n",
    "        Xtr_in=c6.outputs[\"Xtr_out\"],\n",
    "        ytr_in=c6.outputs[\"ytr_out\"],\n",
    "        calib_method=calib_method,\n",
    "        calib_cv=calib_cv,\n",
    "    )\n",
    "\n",
    "    # 8) 프로덕션 임계값 평가 (베이스라인 모델)\n",
    "    c8 = cell8_eval_prod_threshold(\n",
    "        cal_model_in=c7.outputs[\"cal_model_out\"],\n",
    "        Xva_in=c6.outputs[\"Xva_out\"],\n",
    "        yva_in=c6.outputs[\"yva_out\"],\n",
    "        dataset_csv_in=c5.outputs[\"dataset_csv_out\"],\n",
    "        production_threshold=production_threshold,\n",
    "    )\n",
    "\n",
    "    # 9) Katib HP 가져오기\n",
    "    c9 = cell9_fetch_katib(\n",
    "        katib_namespace=katib_namespace,\n",
    "        katib_experiment=katib_experiment,\n",
    "    )\n",
    "\n",
    "    # 10) Katib HP로 재학습 (필요시 원본 CSV 재빌드) → PVC 필요\n",
    "    c10 = cell10_train_with_katib(\n",
    "        katib_params_json_in=c9.outputs[\"katib_params_json_out\"],\n",
    "        input_path=input_path,\n",
    "        known_cats_json=known_cats_json,\n",
    "        base_min_prelogin_events=min_prelogin_events,\n",
    "        Xva_prev_in=c6.outputs[\"Xva_out\"],\n",
    "        yva_prev_in=c6.outputs[\"yva_out\"],\n",
    "        dataset_prev_in=c5.outputs[\"dataset_csv_out\"],\n",
    "    )\n",
    "    k8s.mount_pvc(task=c10, pvc_name=pvc_name, mount_path=\"/home/jovyan\")\n",
    "\n",
    "    # 11) 임계값 튜닝 + 메타/확률/연령대 메트릭 저장 (재학습 모델 기준)\n",
    "    c11 = cell11_tune_threshold_and_save(\n",
    "        cal_model_in=c10.outputs[\"cal_model_out\"],\n",
    "        Xva_in=c10.outputs[\"Xva_out\"],\n",
    "        yva_in=c10.outputs[\"yva_out\"],\n",
    "        dataset_in=c10.outputs[\"dataset_out\"],\n",
    "        df_in=c2.outputs[\"df_out\"],\n",
    "        t_lo=tune_t_lo,\n",
    "        t_hi=tune_t_hi,\n",
    "        t_steps=tune_t_steps,\n",
    "        artifact_dir_hint=artifact_dir,\n",
    "    )\n",
    "\n",
    "    # 12) 튜닝/평가 이후 업로드 보장 (재학습 모델 업로드)\n",
    "    c12 = cell12_upload_model_to_kakao(\n",
    "        cal_model_in=c10.outputs[\"cal_model_out\"],\n",
    "        endpoint=kakao_endpoint,\n",
    "        region=kakao_region,\n",
    "        bucket=kakao_bucket,\n",
    "        access_key=kakao_access_key,   # 파라미터로 전달\n",
    "        secret_key=kakao_secret_key,   # 파라미터로 전달\n",
    "        prefix=kakao_prefix,\n",
    "        object_name=\"model.joblib\",\n",
    "        public_read=kakao_public_read,\n",
    "    )\n",
    "    c12.after(c11)  # 반드시 튜닝/평가 후 업로드\n",
    "\n",
    "    # 13) KServe 배포 — 업로드 완료 후 수행\n",
    "    c13 = cell13_deploy_kserve(\n",
    "        namespace=\"kbm-u-kubeflow-tutorial\",\n",
    "        isvc_name=\"gender-sklearn\",\n",
    "        bucket=kakao_bucket,            \n",
    "        prefix=kakao_prefix,               \n",
    "        s3_endpoint=kakao_endpoint,\n",
    "        s3_region=kakao_region,\n",
    "        aws_access_key_id=kakao_access_key,       # 파이프라인 파라미터로 전달\n",
    "        aws_secret_access_key=kakao_secret_key,   # 파이프라인 파라미터로 전달\n",
    "        min_replicas=1,\n",
    "        max_replicas=1,\n",
    "        use_legacy_spec=False,               # 새 스펙(predictor.model)\n",
    "        append_model_filename=True,          # 업로드 파일명이 model.joblib이므로 파일까지 붙여 배포\n",
    "    )\n",
    "    c13.after(c12)\n",
    "\n",
    "    # 캐싱 비활성화 (선택)\n",
    "    for t in [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13]:\n",
    "        t.set_caching_options(enable_caching=False)\n",
    "\n",
    "from kfp import compiler\n",
    "compiler.Compiler().compile(prelogin_gender_pipeline, \"gender_pipeline.yaml\")\n",
    "print(\"[Done] gender_pipeline.yaml Created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232e250-36ac-4358-84f1-a26f0434c102",
   "metadata": {},
   "source": [
    "## 15) 모델 추론 실습\n",
    "- KServe로 배포된 모델에 샘플 데이터를 입력하여 추론하는 실습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d08805-ca72-4054-9ab7-ac5190e3a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "NS   = \"kbm-u-kubeflow-tutorial\" # 네임스페이스\n",
    "ISVC = \"gender-sklearn\" # KServe InferencesService 이름\n",
    "\n",
    "GATEWAY = \"http://knative-local-gateway.istio-system.svc.cluster.local\"\n",
    "\n",
    "URL = f\"{GATEWAY}/v1/models/{ISVC}:predict\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Host\": f\"{ISVC}-predictor.{NS}.svc.cluster.local\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "LABEL_MAP = {0: \"F\", 1: \"M\"}\n",
    "TRUTH_MAP = {\"여자\": \"F\", \"남자\": \"M\"}\n",
    "\n",
    "#####################\n",
    "# 10개의 샘플 데이터\n",
    "#####################\n",
    "cases = [\n",
    "    {\"case\": \"29세 남자\", \"truth\": \"남자\",\n",
    "     \"data\": [11,0,0,6.0,1.2727272727272727,7,1,0,3,0,0,1,0,0,0.0,0.0,0.09090909090909091,0.0,0.0,0.0,0.0,0.6931471805599453,0.0,0.0], \"note\": \"\"},\n",
    "    {\"case\": \"68세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [10,0,0,5.5,1.3,6,0,16,2,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0], \"note\": \"\"},\n",
    "    {\"case\": \"56세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [9,0,0,5.0,1.2222222222222223,6,2,20,2,2,0,0,0,1,0.2222222222222222,0.0,0.0,0.0,0.1111111111111111,1.0986122886681096,0.0,0.0,0.0,0.6931471805599453], \"note\": \"\"},\n",
    "    {\"case\": \"44세 남자\", \"truth\": \"남자\",\n",
    "     \"data\": [9,0,0,5.0,1.5555555555555556,4,0,17,2,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0], \"note\": \"\"},\n",
    "    {\"case\": \"28세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [8,0,0,4.5,1.25,6,1,20,2,0,0,0,1,0,0.0,0.0,0.0,0.125,0.0,0.0,0.0,0.0,0.6931471805599453,0.0], \"note\": \"\"},\n",
    "    {\"case\": \"68세 남자\", \"truth\": \"남자\",\n",
    "     \"data\": [6,0,0,3.5,1.3333333333333333,5,0,22,2,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0], \"note\": \"\"},\n",
    "    {\"case\": \"70세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [6,0,0,3.5,1.3333333333333333,5,1,19,2,0,0,0,0,1,0.0,0.0,0.0,0.0,0.16666666666666666,0.0,0.0,0.0,0.0,0.6931471805599453], \"note\": \"\"},\n",
    "    {\"case\": \"43세 남자\", \"truth\": \"남자\",\n",
    "     \"data\": [5,0,0,3.0,0.8,4,0,23,2,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "     \"note\": \"검색/상품조회/카테고리 접근 없음\"},\n",
    "    {\"case\": \"29세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [6,0,0,3.5,1.3333333333333333,4,0,2,3,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "     \"note\": \"남성향 카테고리 활동 위주\"},\n",
    "    {\"case\": \"41세 여자\", \"truth\": \"여자\",\n",
    "     \"data\": [2,0,0,1.5,0.5,2,0,20,2,0,0,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
    "     \"note\": \"남성향/여성향 고르게 활동\"},\n",
    "]\n",
    "\n",
    "def infer_one(vec):\n",
    "    # v1 REST: {\"instances\": [[features...]]}\n",
    "    payload = {\"instances\": [[float(x) for x in vec]]}\n",
    "    r = requests.post(URL, json=payload, headers=HEADERS, timeout=8)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    # KServe sklearn predictor(v1) → {\"predictions\": [int_or_float]}\n",
    "    if \"predictions\" not in js:\n",
    "        raise ValueError(f\"Unexpected response keys: {list(js.keys())}\")\n",
    "    return int(js[\"predictions\"][0])\n",
    "\n",
    "rows = []\n",
    "for c in cases:\n",
    "    pred_int = infer_one(c[\"data\"])\n",
    "    truth_mf = TRUTH_MAP.get(c[\"truth\"], c[\"truth\"])\n",
    "    pred_mf  = LABEL_MAP.get(pred_int, pred_int)\n",
    "    rows.append({\n",
    "        \"case\":  c[\"case\"],\n",
    "        \"truth\": truth_mf,\n",
    "        \"pred\":  pred_mf,\n",
    "        \"✓/✗\":   \"✓\" if pred_mf == truth_mf else \"✗\",\n",
    "        \"note\":  c[\"note\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"case\", \"truth\", \"pred\", \"✓/✗\", \"note\"])\n",
    "display(df)\n",
    "acc = (df[\"✓/✗\"] == \"✓\").mean()\n",
    "print(f\"\\n[Done] 정확도(샘플 {len(df)}개): {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
