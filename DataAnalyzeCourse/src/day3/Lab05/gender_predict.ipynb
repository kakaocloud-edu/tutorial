{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00cd8af",
   "metadata": {},
   "source": [
    "# 성별 추론\n",
    "\n",
    "### 사용자의 로그인 전 행동 패턴을 보고 사용자의 성별을 추론하는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cbbea-964c-45f8-9e04-cb504b38f9c3",
   "metadata": {},
   "source": [
    "## 1) 환경 구성\n",
    "- 실습을 원활하게 진행하기 위해 환경을 구성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e95b6-4a74-4f07-8b7b-0e19101cca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import unicodedata\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from joblib import dump\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score,)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# ---------- 경로/상수 ----------\n",
    "DATASETS_DIR  = Path(\"datasets\") / \"gender\" \n",
    "INPUT_PATH    = \"datasets/gender/processed_user_behavior.joined.csv\" # Search_keyword -> Category, Product_id -> Category 매핑 테이블과 원본 테이블 조인\n",
    "OUTPUT_DIR    = DATASETS_DIR                              \n",
    "ARTIFACT_DIR  = Path(\"models\") / \"gender\"     \n",
    "\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MIN_PRELOGIN_EVENTS = 2\n",
    "\n",
    "KNOWN_CATS = [\"Books\", \"Electronics\", \"Gaming\", \"Home\", \"Fashion\"]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "PRODUCTION_THRESHOLD = 0.500\n",
    "\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"공백 제거 + 유니코드 NFKC 정규화(NA 안전)\"\"\"\n",
    "    s = s.astype(\"string\").str.strip()\n",
    "    return s.apply(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ae8d2-1fc2-41b5-98c7-0b91cf8345c8",
   "metadata": {},
   "source": [
    "## 2) 원본 데이터 로드 & 로그인 전 구간 추출\n",
    "- 원본과 매핑 테이블을 조인한 데이터를 읽어와 로그인 전 구간을 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f00e3-e876-4d3f-9a78-ce7a364d2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
    "\n",
    "df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"session_id\", \"ts\"]).copy()\n",
    "\n",
    "sort_cols = [\"session_id\", \"ts\"]\n",
    "if \"event_id\" in df.columns:\n",
    "    sort_cols.append(\"event_id\")\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"원본 행 수:\", len(df))\n",
    "\n",
    "uid_str  = df[\"user_id\"].astype(\"string\").str.strip()\n",
    "anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "\n",
    "appeared = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "pre = df.loc[~appeared].copy()\n",
    "\n",
    "prelogin_counts = pre.groupby(\"session_id\").size()\n",
    "valid_sessions = prelogin_counts.index[prelogin_counts >= MIN_PRELOGIN_EVENTS]\n",
    "pre = pre[pre[\"session_id\"].isin(valid_sessions)].copy()\n",
    "\n",
    "print(\"로그인 전 로그 필터 후 행 수:\", len(pre))\n",
    "print(\"로그인 전 유일 세션 수:\", pre[\"session_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac4b09-71aa-4115-8803-29abe2abf7a0",
   "metadata": {},
   "source": [
    "## 3) Target Label 생성 & 로그인 전 구간 수치 집계\n",
    "- 정답지 역할인 Target Label을 생성하고, 로그인 완료한 세션 수를 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08604857-d072-4d74-a70e-e515a0ad6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "df_gender = df.copy()\n",
    "df_gender[\"gender_norm\"] = (\n",
    "    df_gender[\"gender\"]\n",
    "      .astype(\"string\").str.strip().str.upper()\n",
    "      .replace({\"FEMALE\":\"F\", \"MALE\":\"M\"})\n",
    ")\n",
    "lab_full = (\n",
    "    df_gender[df_gender[\"gender_norm\"].isin([\"M\",\"F\"])] # 로그인 이후의 세션만 확인\n",
    "      .groupby(\"session_id\")[\"gender_norm\"].agg(lambda s: s.iloc[0])\n",
    ")\n",
    "\n",
    "agg_num = pre.groupby(\"session_id\").agg(\n",
    "    n_events=(\"session_id\", \"size\"),\n",
    "    search_count_sum=(\"search_count\", \"sum\"),\n",
    "    cart_item_count_sum=(\"cart_item_count\", \"sum\"),\n",
    "    page_depth_mean=(\"page_depth\", \"mean\"),\n",
    "    last_elapsed_mean=(\"last_action_elapsed\", \"mean\"),\n",
    "    unique_pages=(\"current_state\", \"nunique\"),\n",
    "    unique_categories=(\"resolved_category\", \"nunique\"),\n",
    ").fillna(0.0)\n",
    "\n",
    "first_ts = pre.groupby(\"session_id\")[\"ts\"].min()\n",
    "agg_num[\"start_hour\"] = first_ts.dt.hour\n",
    "agg_num[\"start_weekday\"] = first_ts.dt.weekday\n",
    "\n",
    "keep_sessions = agg_num.index.intersection(lab_full.index)\n",
    "X_num = agg_num.loc[keep_sessions].copy()\n",
    "y = lab_full.loc[keep_sessions].rename(\"gender\").copy()\n",
    "\n",
    "print(\"로그인 완료한 세션 수:\", len(keep_sessions))\n",
    "print(\"성별 분포:\\n\", y.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53865690-c02f-4b12-9dd8-359630be376b",
   "metadata": {},
   "source": [
    "## 4) 카테고리 카운트 & 파생 컬럼 생성 (Feature Engineering)\n",
    "- 카테고리 등장 횟수를 저장하는 카운트 컬럼, 횟수를 비율로 치환하는 파생 컬럼을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3161882-35c7-4f67-b1c8-56a8cd74db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "KNOWN_CATS_NORM = [unicodedata.normalize(\"NFKC\", c.strip()) for c in KNOWN_CATS]\n",
    "norm_to_orig = {unicodedata.normalize(\"NFKC\", c.strip()): c for c in KNOWN_CATS}\n",
    "\n",
    "pre_cat_norm = norm(pre[\"resolved_category\"])\n",
    "\n",
    "mask = pre_cat_norm.isin(KNOWN_CATS_NORM)\n",
    "pre_kept = pre[mask].copy()\n",
    "pre_kept[\"cat_norm\"] = pre_cat_norm[mask].values\n",
    "pre_kept[\"one\"] = 1\n",
    "\n",
    "cat_cnt = pre_kept.pivot_table(\n",
    "    index=\"session_id\",\n",
    "    columns=\"cat_norm\",\n",
    "    values=\"one\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "cat_cnt = cat_cnt.reindex(columns=KNOWN_CATS_NORM, fill_value=0)\n",
    "cat_cnt.columns = [norm_to_orig[c] for c in cat_cnt.columns]\n",
    "\n",
    "cat_cnt.columns = [f\"cat_cnt::{c}\" for c in cat_cnt.columns]\n",
    "cat_cnt = cat_cnt.reindex(X_num.index).fillna(0).astype(int)\n",
    "\n",
    "cat_cnt_cols = list(cat_cnt.columns)\n",
    "\n",
    "cat_prop = cat_cnt.div(X_num[\"n_events\"].replace(0, 1), axis=0)\n",
    "cat_prop.columns = [c.replace(\"cat_cnt::\", \"cat_prop::\") for c in cat_cnt_cols]\n",
    "\n",
    "cat_log = np.log1p(cat_cnt)\n",
    "cat_log.columns = [c.replace(\"cat_cnt::\", \"cat_log::\") for c in cat_cnt_cols]\n",
    "\n",
    "print(\"카테고리 카운트 컬럼:\", list(cat_cnt.columns))\n",
    "print(\"카테고리 파생 컬럼:\", list(cat_prop.columns[:3]) + list(cat_log.columns[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d8870-5e27-4b56-9ef1-39303de7b8ad",
   "metadata": {},
   "source": [
    "## 5) 파생 컬럼 결합 및 저장 (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd50ab1-7f2a-4dd8-9137-9bda6afc8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "X = (\n",
    "    X_num\n",
    "    .join(cat_cnt, how=\"left\")\n",
    "    .join(cat_prop, how=\"left\")\n",
    "    .join(cat_log, how=\"left\")\n",
    ").fillna(0)\n",
    "\n",
    "X_ = X.copy()\n",
    "if X_.index.name != \"session_id\":\n",
    "    X_.index.name = \"session_id\"\n",
    "\n",
    "dataset = X_.join(y).reset_index()\n",
    "\n",
    "print(\"dataset shape:\", dataset.shape)\n",
    "print(dataset.head(10))\n",
    "\n",
    "FEATURES_CSV = f\"{OUTPUT_DIR}/prelogin_gender_features.csv\"\n",
    "FEATURES_PARQUET = f\"{OUTPUT_DIR}/prelogin_gender_features.parquet\"\n",
    "\n",
    "dataset.to_csv(FEATURES_CSV, index=False)\n",
    "dataset.to_parquet(FEATURES_PARQUET, index=False)\n",
    "\n",
    "print(f\"\\nSaved: {FEATURES_CSV}\")\n",
    "print(f\"Saved: {FEATURES_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a708a28-21fb-4f7f-b78d-a7c975ef40c4",
   "metadata": {},
   "source": [
    "## 6) 학습용 / 검증용 데이터셋 분할\n",
    "- 데이터를 학습용 / 검증용 데이터셋으로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550a977-e5ee-46f3-86b6-bea59dc46626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "dataset[\"gender\"] = (\n",
    "    dataset[\"gender\"].astype(\"string\").str.strip().str.upper()\n",
    "    .replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    ")\n",
    "\n",
    "non_feature = {\"session_id\",\"gender\"}\n",
    "feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "X = dataset[feature_cols].copy()\n",
    "y_bin = dataset[\"gender\"].map({\"F\":0, \"M\":1}).astype(int)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "train_idx, valid_idx = next(sss.split(X, y_bin))\n",
    "X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "y_train, y_valid = y_bin.iloc[train_idx], y_bin.iloc[valid_idx]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Valid:\", X_valid.shape)\n",
    "print(\"Label dist (train):\\n\", y_train.value_counts(normalize=True).rename({0:\"F\",1:\"M\"}))\n",
    "print(\"Label dist (valid):\\n\", y_valid.value_counts(normalize=True).rename({0:\"F\",1:\"M\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3fa2f-cdc8-4cc8-a13b-fda91bc788f7",
   "metadata": {},
   "source": [
    "## 7) 모델 학습\n",
    "- 학습용 데이터 셋으로 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2488f-ac8c-446a-aba7-7c1c63f1edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def tune_threshold(model, X_va, y_va, search=(0.2,0.8,61)):\n",
    "    \"\"\"Macro-F1 최대화 임계값 탐색 (predict_proba 사용 가능한 모델 전제)\"\"\"\n",
    "    lo, hi, n = search\n",
    "    ths = np.linspace(lo, hi, n)\n",
    "    proba = model.predict_proba(X_va)[:,1]\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in ths:\n",
    "        preds = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_va, preds, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1 = float(t), float(f1)\n",
    "    return best_t, best_f1, proba\n",
    "\n",
    "def evaluate(model, X_va, y_va, threshold=0.5, name=\"Model\"):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X_va)[:,1]\n",
    "        y_hat = (proba >= threshold).astype(int)\n",
    "    else:\n",
    "        y_hat = model.predict(X_va)\n",
    "        proba = None\n",
    "\n",
    "    print(f\"\\n=== {name} @ threshold={threshold:.3f} ===\")\n",
    "    print(classification_report(y_va, y_hat, target_names=[\"F\",\"M\"], digits=4))\n",
    "    print(\"Confusion matrix [rows=true F,M | cols=pred F,M]:\\n\", confusion_matrix(y_va, y_hat))\n",
    "    return y_hat, proba\n",
    "\n",
    "def print_scores(name, y_true, proba, t):\n",
    "    y_pred = (proba >= t).astype(int)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = (y_true.values == y_pred).mean()\n",
    "    print(f\"[{name}] Macro-F1 : {macro_f1:.4f} / Accuracy : {acc:.4f}\")\n",
    "\n",
    "CALIB_METHOD = globals().get(\"CALIB_METHOD\", \"isotonic\")\n",
    "CALIB_CV     = globals().get(\"CALIB_CV\", 5)\n",
    "\n",
    "log_best = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\", C=1.0,\n",
    "        max_iter=5000, random_state=RANDOM_STATE,\n",
    "    ))\n",
    "])\n",
    "log_best.fit(X_train, y_train)\n",
    "\n",
    "try:\n",
    "    cal_log = CalibratedClassifierCV(estimator=log_best, method=CALIB_METHOD, cv=CALIB_CV)\n",
    "except TypeError:\n",
    "    cal_log = CalibratedClassifierCV(base_estimator=log_best, method=CALIB_METHOD, cv=CALIB_CV)\n",
    "\n",
    "cal_log.fit(X_train, y_train)\n",
    "print(f\"[Calibration] done: method={CALIB_METHOD}, cv={CALIB_CV}\")\n",
    "\n",
    "best_params_jsonable = {\n",
    "    \"scaler\": {\"name\": \"StandardScaler\"},\n",
    "    \"clf\": {\"name\": \"LogisticRegression\", \"params\": log_best.named_steps[\"clf\"].get_params(deep=False)}\n",
    "}\n",
    "best_cv_f1_macro = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4925d9-762b-4c61-badb-ccebd8b21c29",
   "metadata": {},
   "source": [
    "## 8) 학습 평가\n",
    "- 검증용 데이터 셋으로 모델을 검증하여 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8babf9-5db9-4042-a5c4-3d4b9280ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "yhat_prod, _ = evaluate(\n",
    "    cal_log, X_valid, y_valid,\n",
    "    threshold=PRODUCTION_THRESHOLD,\n",
    "    name=f\"LogisticCal (prod={PRODUCTION_THRESHOLD:.3f})\"\n",
    ")\n",
    "proba_v = cal_log.predict_proba(X_valid)[:, 1]\n",
    "print_scores(f\"LogisticCal (prod@{PRODUCTION_THRESHOLD:.3f})\", y_valid, proba_v, PRODUCTION_THRESHOLD)\n",
    "print(\"\\n\" + \"-\"*82)\n",
    "\n",
    "sess_ids = (\n",
    "    dataset.iloc[valid_idx][\"session_id\"].values\n",
    "    if \"session_id\" in dataset.columns else np.arange(len(valid_idx))\n",
    ")\n",
    "pred_bin   = (proba_v >= PRODUCTION_THRESHOLD).astype(int)\n",
    "pred_col   = f\"y_pred@{PRODUCTION_THRESHOLD:.3f}\"\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    \"session_id\": sess_ids,\n",
    "    \"y_true\": y_valid.values,\n",
    "    pred_col: pred_bin,\n",
    "    \"proba_cal\": proba_v,\n",
    "})\n",
    "\n",
    "extra_cols = [c for c in [\"age\", \"age_group\"] if c in dataset.columns]\n",
    "if extra_cols:\n",
    "    out_df = out_df.merge(\n",
    "        dataset[[\"session_id\"] + extra_cols].drop_duplicates(\"session_id\"),\n",
    "        on=\"session_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "metrics_df = out_df.copy()\n",
    "if \"age_group\" not in metrics_df.columns:\n",
    "    post = df.loc[appeared].copy()\n",
    "    if \"age\" in post.columns:\n",
    "        age_post_by_sess = (\n",
    "            pd.to_numeric(post[\"age\"], errors=\"coerce\")\n",
    "              .groupby(post[\"session_id\"])\n",
    "              .apply(lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan)\n",
    "              .rename(\"age\")\n",
    "        )\n",
    "        age_map = age_post_by_sess.to_frame()\n",
    "        def to_age_bucket(x):\n",
    "            if pd.isna(x): return np.nan\n",
    "            x = float(x)\n",
    "            return \"young\" if x < 25 else (\"middle\" if x < 50 else \"old\")\n",
    "        age_map[\"age_group\"] = age_map[\"age\"].apply(to_age_bucket)\n",
    "        age_map = age_map.reset_index()  # session_id 복원\n",
    "        metrics_df = metrics_df.merge(age_map, on=\"session_id\", how=\"left\")\n",
    "\n",
    "metrics_df[\"age_group\"] = (\n",
    "    metrics_df.get(\"age_group\", pd.Series([np.nan]*len(metrics_df)))\n",
    "    .astype(\"string\")\n",
    "    .fillna(\"unknown\")\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for g, sub in metrics_df.groupby(\"age_group\", dropna=False):\n",
    "    y_true = sub[\"y_true\"].astype(int).to_numpy()\n",
    "    y_hat  = sub[pred_col].astype(int).to_numpy()\n",
    "    acc = float((y_true == y_hat).mean())\n",
    "    f1  = float(f1_score(y_true, y_hat, average=\"macro\")) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    rows.append({\"age_group\": str(g), \"acc\": acc, \"macro_f1\": f1})\n",
    "\n",
    "scores_by_age = pd.DataFrame(rows)\n",
    "order = pd.Categorical(scores_by_age[\"age_group\"], categories=[\"young\",\"middle\",\"old\",\"unknown\"], ordered=True)\n",
    "scores_by_age = (scores_by_age.assign(_ord=order).sort_values(\"_ord\").drop(columns=\"_ord\").reset_index(drop=True))\n",
    "scores_by_age[[\"acc\",\"macro_f1\"]] = scores_by_age[[\"acc\",\"macro_f1\"]].round(4)\n",
    "\n",
    "print(\"\\n[Age-group metrics]\")\n",
    "print(scores_by_age[[\"age_group\",\"acc\",\"macro_f1\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c079e-1158-4993-abb4-c529f2fdffbb",
   "metadata": {},
   "source": [
    "## 9) Katib 하이퍼파라미터 튜닝 & 추출값 확인\n",
    "- Katib 하이퍼파라미터 튜닝한 결과값을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3bad0-ba33-40af-9393-36f3977554c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "\n",
    "KATIB_NS  = os.environ.get(\"KATIB_NS\",  \"kbm-u-kubeflow-tutorial\")\n",
    "KATIB_EXP = os.environ.get(\"KATIB_EXP\", \"gender-logistic-random\")\n",
    "\n",
    "def fetch_katib_params(ns: str, exp: str):\n",
    "    out = subprocess.check_output(\n",
    "        [\"kubectl\", \"-n\", ns, \"get\", \"experiment\", exp, \"-o\", \"json\"],\n",
    "        stderr=subprocess.STDOUT\n",
    "    )\n",
    "    data = json.loads(out.decode(\"utf-8\"))\n",
    "    trial = (\n",
    "        data.get(\"status\", {}).get(\"currentOptimalTrial\") or\n",
    "        data.get(\"status\", {}).get(\"optimalTrial\") or\n",
    "        data.get(\"status\", {}).get(\"bestTrial\") or\n",
    "        {}\n",
    "    )\n",
    "    params = trial.get(\"parameterAssignments\", [])\n",
    "    if not params:\n",
    "        raise RuntimeError(\"Katib parameterAssignments를 찾지 못했습니다.\")\n",
    "    return params\n",
    "\n",
    "KATIB_PARAMS_LIST = fetch_katib_params(KATIB_NS, KATIB_EXP)\n",
    "KATIB_PARAM_MAP   = {d[\"name\"]: str(d[\"value\"]).strip() for d in KATIB_PARAMS_LIST}\n",
    "\n",
    "print(f\"[Katib] {KATIB_NS}/{KATIB_EXP} 최적 하이퍼파라미터:\")\n",
    "display(pd.DataFrame(KATIB_PARAMS_LIST))\n",
    "print(\"\\nparam map:\", KATIB_PARAM_MAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9e4b3-dc1b-4a28-a93e-0ebb39b36fdc",
   "metadata": {},
   "source": [
    "## 10) Katib 최적 하이퍼파라미터로 모델 학습\n",
    "- Katib에서 찾은 최적의 하이퍼파라미터로 모델을 재학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acad8c-9d1d-4c4e-a779-61fe753f2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "def _get_str(m, k, default=None):\n",
    "    v = m.get(k, default)\n",
    "    return None if v is None else str(v).strip()\n",
    "\n",
    "def _get_float(m, k, default=None):\n",
    "    v = m.get(k, None)\n",
    "    try: return float(v)\n",
    "    except: return default\n",
    "\n",
    "def _get_int(m, k, default=None):\n",
    "    v = m.get(k, None)\n",
    "    try: return int(float(v))\n",
    "    except: return default\n",
    "\n",
    "# ---- Katib 파라미터 파싱 ----\n",
    "penalty   = _get_str(KATIB_PARAM_MAP, \"penalty\", \"l2\").lower()\n",
    "C         = _get_float(KATIB_PARAM_MAP, \"C\", 1.0)\n",
    "l1_ratio  = _get_float(KATIB_PARAM_MAP, \"l1_ratio\", None)\n",
    "scaler_nm = _get_str(KATIB_PARAM_MAP, \"scaler\", \"standard\").lower()\n",
    "cw_nm     = _get_str(KATIB_PARAM_MAP, \"class_weight\", None)\n",
    "class_weight = None if (cw_nm is None or cw_nm.lower()==\"none\") else (\"balanced\" if cw_nm.lower()==\"balanced\" else cw_nm)\n",
    "\n",
    "calib_method = _get_str(KATIB_PARAM_MAP, \"calib_method\", \"isotonic\").lower()\n",
    "if calib_method not in {\"isotonic\",\"sigmoid\"}:\n",
    "    calib_method = \"isotonic\"\n",
    "calib_cv   = _get_int(KATIB_PARAM_MAP, \"calib_cv\", 5)\n",
    "\n",
    "t_lo    = _get_float(KATIB_PARAM_MAP, \"tune_t_lo\", 0.2)\n",
    "t_hi    = _get_float(KATIB_PARAM_MAP, \"tune_t_hi\", 0.8)\n",
    "t_steps = _get_int (KATIB_PARAM_MAP, \"tune_t_steps\", 61)\n",
    "if t_lo is not None and t_hi is not None and t_lo > t_hi:\n",
    "    t_lo, t_hi = t_hi, t_lo\n",
    "\n",
    "min_prelogin_events_k = _get_int(KATIB_PARAM_MAP, \"min_prelogin_events\", None)\n",
    "base_min = globals().get(\"MIN_PRELOGIN_EVENTS\", None)\n",
    "min_events_used = min_prelogin_events_k if min_prelogin_events_k is not None else base_min\n",
    "\n",
    "# ---- (필요시) 피처 재구성 및 학습/검증 분할 보장 ----\n",
    "def _norm(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "    return s.apply(lambda x: unicodedata.normalize(\"NFKC\", x) if pd.notna(x) else x)\n",
    "\n",
    "def _rebuild_features_with_min(df_raw: pd.DataFrame, min_events: int):\n",
    "    df2 = df_raw.copy()\n",
    "    df2[\"ts\"] = pd.to_datetime(df2[\"timestamp\"], errors=\"coerce\")\n",
    "    df2 = df2.dropna(subset=[\"session_id\",\"ts\"]).copy()\n",
    "    sort_cols = [\"session_id\",\"ts\"] + ([\"event_id\"] if \"event_id\" in df2.columns else [])\n",
    "    df2 = df2.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    uid_str  = df2[\"user_id\"].astype(\"string\").str.strip()\n",
    "    anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "    has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "    appeared_local = has_uid.groupby(df2[\"session_id\"]).cummax()\n",
    "    pre_local = df2.loc[~appeared_local].copy()\n",
    "\n",
    "    pre_cnt = pre_local.groupby(\"session_id\").size()\n",
    "    valid_sessions = pre_cnt.index[pre_cnt >= int(min_events)]\n",
    "    pre_local = pre_local[pre_local[\"session_id\"].isin(valid_sessions)].copy()\n",
    "\n",
    "    df_gender = df2.copy()\n",
    "    df_gender[\"gender_norm\"] = (\n",
    "        df_gender.get(\"gender\", pd.Series(index=df_gender.index, dtype=\"object\"))\n",
    "                 .astype(\"string\").str.strip().str.upper()\n",
    "                 .replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    "    )\n",
    "    lab_full = (\n",
    "        df_gender[df_gender[\"gender_norm\"].isin([\"M\",\"F\"])]\n",
    "          .groupby(\"session_id\")[\"gender_norm\"].agg(lambda s: s.iloc[0])\n",
    "    )\n",
    "\n",
    "    agg_num = pre_local.groupby(\"session_id\").agg(\n",
    "        n_events=(\"session_id\",\"size\"),\n",
    "        search_count_sum=(\"search_count\",\"sum\") if \"search_count\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "        cart_item_count_sum=(\"cart_item_count\",\"sum\") if \"cart_item_count\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "        page_depth_mean=(\"page_depth\",\"mean\") if \"page_depth\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "        last_elapsed_mean=(\"last_action_elapsed\",\"mean\") if \"last_action_elapsed\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "        unique_pages=(\"current_state\",\"nunique\") if \"current_state\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "        unique_categories=(\"resolved_category\",\"nunique\") if \"resolved_category\" in pre_local.columns else (\"session_id\",\"size\"),\n",
    "    ).fillna(0.0)\n",
    "    first_ts = pre_local.groupby(\"session_id\")[\"ts\"].min()\n",
    "    agg_num[\"start_hour\"] = first_ts.dt.hour\n",
    "    agg_num[\"start_weekday\"] = first_ts.dt.weekday\n",
    "\n",
    "    KNOWN_CATS_LOCAL = globals().get(\"KNOWN_CATS\", [\"Books\",\"Electronics\",\"Gaming\",\"Home\",\"Fashion\"])\n",
    "    KNOWN_CATS_NORM = [unicodedata.normalize(\"NFKC\", c.strip()) for c in KNOWN_CATS_LOCAL]\n",
    "    norm_to_orig = {unicodedata.normalize(\"NFKC\", c.strip()): c for c in KNOWN_CATS_LOCAL}\n",
    "    pre_cat_norm = _norm(pre_local.get(\"resolved_category\", pd.Series(index=pre_local.index, dtype=\"object\")))\n",
    "    mask = pre_cat_norm.isin(KNOWN_CATS_NORM)\n",
    "    pre_kept = pre_local[mask].copy()\n",
    "    pre_kept[\"cat_norm\"] = pre_cat_norm[mask].values\n",
    "    pre_kept[\"one\"] = 1\n",
    "    cat_cnt = pre_kept.pivot_table(index=\"session_id\", columns=\"cat_norm\", values=\"one\", aggfunc=\"sum\", fill_value=0)\n",
    "    cat_cnt = cat_cnt.reindex(columns=KNOWN_CATS_NORM, fill_value=0)\n",
    "    cat_cnt.columns = [norm_to_orig[c] for c in cat_cnt.columns]\n",
    "    cat_cnt.columns = [f\"cat_cnt::{c}\" for c in cat_cnt.columns]\n",
    "    cat_cnt = cat_cnt.reindex(agg_num.index).fillna(0).astype(int)\n",
    "\n",
    "    cat_prop = cat_cnt.div(agg_num[\"n_events\"].replace(0,1), axis=0)\n",
    "    cat_prop.columns = [c.replace(\"cat_cnt::\",\"cat_prop::\") for c in cat_cnt.columns]\n",
    "    cat_log = np.log1p(cat_cnt)\n",
    "    cat_log.columns = [c.replace(\"cat_cnt::\",\"cat_log::\") for c in cat_cnt.columns]\n",
    "\n",
    "    X = (agg_num.join(cat_cnt, how=\"left\").join(cat_prop, how=\"left\").join(cat_log, how=\"left\")).fillna(0)\n",
    "    keep_sessions = X.index.intersection(lab_full.index)\n",
    "    X = X.loc[keep_sessions].copy()\n",
    "    y = lab_full.loc[keep_sessions].rename(\"gender\").copy()\n",
    "\n",
    "    X_ = X.copy(); X_.index.name = \"session_id\"\n",
    "    dataset_local = X_.join(y).reset_index()\n",
    "    dataset_local[\"gender\"] = (\n",
    "        dataset_local[\"gender\"].astype(\"string\").str.strip().str.upper()\n",
    "        .replace({\"FEMALE\":\"F\",\"MALE\":\"M\"})\n",
    "    )\n",
    "    non_feature = {\"session_id\",\"gender\"}\n",
    "    feature_cols = [c for c in dataset_local.columns if c not in non_feature]\n",
    "    X_all = dataset_local[feature_cols].copy()\n",
    "    y_bin = dataset_local[\"gender\"].map({\"F\":0,\"M\":1}).astype(int)\n",
    "\n",
    "    RAND = globals().get(\"RANDOM_STATE\", 42)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RAND)\n",
    "    train_idx_local, valid_idx_local = next(sss.split(X_all, y_bin))\n",
    "    return {\n",
    "        \"dataset\": dataset_local,\n",
    "        \"X_train\": X_all.iloc[train_idx_local], \"X_valid\": X_all.iloc[valid_idx_local],\n",
    "        \"y_train\": y_bin.iloc[train_idx_local], \"y_valid\": y_bin.iloc[valid_idx_local],\n",
    "    }\n",
    "\n",
    "need_rebuild = (\n",
    "    min_prelogin_events_k is not None and\n",
    "    \"MIN_PRELOGIN_EVENTS\" in globals() and\n",
    "    int(min_prelogin_events_k) != int(MIN_PRELOGIN_EVENTS)\n",
    ")\n",
    "if need_rebuild:\n",
    "    assert \"df\" in globals(), \"원본 df가 필요합니다(df).\"\n",
    "    rebuilt = _rebuild_features_with_min(df, int(min_prelogin_events_k))\n",
    "    dataset = rebuilt[\"dataset\"]\n",
    "    X_train, X_valid = rebuilt[\"X_train\"], rebuilt[\"X_valid\"]\n",
    "    y_train, y_valid = rebuilt[\"y_train\"], rebuilt[\"y_valid\"]\n",
    "else:\n",
    "    if not all(v in globals() for v in [\"X_train\",\"X_valid\",\"y_train\",\"y_valid\"]):\n",
    "        assert \"dataset\" in globals(), \"dataset이 필요합니다.\"\n",
    "        non_feature = {\"session_id\",\"gender\"}\n",
    "        feature_cols = [c for c in dataset.columns if c not in non_feature]\n",
    "        X_all = dataset[feature_cols].copy()\n",
    "        y_bin = dataset[\"gender\"].map({\"F\":0,\"M\":1}).astype(int)\n",
    "        RAND = globals().get(\"RANDOM_STATE\", 42)\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RAND)\n",
    "        tr_idx, va_idx = next(sss.split(X_all, y_bin))\n",
    "        X_train, X_valid = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        y_train, y_valid = y_bin.iloc[tr_idx], y_bin.iloc[va_idx]\n",
    "\n",
    "# ---- 반복 학습 설정 ----\n",
    "RANDOM_STATE = globals().get(\"RANDOM_STATE\", 42)\n",
    "ScalerClass  = StandardScaler if scaler_nm == \"standard\" else RobustScaler\n",
    "\n",
    "clf_kwargs = dict(solver=\"saga\", penalty=penalty, C=float(C), max_iter=5000, random_state=RANDOM_STATE)\n",
    "if penalty == \"elasticnet\" and l1_ratio is not None:\n",
    "    clf_kwargs[\"l1_ratio\"] = float(l1_ratio)\n",
    "if class_weight is not None:\n",
    "    clf_kwargs[\"class_weight\"] = class_weight\n",
    "\n",
    "# 반복 파라미터(전역에서 덮어쓸 수 있음)\n",
    "N_REPEATS = 20\n",
    "PATIENCE = 10\n",
    "BOOTSTRAP = True\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def _make_scaler():\n",
    "    if ScalerClass is StandardScaler:\n",
    "        return ScalerClass(with_mean=True, with_std=True)\n",
    "    else:\n",
    "        return ScalerClass(with_centering=True, with_scaling=True)\n",
    "\n",
    "def _build_calibrated_model():\n",
    "    pipe = Pipeline([(\"scaler\", _make_scaler()), (\"clf\", LogisticRegression(**clf_kwargs))])\n",
    "    try:\n",
    "        cal = CalibratedClassifierCV(estimator=pipe, method=calib_method, cv=calib_cv)\n",
    "    except TypeError:\n",
    "        cal = CalibratedClassifierCV(base_estimator=pipe, method=calib_method, cv=calib_cv)\n",
    "    return cal\n",
    "\n",
    "def _bootstrap_xy(X, y, seed):\n",
    "    if not BOOTSTRAP:\n",
    "        return X, y\n",
    "    rs = np.random.RandomState(seed)\n",
    "    idx = rs.choice(len(X), size=len(X), replace=True)\n",
    "    return X.iloc[idx], y.iloc[idx]\n",
    "\n",
    "def _tune_threshold_from_proba(y_true, proba, lo=0.2, hi=0.8, steps=61):\n",
    "    ths = np.linspace(lo, hi, steps)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in ths:\n",
    "        y_pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1 = float(t), float(f1)\n",
    "    acc = (y_true == (proba >= best_t).astype(int)).mean()\n",
    "    return best_t, best_f1, float(acc)\n",
    "\n",
    "class AvgEnsemble:\n",
    "    \"\"\"여러 Calibrated 모델의 양성 확률을 평균해 predict_proba 반환 (sklearn 호환)\"\"\"\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    def predict_proba(self, X):\n",
    "        probs = [m.predict_proba(X)[:, 1] for m in self.models]\n",
    "        mean_p1 = np.mean(np.vstack(probs), axis=0)\n",
    "        return np.c_[1.0 - mean_p1, mean_p1]\n",
    "\n",
    "# ---- 반복 학습 루프 ----\n",
    "rng_master = np.random.RandomState(RANDOM_STATE)\n",
    "models = []\n",
    "probas_valid_each = []\n",
    "best_score = -1.0\n",
    "no_improve = 0\n",
    "best_state = {\"n_models\": 0, \"threshold\": 0.5, \"f1\": -1.0, \"acc\": -1.0}\n",
    "\n",
    "for r in range(N_REPEATS):\n",
    "    seed = int(rng_master.randint(0, 10_000_000))\n",
    "    X_tr, y_tr = _bootstrap_xy(X_train, y_train, seed)\n",
    "\n",
    "    cal_model = _build_calibrated_model()\n",
    "    cal_model.fit(X_tr, y_tr)\n",
    "\n",
    "    p_valid = cal_model.predict_proba(X_valid)[:, 1]\n",
    "    t1, f1_1, acc_1 = _tune_threshold_from_proba(y_valid, p_valid, lo=t_lo or 0.2, hi=t_hi or 0.8, steps=t_steps or 61)\n",
    "    print(f\"[{r+1:02d}/{N_REPEATS}] Single  F1={f1_1:.4f}  ACC={acc_1:.4f}  (t*={t1:.3f})\")\n",
    "\n",
    "    models.append(deepcopy(cal_model))\n",
    "    probas_valid_each.append(p_valid)\n",
    "\n",
    "    ens_proba = np.mean(np.vstack(probas_valid_each), axis=0)\n",
    "    tE, f1_E, acc_E = _tune_threshold_from_proba(y_valid, ens_proba, lo=t_lo or 0.2, hi=t_hi or 0.8, steps=t_steps or 61)\n",
    "    print(f\"             Ensemble({len(models)}) F1={f1_E:.4f}  ACC={acc_E:.4f}  (t*={tE:.3f})\")\n",
    "\n",
    "    if f1_E > best_score:\n",
    "        best_score = f1_E\n",
    "        best_state = {\"n_models\": len(models), \"threshold\": tE, \"f1\": f1_E, \"acc\": acc_E}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stop: no improvement for {PATIENCE} rounds.\")\n",
    "            break\n",
    "\n",
    "# ---- 최종 객체 바인딩 (다음 셀 호환)\n",
    "# log_katib: 마지막 학습된 단일 모델(덤프 용)\n",
    "log_katib = models[best_state[\"n_models\"] - 1] if best_state[\"n_models\"] >= 1 else models[-1]\n",
    "# cal_katib: 앙상블 래퍼 (Cell 11/12가 그대로 사용)\n",
    "cal_katib = AvgEnsemble(models[:best_state[\"n_models\"]]) if best_state[\"n_models\"] >= 1 else AvgEnsemble(models)\n",
    "\n",
    "print(f\"\\n[OK] 반복 학습 완료 → min_prelogin_events={min_events_used}, scaler={scaler_nm}, \"\n",
    "      f\"penalty={penalty}, C={C}, class_weight={class_weight}, calibration=({calib_method}, cv={calib_cv})\")\n",
    "print(f\"[BEST] Ensemble k={best_state['n_models']}  macro-F1={best_state['f1']:.4f}  acc={best_state['acc']:.4f}  \"\n",
    "      f\"(t*={best_state['threshold']:.3f}, repeats={N_REPEATS}, bootstrap={BOOTSTRAP}, patience={PATIENCE})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa3f8d-7ba4-42ac-97d2-5ca75f7d4901",
   "metadata": {},
   "source": [
    "## 11) Katib 모델 평가 밎 저장\n",
    "- 재학습한 모델을 평가하고 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011a95c-b84a-482d-afd5-c10114c7fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "\n",
    "# 필수 객체 확인\n",
    "assert all(k in globals() for k in [\n",
    "    \"tune_threshold\",\"evaluate\",\"print_scores\",\n",
    "    \"X_valid\",\"y_valid\",\"dataset\",\"df\",\"ARTIFACT_DIR\",\n",
    "    \"cal_katib\",\"log_katib\"  # log_katib도 저장/메타 위해 필요\n",
    "]), \\\n",
    "    \"필요 변수/함수(tune_threshold/evaluate/print_scores, X_valid/y_valid/dataset/df/ARTIFACT_DIR/cal_katib/log_katib)가 필요합니다.\"\n",
    "\n",
    "# ----- 헬퍼들 -----\n",
    "def _unwrap_pipeline(obj):\n",
    "    \"\"\"Pipeline(또는 그 안의 Pipeline)을 찾아 반환. 없으면 None.\"\"\"\n",
    "    # 1) 이미 Pipeline\n",
    "    if hasattr(obj, \"named_steps\"):\n",
    "        return obj\n",
    "    # 2) CalibratedClassifierCV(estimator|base_estimator=Pipeline)\n",
    "    for attr in (\"estimator\", \"base_estimator\"):\n",
    "        inner = getattr(obj, attr, None)\n",
    "        if inner is not None and hasattr(inner, \"named_steps\"):\n",
    "            return inner\n",
    "    # 3) CalibratedClassifierCV 내부 fold에서 접근\n",
    "    if hasattr(obj, \"calibrated_classifiers_\") and obj.calibrated_classifiers_:\n",
    "        try:\n",
    "            inner = obj.calibrated_classifiers_[0].classifier\n",
    "            if hasattr(inner, \"named_steps\"):\n",
    "                return inner\n",
    "            base = getattr(inner, \"base_estimator\", None)\n",
    "            if base is not None and hasattr(base, \"named_steps\"):\n",
    "                return base\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _to_jsonable(v):\n",
    "    if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "        return v\n",
    "    if hasattr(v, \"get_params\"):\n",
    "        try:\n",
    "            return {\"name\": v.__class__.__name__, \"params\": v.get_params(deep=False)}\n",
    "        except Exception:\n",
    "            return {\"name\": v.__class__.__name__}\n",
    "    return str(v)\n",
    "\n",
    "def _safe_dump(obj, path):\n",
    "    \"\"\"joblib.dump를 안전하게 수행(실패 시 False 반환).\"\"\"\n",
    "    from joblib import dump\n",
    "    try:\n",
    "        dump(obj, path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] dump 실패: {path} ({type(obj).__name__}) → {e}\")\n",
    "        return False\n",
    "\n",
    "# ----- 임계값 탐색 범위 -----\n",
    "t_lo    = float(globals().get(\"t_lo\", 0.2))\n",
    "t_hi    = float(globals().get(\"t_hi\", 0.8))\n",
    "t_steps = int(globals().get(\"t_steps\", 61))\n",
    "if t_lo > t_hi:\n",
    "    t_lo, t_hi = t_hi, t_lo\n",
    "\n",
    "# ----- cal_katib(AvgEnsemble 포함)으로 최적 임계값 재탐색 → 평가/출력 -----\n",
    "best_t, best_f1, _ = tune_threshold(cal_katib, X_valid, y_valid, search=(t_lo, t_hi, t_steps))\n",
    "print(f\"[Katib HP/Ensemble] Best macro-F1 within [{t_lo:.3f}, {t_hi:.3f}] (steps={t_steps}): t={best_t:.3f}, F1={best_f1:.4f}\")\n",
    "\n",
    "_ , _ = evaluate(\n",
    "    cal_katib, X_valid, y_valid,\n",
    "    threshold=best_t,\n",
    "    name=f\"KatibCal/Ensemble (best_t={best_t:.3f})\"\n",
    ")\n",
    "\n",
    "# 앙상블/단일 모두 호환되는 확률 얻기\n",
    "if hasattr(cal_katib, \"predict_proba\"):\n",
    "    proba_k = cal_katib.predict_proba(X_valid)[:, 1]\n",
    "else:\n",
    "    # predict_proba가 없다면 evaluate 내부에서 y_hat만 계산되므로, 여기선 불가\n",
    "    raise RuntimeError(\"cal_katib이 predict_proba를 제공하지 않습니다.\")\n",
    "\n",
    "print_scores(f\"KatibCal/Ensemble (best@{best_t:.3f})\", y_valid, proba_k, best_t)\n",
    "\n",
    "# ----- 결과 테이블 구성 -----\n",
    "if \"session_id\" in dataset.columns:\n",
    "    sess_ids = dataset.loc[X_valid.index, \"session_id\"].to_numpy()\n",
    "else:\n",
    "    sess_ids = np.arange(len(X_valid))\n",
    "\n",
    "pred_bin = (proba_k >= best_t).astype(int)\n",
    "pred_col = f\"y_pred@{best_t:.3f}\"\n",
    "\n",
    "out_df = pd.DataFrame({\n",
    "    \"session_id\": sess_ids,\n",
    "    \"y_true\": y_valid.values,\n",
    "    pred_col: pred_bin,\n",
    "    \"proba_cal\": proba_k,\n",
    "})\n",
    "\n",
    "# 추가 컬럼(age/age_group) 병합\n",
    "extra_cols = [c for c in [\"age\", \"age_group\"] if c in dataset.columns]\n",
    "if extra_cols:\n",
    "    out_df = out_df.merge(\n",
    "        dataset[[\"session_id\"] + extra_cols].drop_duplicates(\"session_id\"),\n",
    "        on=\"session_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "# 로그인 이후(post)에서 age를 세션별 대표값으로 추출해 그룹 생성 (age_group 없을 때)\n",
    "uid_str  = df.get(\"user_id\", pd.Series([\"\"] * len(df))).astype(\"string\").str.strip()\n",
    "anon_like = {\"\", \"0\", \"-1\", \"None\", \"none\", \"NULL\", \"null\", \"NaN\", \"nan\"}\n",
    "has_uid = uid_str.notna() & ~uid_str.isin(anon_like)\n",
    "appeared_local = has_uid.groupby(df[\"session_id\"]).cummax()\n",
    "post = df.loc[appeared_local].copy()\n",
    "\n",
    "metrics_df = out_df.copy()\n",
    "if \"age_group\" not in metrics_df.columns and \"age\" in post.columns:\n",
    "    age_post_by_sess = (\n",
    "        pd.to_numeric(post[\"age\"], errors=\"coerce\")\n",
    "          .groupby(post[\"session_id\"])\n",
    "          .apply(lambda s: s.dropna().iloc[0] if len(s.dropna()) else np.nan)\n",
    "          .rename(\"age\")\n",
    "    )\n",
    "    age_map = age_post_by_sess.to_frame()\n",
    "\n",
    "    def _to_age_bucket(x):\n",
    "        if pd.isna(x): return np.nan\n",
    "        x = float(x)\n",
    "        return \"young\" if x < 25 else (\"middle\" if x < 50 else \"old\")\n",
    "\n",
    "    age_map[\"age_group\"] = age_map[\"age\"].apply(_to_age_bucket)\n",
    "    metrics_df = metrics_df.merge(age_map.reset_index(), on=\"session_id\", how=\"left\")\n",
    "\n",
    "metrics_df[\"age_group\"] = metrics_df.get(\"age_group\").astype(\"string\").fillna(\"unknown\")\n",
    "\n",
    "rows = []\n",
    "for g, sub in metrics_df.groupby(\"age_group\", dropna=False):\n",
    "    y_true = sub[\"y_true\"].astype(int).to_numpy()\n",
    "    y_hat  = sub[pred_col].astype(int).to_numpy()\n",
    "    acc = float((y_true == y_hat).mean())\n",
    "    f1  = float(f1_score(y_true, y_hat, average=\"macro\")) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    rows.append({\"age_group\": str(g), \"acc\": acc, \"macro_f1\": f1})\n",
    "\n",
    "scores_by_age = pd.DataFrame(rows).sort_values(\"age_group\")\n",
    "scores_by_age[[\"acc\",\"macro_f1\"]] = scores_by_age[[\"acc\",\"macro_f1\"]].round(4)\n",
    "print(\"\\n[Age-group metrics — Katib/Ensemble @ best_t]\")\n",
    "print(scores_by_age[[\"age_group\",\"acc\",\"macro_f1\"]].to_string(index=False))\n",
    "\n",
    "# ----- 아티팩트 저장 -----\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) 기본(참조) 모델 저장: log_katib\n",
    "#    log_katib이 Pipeline이 아닐 수 있으므로 언랩 후 저장 시도\n",
    "log_ref = _unwrap_pipeline(log_katib) or log_katib\n",
    "saved_model_ok = _safe_dump(log_ref, os.path.join(ARTIFACT_DIR, \"model.joblib\"))\n",
    "\n",
    "# 2) 보정/앙상블 모델 저장\n",
    "ensemble_info = None\n",
    "if hasattr(cal_katib, \"models\"):  # AvgEnsemble\n",
    "    # 멤버별로 저장 + 메타만 기록(래퍼 직렬화 실패 가능성 고려)\n",
    "    ens_dir = Path(ARTIFACT_DIR) / \"ensemble\"\n",
    "    ens_dir.mkdir(parents=True, exist_ok=True)\n",
    "    member_paths = []\n",
    "    for i, m in enumerate(cal_katib.models, 1):\n",
    "        p = ens_dir / f\"member_{i:02d}.joblib\"\n",
    "        _safe_dump(m, p)\n",
    "        member_paths.append(str(p))\n",
    "    ensemble_info = {\n",
    "        \"type\": \"AvgEnsemble\",\n",
    "        \"k\": len(cal_katib.models),\n",
    "        \"members\": member_paths\n",
    "    }\n",
    "    # 래퍼 자체 저장도 시도(성공하면 그대로 사용 가능)\n",
    "    _safe_dump(cal_katib, os.path.join(ARTIFACT_DIR, \"model_calibrated.joblib\"))\n",
    "else:\n",
    "    # 단일 CalibratedClassifierCV 또는 Pipeline이면 그대로 저장\n",
    "    _safe_dump(cal_katib, os.path.join(ARTIFACT_DIR, \"model_calibrated.joblib\"))\n",
    "\n",
    "# 3) 메타 정보 저장\n",
    "#    - best_params_jsonable: 가능한 경우 내부 Pipeline의 scaler/clf 파라미터를 기록\n",
    "pipe_for_meta = _unwrap_pipeline(log_ref)\n",
    "if pipe_for_meta is not None:\n",
    "    best_params_jsonable = {\n",
    "        \"scaler\": _to_jsonable(pipe_for_meta.named_steps[\"scaler\"]),\n",
    "        \"clf\": _to_jsonable(pipe_for_meta.named_steps[\"clf\"]),\n",
    "    }\n",
    "else:\n",
    "    # 파이프라인이 아니면 최소한의 정보만 기록\n",
    "    best_params_jsonable = {\n",
    "        \"model_repr\": str(type(log_ref)),\n",
    "        \"model_params\": _to_jsonable(getattr(log_ref, \"get_params\", lambda: {})())\n",
    "    }\n",
    "\n",
    "calib_method_meta = globals().get(\"calib_method\", \"isotonic\")\n",
    "calib_cv_meta     = int(globals().get(\"calib_cv\", 5))\n",
    "\n",
    "katib_param_map_meta = globals().get(\"KATIB_PARAM_MAP\", {})\n",
    "with open(os.path.join(ARTIFACT_DIR, \"model_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"source\": \"katib\",\n",
    "        \"best_params\": best_params_jsonable,\n",
    "        \"best_cv_f1_macro\": None,\n",
    "        \"production_threshold\": float(best_t),\n",
    "        \"calibration\": {\"method\": calib_method_meta, \"cv\": calib_cv_meta},\n",
    "        \"threshold_tuning\": {\"lo\": float(t_lo), \"hi\": float(t_hi), \"steps\": int(t_steps)},\n",
    "        \"katib_param_map\": katib_param_map_meta,\n",
    "        \"ensemble\": ensemble_info\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n[Saved/Overwritten] model.joblib (ok={saved_model_ok}), model_calibrated.joblib, model_meta.json → {ARTIFACT_DIR}\")\n",
    "\n",
    "# 후속 셀 호환을 위해 유지\n",
    "valid_idx = X_valid.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1e746-a4e6-4a03-8a88-da856b2d5475",
   "metadata": {},
   "source": [
    "## 12) 성별 추론 결과값 시각화\n",
    "- Katib 하이퍼파라미터로 추론한 결과값을 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43832854-acf9-4689-b645-e8ebfa3f9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 필수 값 체크\n",
    "assert all(k in globals() for k in [\"best_t\", \"proba_k\", \"y_valid\", \"scores_by_age\"]), \\\n",
    "    \"Cell 11을 먼저 실행해 best_t, proba_k, y_valid, scores_by_age를 준비하세요.\"\n",
    "\n",
    "t = float(best_t)\n",
    "y_true = np.asarray(y_valid).astype(int)\n",
    "y_prob = np.asarray(proba_k).astype(float)\n",
    "y_pred = (y_prob >= t).astype(int)\n",
    "\n",
    "# total 성능\n",
    "total_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "total_acc = (y_true == y_pred).mean()\n",
    "\n",
    "# age-group 성능 (young, middle, old 사용)\n",
    "labels = [\"total\", \"young\", \"middle\", \"old\"]\n",
    "f1_vals = [total_f1]\n",
    "acc_vals = [total_acc]\n",
    "\n",
    "sba = scores_by_age.set_index(\"age_group\")\n",
    "for g in [\"young\", \"middle\", \"old\"]:\n",
    "    if g in sba.index:\n",
    "        f1_vals.append(float(sba.loc[g, \"macro_f1\"]))\n",
    "        acc_vals.append(float(sba.loc[g, \"acc\"]))\n",
    "    else:\n",
    "        f1_vals.append(np.nan)\n",
    "        acc_vals.append(np.nan)\n",
    "\n",
    "# 막대그래프\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, f1_vals, width, label=\"F1 (macro)\", color=\"green\")\n",
    "rects2 = ax.bar(x + width/2, acc_vals, width, label=\"Accuracy\", color=\"blue\")\n",
    "\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xticks(x, labels)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(f\"Validation Scores by Age Group @ t={t:.3f}\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# 막대 위에 수치 표시\n",
    "def autolabel(rects):\n",
    "    for r in rects:\n",
    "        h = r.get_height()\n",
    "        if np.isnan(h):\n",
    "            continue\n",
    "        ax.annotate(f\"{h:.4f}\",\n",
    "                    xy=(r.get_x() + r.get_width()/2, h),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
