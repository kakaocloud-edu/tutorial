{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25249867-3760-45a1-bb3e-52fec245c9c3",
   "metadata": {},
   "source": [
    "# 세션 이탈\n",
    "### 사용자의 첫 활동부터 K개의 활동까지 분석하여 세션이 완료될지 중간에 이탈할지 추론하는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c10e7-7711-4b02-9e19-b2b23e7bb5ff",
   "metadata": {},
   "source": [
    "# 0) 패키지 설치\n",
    "- 실습에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962590e-2f0c-4b6f-8c91-88dfec598ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec43eb-ccb2-4ded-bdbb-5b9e23f897b9",
   "metadata": {},
   "source": [
    "## 1) 환경 구성\n",
    "- 실습을 원활하게 진행하기 위해 환경을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd433fae-7f93-4f27-bc7e-fd3d540112b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1. 환경 구성 ===\n",
    "import os, re, json, inspect\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix, average_precision_score\n",
    ")\n",
    "from sklearn.utils import check_random_state\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "_imb_ok = True\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except Exception as e:\n",
    "    print(\"[warn] imbalanced-learn를 불러오지 못했습니다. 오버샘플링 비활성화:\", e)\n",
    "    _imb_ok = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = check_random_state(RANDOM_STATE)\n",
    "TOPK_BIGRAMS = 50\n",
    "K_LIST       = [3, 5, 7]\n",
    "TERMINALS_FINAL = {\"/logout\", \"/delete_user\"}\n",
    "\n",
    "OPER_POLICY = \"balanced\" \n",
    "T_OPER_DEFAULT = 0.730\n",
    "OPER_T_OVERRIDE = {3: 0.700, 5: 0.700, 7: 0.620}\n",
    "\n",
    "# 오버샘플링 옵션 - 한 쪽의 데이터가 너무 적을 시 사용\n",
    "# 아래 K별 오버샘플링 옵션 사용 권장\n",
    "OVERSAMPLING = {\n",
    "    \"enable\": False,          # 전역 기본값 ( True = on / False = off )\n",
    "    \"method\": \"smote\",        # \"random\" | \"smote\"\n",
    "    \"ratio\": 0.5,             # 1.0=완전균형, 0.5=소수/다수=0.5\n",
    "    \"k_neighbors\": 5,         # SMOTE용\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "# K별 오버샘플링 override (전역 기본값을 덮어씀)\n",
    "OVERSAMPLING_BY_K = {\n",
    "    3: {\"enable\": False, \"method\": \"smote\", \"ratio\": 0.5, \"k_neighbors\": 5}, # K=3 \n",
    "    5: {\"enable\": False, \"method\": \"smote\", \"ratio\": 0.5, \"k_neighbors\": 5}, # K=5\n",
    "    7: {\"enable\": False, \"method\": \"smote\", \"ratio\": 0.5, \"k_neighbors\": 5}, # K=7\n",
    "}\n",
    "\n",
    "def _merge_oversampling_cfg(global_cfg: dict, per_k_cfg: dict) -> dict:\n",
    "    cfg = dict(global_cfg)\n",
    "    if per_k_cfg:\n",
    "        cfg.update(per_k_cfg)\n",
    "    return cfg\n",
    "\n",
    "def _build_sampler(cfg: dict):\n",
    "    \"\"\"오버샘플러 생성(파이프라인 내부에서만 사용).\"\"\"\n",
    "    if (not cfg.get(\"enable\")) or (not _imb_ok):\n",
    "        return None\n",
    "    method = str(cfg.get(\"method\", \"random\")).lower()\n",
    "    ratio  = cfg.get(\"ratio\", 1.0)\n",
    "    rs     = cfg.get(\"random_state\", RANDOM_STATE)\n",
    "    if method == \"random\":\n",
    "        return RandomOverSampler(sampling_strategy=ratio, random_state=rs)\n",
    "    elif method == \"smote\":\n",
    "        return SMOTE(sampling_strategy=ratio, random_state=rs, k_neighbors=int(cfg.get(\"k_neighbors\", 5)))\n",
    "    else:\n",
    "        print(f\"[warn] 알 수 없는 오버샘플링 방법: {method}. 비활성화합니다.\")\n",
    "        return None\n",
    "\n",
    "# 입력/출력 경로\n",
    "INPUT_PATH = \"datasets/processed_user_behavior.sorted.csv\"\n",
    "OUT_DIR = \"datasets/sessionDrop\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_ALL = os.path.join(OUT_DIR, \"sessionDrop.features.csv\")\n",
    "def OUT_PREFIXF_PATH(k:int) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"sessionDrop.prefix{k}.features.filtered.csv\")\n",
    "\n",
    "MODEL_OUT_DIR = \"models/sessionDrop\"\n",
    "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
    "def MODEL_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.joblib\")\n",
    "def META_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.meta.json\")\n",
    "\n",
    "FEATURE_COLS = [\"timestamp\",\"current_state\",\"page_depth\",\n",
    "                \"delta_search_count\",\"delta_cart_item_count\",\"gap_sec\"]\n",
    "\n",
    "def make_param_dist(seed: int):\n",
    "    r = check_random_state(seed)\n",
    "    return {\n",
    "        \"n_estimators\":      r.randint(600, 1601, size=20),\n",
    "        \"learning_rate\":     r.choice([0.02, 0.03, 0.05, 0.07, 0.1], size=20),\n",
    "        \"max_depth\":         r.choice([3,4,5,6,7], size=20),\n",
    "        \"min_child_weight\":  r.choice([1,2,3,5], size=20),\n",
    "        \"subsample\":         r.uniform(0.6, 1.0, size=20),\n",
    "        \"colsample_bytree\":  r.uniform(0.6, 1.0, size=20),\n",
    "        \"reg_lambda\":        r.choice([0.5,1.0,1.5,2.0], size=20),\n",
    "        \"reg_alpha\":         r.choice([0.0,0.1,0.3,0.5], size=20),\n",
    "    }\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db08ee-9994-4374-b02f-ee8b1cddf8c1",
   "metadata": {},
   "source": [
    "## 2) 원본 데이터 로드\n",
    "- 원본 데이터를 로드하고 전체 세션의 개수를 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347123a-97a8-48d9-88c4-339b1e365600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2. 데이터 로드 ===\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "required = [\"session_id\",\"timestamp\",\"current_state\",\"next_state\",\"page_depth\",\"search_count\",\"cart_item_count\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"필수 컬럼 누락: {missing}\")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"session_id\",\"timestamp\"]).copy()\n",
    "df[\"session_id\"] = df[\"session_id\"].astype(str)\n",
    "df = df.sort_values([\"session_id\",\"timestamp\"])\n",
    "\n",
    "def norm_state(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\?.*$\",\"\", s)\n",
    "    if len(s)>1 and s.endswith(\"/\"):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "df[\"current_state\"] = df[\"current_state\"].map(norm_state)\n",
    "df[\"next_state\"]    = df[\"next_state\"].map(norm_state)\n",
    "\n",
    "print(\"Loaded rows:\", len(df), \"sessions:\", df[\"session_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349630be-6cdb-4676-8c95-de9a9ef399a5",
   "metadata": {},
   "source": [
    "## 3) 간격/증분 계산\n",
    "- 세션 내 이벤트 간 시간차와 누적 지표의 증가분을 만들어 이후 피처 집계에 쓰기 좋게 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0f05c-33bd-401a-93b5-4215700d7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3. 간격/증분 생성 ===\n",
    "df[\"gap_sec\"] = (\n",
    "    df.groupby(\"session_id\")[\"timestamp\"]\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    "      .clip(lower=0)\n",
    ")\n",
    "\n",
    "for col in [\"page_depth\",\"search_count\",\"cart_item_count\"]:\n",
    "    dcol = f\"delta_{col}\"\n",
    "    df[dcol] = df.groupby(\"session_id\")[col].diff()\n",
    "    df[dcol] = df[dcol].fillna(df[col]).clip(lower=0)\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8525c12-f745-40ff-aff1-c3981619e684",
   "metadata": {},
   "source": [
    "## 4) Targit Label 생성 & 누수 세션 식별\n",
    "- 실제 결과인 Label을 생성하고, 모델 학습을 위해 첫 활동부터 K 활동 내에 실제 결과 값이 포함된 세션을 식별합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09409371-2846-4c33-b9d0-55114f77dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4. 세션 라벨 + prefix-K 및 누수 세션 식별 ===\n",
    "is_final = df[\"current_state\"].isin(TERMINALS_FINAL) | df[\"next_state\"].isin(TERMINALS_FINAL)\n",
    "labels = (\n",
    "    df.assign(is_final=is_final)\n",
    "      .groupby(\"session_id\", sort=False)[\"is_final\"]\n",
    "      .any()\n",
    "      .astype(\"int8\")\n",
    "      .reset_index(name=\"label\")\n",
    ")\n",
    "\n",
    "prefix_by_k    = {}\n",
    "term_sid_by_k  = {}\n",
    "\n",
    "for K in K_LIST:\n",
    "    prefix_k = (\n",
    "        df.sort_values([\"session_id\",\"timestamp\"])\n",
    "          .groupby(\"session_id\", group_keys=False)\n",
    "          .head(K)\n",
    "    )\n",
    "    term_sid_k = prefix_k.loc[\n",
    "        prefix_k[\"current_state\"].isin(TERMINALS_FINAL) | prefix_k[\"next_state\"].isin(TERMINALS_FINAL),\n",
    "        \"session_id\"\n",
    "    ].unique()\n",
    "    prefix_by_k[K]   = prefix_k\n",
    "    term_sid_by_k[K] = term_sid_k\n",
    "\n",
    "print(f\"[info] total sessions: {df['session_id'].nunique():,}\")\n",
    "for K in K_LIST:\n",
    "    print(f\"[info] K={K}: sessions reaching terminals in first {K} events (removed): {len(term_sid_by_k[K]):,}\")\n",
    "\n",
    "def bigrams(seq):\n",
    "    a, b = tee(seq)\n",
    "    next(b, None)\n",
    "    for x, y in zip(a, b):\n",
    "        yield (x, y)\n",
    "\n",
    "bg_counter = Counter()\n",
    "TERMINALS = TERMINALS_FINAL\n",
    "for _, g in df.groupby(\"session_id\", sort=False):\n",
    "    seq = list(g[\"current_state\"].fillna(\"\").values)\n",
    "    for bg in bigrams(seq):\n",
    "        if (bg[0] in TERMINALS) or (bg[1] in TERMINALS):\n",
    "            continue\n",
    "        bg_counter[bg] += 1\n",
    "\n",
    "top_bigrams = [bg for bg, _ in bg_counter.most_common(TOPK_BIGRAMS)]\n",
    "bigram_to_idx = {bg:i for i,bg in enumerate(top_bigrams)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c3a31-b3b0-4784-9355-d7ea120d81ff",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "- 모델을 학습하기 위해 데이터를 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14c11e-f07e-4fc7-b9ce-c673240f595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5. 세션 피처 집계 함수 ===\n",
    "def agg_features(g: pd.DataFrame) -> pd.Series:\n",
    "    g = g.sort_values(\"timestamp\")\n",
    "    states = g[\"current_state\"].fillna(\"\").tolist()\n",
    "    gaps   = g[\"gap_sec\"].values\n",
    "\n",
    "    feats = {}\n",
    "    feats[\"n_events\"] = len(g)\n",
    "    feats[\"session_duration_sec\"] = (\n",
    "        (g[\"timestamp\"].iloc[-1] - g[\"timestamp\"].iloc[0]).total_seconds()\n",
    "        if len(g)>1 else 0.0\n",
    "    )\n",
    "    feats[\"events_per_min\"] = feats[\"n_events\"] / max(feats[\"session_duration_sec\"]/60.0, 1e-9)\n",
    "\n",
    "    if len(gaps)>0:\n",
    "        feats[\"gap_mean\"] = float(np.mean(gaps))\n",
    "        feats[\"gap_std\"]  = float(np.std(gaps))\n",
    "        feats[\"gap_p95\"]  = float(np.quantile(gaps, 0.95))\n",
    "    else:\n",
    "        feats[\"gap_mean\"]=feats[\"gap_std\"]=feats[\"gap_p95\"]=0.0\n",
    "\n",
    "    feats[\"page_depth_max\"]   = float(g[\"page_depth\"].max())\n",
    "    feats[\"page_depth_slope\"] = float((g[\"page_depth\"].iloc[-1] - g[\"page_depth\"].iloc[0]) / max(len(g)-1,1))\n",
    "    feats[\"depth_backtracks\"] = int((g[\"page_depth\"].diff()<0).sum())\n",
    "\n",
    "    feats[\"search_delta_sum\"] = float(g[\"delta_search_count\"].sum())\n",
    "    feats[\"search_rate\"]      = feats[\"search_delta_sum\"] / max(feats[\"n_events\"],1)\n",
    "\n",
    "    cart_delta = g[\"delta_cart_item_count\"].clip(lower=0)\n",
    "    feats[\"cart_adds\"]        = float(cart_delta.sum())\n",
    "    feats[\"cart_touch_count\"] = int((cart_delta>0).sum())\n",
    "    feats[\"cart_adds_per_event\"] = feats[\"cart_adds\"] / max(feats[\"n_events\"],1)\n",
    "    feats[\"search_to_cart_ratio\"] = (feats[\"search_delta_sum\"] + 1.0) / (feats[\"cart_adds\"] + 1.0)\n",
    "\n",
    "    first_ts = g[\"timestamp\"].iloc[0]\n",
    "    feats[\"first_event_hour\"] = int(first_ts.hour)\n",
    "    feats[\"is_weekend\"]       = int(first_ts.weekday()>=5)\n",
    "\n",
    "    def idx_of(pred_list, key):\n",
    "        for i, s in enumerate(pred_list):\n",
    "            if key in s: return i\n",
    "        return None\n",
    "    i_search = idx_of(states, \"/search\")\n",
    "    i_cart   = idx_of(states, \"/cart\")\n",
    "    feats[\"time_to_first_search\"] = float((g[\"timestamp\"].iloc[i_search] - first_ts).total_seconds()) if i_search is not None else -1.0\n",
    "    feats[\"time_to_first_cart\"]   = float((g[\"timestamp\"].iloc[i_cart]   - first_ts).total_seconds()) if i_cart   is not None else -1.0\n",
    "\n",
    "    lastk = states[-5:]\n",
    "    joined_lastk = \" \".join(lastk)\n",
    "    feats[\"last5_has_search\"]   = int(\"search\" in joined_lastk)\n",
    "    feats[\"last5_has_cart\"]     = int(\"cart\" in joined_lastk)\n",
    "    feats[\"last5_has_checkout\"] = int(\"checkout\" in joined_lastk)\n",
    "\n",
    "    bg_counts = Counter(list(bigrams(states)))\n",
    "    for bg, idx in bigram_to_idx.items():\n",
    "        feats[f\"bg_{idx}\"] = int(bg_counts.get(bg, 0))\n",
    "\n",
    "    uniq = pd.Series(states).value_counts(normalize=True)\n",
    "    feats[\"unique_states\"] = int(uniq.size)\n",
    "    feats[\"entropy_state\"] = float(-(uniq*np.log(uniq+1e-12)).sum())\n",
    "\n",
    "    st0 = states[0] if states else \"\"\n",
    "    stL = states[-1] if states else \"\"\n",
    "    for name, st in [(\"start\", st0), (\"last\", stL)]:\n",
    "        feats[f\"{name}_is_root\"]      = int(st == \"/\")\n",
    "        feats[f\"{name}_is_search\"]    = int(\"/search\" in st)\n",
    "        feats[f\"{name}_is_products\"]  = int(\"/products\" in st)\n",
    "        feats[f\"{name}_is_cart\"]      = int(\"/cart\" in st)\n",
    "        feats[f\"{name}_is_checkout\"]  = int(\"/checkout\" in st)\n",
    "\n",
    "    all_states = \" \".join(states)\n",
    "    feats[\"hit_search\"]   = int(\"/search\" in all_states)\n",
    "    feats[\"hit_products\"] = int(\"/products\" in all_states)\n",
    "    feats[\"hit_cart\"]     = int(\"/cart\" in all_states)\n",
    "    feats[\"hit_checkout\"] = int(\"/checkout\" in all_states)\n",
    "\n",
    "    return pd.Series(feats)\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6c2bc-e0cf-4452-bb8a-13d310682ca1",
   "metadata": {},
   "source": [
    "## 6) 전처리 데이터 저장\n",
    "- Feature Engineering 한 데이터를 K별로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79ba5e-6fc4-4cba-83a6-dac260622fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6. 세션 피처 테이블 생성 & 저장 ===\n",
    "X_all = (\n",
    "    df.groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "      .apply(agg_features)\n",
    "      .reset_index()\n",
    ")\n",
    "dataset_all = X_all.merge(labels, on=\"session_id\", how=\"left\")\n",
    "dataset_all.to_csv(OUT_ALL, index=False)\n",
    "\n",
    "datasets_by_k = {}\n",
    "print(\"Saved CSVs:\")\n",
    "print(\" -\", OUT_ALL)\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_prefix_k = (\n",
    "        prefix_by_k[K].groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "                      .apply(agg_features)\n",
    "                      .reset_index()\n",
    "    )\n",
    "    dataset_prefix_k = X_prefix_k.merge(labels, on=\"session_id\", how=\"left\")\n",
    "    dataset_prefix_filt_k = dataset_prefix_k[~dataset_prefix_k[\"session_id\"].isin(term_sid_by_k[K])].reset_index(drop=True)\n",
    "    datasets_by_k[K] = dataset_prefix_filt_k\n",
    "    out_path_k = OUT_PREFIXF_PATH(K)\n",
    "    dataset_prefix_filt_k.to_csv(out_path_k, index=False)\n",
    "    print(\" -\", out_path_k, \"| shape:\", tuple(dataset_prefix_filt_k.shape))\n",
    "    \n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0656042-6faa-4caf-a7fd-29fbf36cd4ff",
   "metadata": {},
   "source": [
    "## 7) 학습용 / 검증용 데이터셋 분할\n",
    "- 데이터를 학습용 / 검증용 데이터셋으로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c929dc3-d7bf-49b6-a4d4-b1dc18ca4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7. 학습/검증 분할 (K별) ===\n",
    "splits_by_k = {}\n",
    "for K in K_LIST:\n",
    "    dfp = datasets_by_k[K].copy()\n",
    "    y = dfp[\"label\"].astype(int)\n",
    "    leak_cols = [c for c in dfp.columns if (\"logout\" in c.lower()) or (\"delete\" in c.lower())]\n",
    "    X = dfp.drop(columns=[\"session_id\",\"label\"] + leak_cols, errors=\"ignore\").fillna(0.0)\n",
    "\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE)\n",
    "    splits_by_k[K] = (X_tr, X_va, y_tr, y_va, X.columns.tolist())\n",
    "\n",
    "    print(f\"[K={K}] Shape: {X.shape}  PosRate: {float(y.mean()):.4f}\")\n",
    "    print(f\"[K={K}] Train: {X_tr.shape}  Valid: {X_va.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acc53f-496b-42cd-8122-84eb06647347",
   "metadata": {},
   "source": [
    "## 8) 모델 선택\n",
    "- 학습을 위한 모델을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89db6-9054-4cd1-9e97-813cc5dc8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8. 모델 선택 ===\n",
    "use_xgb = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    use_xgb = True\n",
    "except Exception as e:\n",
    "    print(\"[warn] xgboost 불가, LogisticRegression으로 폴백:\", e)\n",
    "\n",
    "best_by_k = {}             \n",
    "use_pipeline = _imb_ok  \n",
    "\n",
    "def _fixed_xgb(sampler_enabled: bool, y_tr):\n",
    "    \"\"\"튜닝 없이 사용할 고정 XGB 분류기 구성.\"\"\"\n",
    "    pos = int(y_tr.sum()); neg = int(len(y_tr) - pos)\n",
    "    spw = 1.0 if sampler_enabled else (neg / max(pos, 1))\n",
    "    return XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.80,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=spw,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "def _fixed_logreg(sampler_enabled: bool):\n",
    "    \"\"\"튜닝 없이 사용할 고정 로지스틱 회귀 구성.\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    return LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=5000,\n",
    "        class_weight=None if sampler_enabled else \"balanced\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "\n",
    "    cfg_k = _merge_oversampling_cfg(OVERSAMPLING, OVERSAMPLING_BY_K.get(K, {}))\n",
    "    sampler = _build_sampler(cfg_k)\n",
    "    sampler_enabled = sampler is not None\n",
    "\n",
    "    if use_xgb:\n",
    "        base_clf = _fixed_xgb(sampler_enabled, y_tr)\n",
    "    else:\n",
    "        base_clf = _fixed_logreg(sampler_enabled)\n",
    "\n",
    "    if use_pipeline:\n",
    "        steps = []\n",
    "        if sampler_enabled:\n",
    "            steps.append((\"sampler\", sampler))\n",
    "        steps.append((\"clf\", base_clf))\n",
    "        estimator = ImbPipeline(steps)\n",
    "    else:\n",
    "        estimator = base_clf\n",
    "\n",
    "    clf_params = base_clf.get_params(deep=False)\n",
    "    summary_keys = (\n",
    "        [\"n_estimators\", \"learning_rate\", \"max_depth\", \"min_child_weight\",\n",
    "         \"subsample\", \"colsample_bytree\", \"reg_lambda\", \"reg_alpha\"]\n",
    "        if use_xgb else\n",
    "        [\"penalty\", \"C\", \"class_weight\", \"solver\", \"max_iter\"]\n",
    "    )\n",
    "    used_params = {k: clf_params.get(k) for k in summary_keys if k in clf_params}\n",
    "\n",
    "    best_by_k[K] = (estimator, used_params)\n",
    "    print(f\"[K={K}] Using {'XGBClassifier' if use_xgb else 'LogisticRegression'} Params:\", used_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c1596-71dd-4a52-b8df-66df0442493a",
   "metadata": {},
   "source": [
    "## 9) 모델 학습 & 평가\n",
    "- 선택한 모델을 학습하고 검증용 데이터셋으로 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb82746-a643-4eff-9806-653b1ec8b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9. 모델 학습 & 평가 ===\n",
    "\n",
    "def safe_fit_xgb(model, Xtr, ytr, Xva, yva):\n",
    "    \"\"\"Pipeline 또는 단일 모델 모두 지원. XGBClassifier에 eval_set/early_stopping 전달.\"\"\"\n",
    "    is_pipe = hasattr(model, \"named_steps\") and (\"clf\" in getattr(model, \"named_steps\", {}))\n",
    "    clf = model.named_steps[\"clf\"] if is_pipe else model\n",
    "    prefix = \"clf__\" if is_pipe else \"\"\n",
    "\n",
    "    fit_kwargs = {}\n",
    "    sig = inspect.signature(clf.fit)\n",
    "\n",
    "    # eval_set\n",
    "    if \"eval_set\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}eval_set\"] = [(Xva, yva)]\n",
    "\n",
    "    try:\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        if \"callbacks\" in sig.parameters:\n",
    "            fit_kwargs[f\"{prefix}callbacks\"] = [EarlyStopping(rounds=100, save_best=True)]\n",
    "        elif \"early_stopping_rounds\" in sig.parameters:\n",
    "            fit_kwargs[f\"{prefix}early_stopping_rounds\"] = 100\n",
    "    except Exception:\n",
    "        if \"early_stopping_rounds\" in sig.parameters:\n",
    "            fit_kwargs[f\"{prefix}early_stopping_rounds\"] = 100\n",
    "\n",
    "    if \"verbose\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}verbose\"] = False\n",
    "\n",
    "    model.fit(Xtr, ytr, **fit_kwargs)\n",
    "    return model\n",
    "\n",
    "SHOW_05 = False\n",
    "\n",
    "trained_models_by_k = {}\n",
    "eval_by_k = {}\n",
    "T_OPER_BY_K = {}     # K별 운영 임계값 저장\n",
    "summary_rows = []\n",
    "\n",
    "SEP = \"-\" * 86 \n",
    "\n",
    "for K in K_LIST:\n",
    "    X_tr, X_va, y_tr, y_va, feat_names = splits_by_k[K]\n",
    "    best_estimator_, best_params_ = best_by_k[K]\n",
    "\n",
    "    final_model = best_estimator_\n",
    "\n",
    "    if use_xgb:\n",
    "        final_model = safe_fit_xgb(final_model, X_tr, y_tr, X_va, y_va)\n",
    "    else:\n",
    "        final_model.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "    p_va = final_model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    if SHOW_05:\n",
    "        yhat05 = (p_va >= 0.5).astype(int)\n",
    "        print(f\"[K={K}] [Valid @0.50] Acc={accuracy_score(y_va, yhat05):.4f} \"\n",
    "              f\"Macro-F1={f1_score(y_va, yhat05, average='macro'):.4f} \"\n",
    "              f\"PR-AUC={average_precision_score(y_va, p_va):.4f}\")\n",
    "        print(confusion_matrix(y_va, yhat05))\n",
    "        print(classification_report(y_va, yhat05, digits=4))\n",
    "\n",
    "    ths = np.linspace(0.01, 0.99, 197)\n",
    "    def balanced_score(y_true, p, t):\n",
    "        y_pred = (p >= t).astype(int)\n",
    "        return 0.5*(accuracy_score(y_true, y_pred) + f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    t_best, s_best = max(((t, balanced_score(y_va, p_va, t)) for t in ths), key=lambda x: x[1])\n",
    "\n",
    "    if isinstance(OPER_T_OVERRIDE, dict) and (K in OPER_T_OVERRIDE):\n",
    "        op_t = float(OPER_T_OVERRIDE[K])\n",
    "    elif OPER_POLICY == \"balanced\":\n",
    "        op_t = float(t_best)\n",
    "    else:\n",
    "        op_t = float(T_OPER_DEFAULT)\n",
    "    T_OPER_BY_K[K] = op_t\n",
    "\n",
    "    yhat_oper = (p_va >= op_t).astype(int)\n",
    "    acc_oper = accuracy_score(y_va, yhat_oper)\n",
    "    f1_oper  = f1_score(y_va, yhat_oper, average=\"macro\")\n",
    "    print(f\"[K={K}] [Valid @t={op_t:.3f} (Oper/K-specific)] Acc={acc_oper:.4f} Macro-F1={f1_oper:.4f}\")\n",
    "    print(confusion_matrix(y_va, yhat_oper))\n",
    "    print(classification_report(y_va, yhat_oper, digits=4))\n",
    "\n",
    "    print(SEP)\n",
    "    print()\n",
    "\n",
    "    trained_models_by_k[K] = final_model\n",
    "    eval_by_k[K] = {\n",
    "        \"Valid@Oper.Acc\": float(acc_oper),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper),\n",
    "        \"Balanced.t\": float(t_best),\n",
    "        \"Balanced.Score\": float(s_best),\n",
    "        \"Oper.t\": float(op_t),\n",
    "        \"features\": feat_names,\n",
    "    }\n",
    "    summary_rows.append({\n",
    "        \"K\": K,\n",
    "        \"Oper.t\": float(op_t),\n",
    "        \"Valid@Oper.Acc\": float(acc_oper),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper),\n",
    "        \"Balanced.t\": float(t_best),\n",
    "        \"Balanced.Score\": float(s_best),\n",
    "    })\n",
    "\n",
    "print(\"=== Summary (K별) ===\")\n",
    "for r in sorted(summary_rows, key=lambda x: x[\"K\"]):\n",
    "    print(f\"K={r['K']} | Oper.t={r['Oper.t']:.3f} \"\n",
    "          f\"| Acc@Oper={r['Valid@Oper.Acc']:.4f} \"\n",
    "          f\"| MacroF1@Oper={r['Valid@Oper.MacroF1']:.4f} \"\n",
    "          f\"| t_bal={r['Balanced.t']:.3f} | Score_bal={r['Balanced.Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906b71-b29f-409f-95a8-2c6ed4ea0664",
   "metadata": {},
   "source": [
    "## 10) 모델 저장\n",
    "- K별로 모델을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d498f73-a6fd-4760-a2a1-4eb322d1da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10. 모델 & 메타 저장 (K별) ===\n",
    "\n",
    "def to_builtin(v):\n",
    "    if isinstance(v, (np.integer,)):  return int(v)\n",
    "    if isinstance(v, (np.floating,)): return float(v)\n",
    "    if isinstance(v, (np.bool_,)):    return bool(v)\n",
    "    return v\n",
    "\n",
    "print(\"Saving models & meta...\")\n",
    "for K in K_LIST:\n",
    "    model = trained_models_by_k[K]\n",
    "    best_params_ = best_by_k[K][1]\n",
    "    feat_names   = eval_by_k[K][\"features\"]\n",
    "    oper_t       = eval_by_k[K][\"Oper.t\"]   # K별 운영 임계값\n",
    "\n",
    "    model_path = MODEL_PATH_K(K)\n",
    "    dump(model, model_path)\n",
    "\n",
    "    best_params_clean = {k: to_builtin(v) for k, v in (best_params_ or {}).items()}\n",
    "    meta = {\n",
    "        \"variant\": f\"Baseline (prefix{K}; leakage-safe; no transform; no calibration)\",\n",
    "        \"features\": [str(c) for c in feat_names],\n",
    "        \"threshold\": float(oper_t),\n",
    "        \"best_params\": best_params_clean,\n",
    "        \"random_state\": int(RANDOM_STATE),\n",
    "        \"preprocess\": {\n",
    "            \"PREFIX_K\": int(K),\n",
    "            \"TOPK_BIGRAMS\": int(TOPK_BIGRAMS),\n",
    "            \"terminals\": sorted(list(TERMINALS_FINAL)),\n",
    "            \"note\": \"prefix-K 내 terminal 등장 세션 제외, terminal 전이 제외 bigram 사전\"\n",
    "        },\n",
    "        \"oversampling\": {\n",
    "            \"enabled\": bool(OVERSAMPLING.get(\"enable\", False) and _imb_ok),\n",
    "            \"method\": OVERSAMPLING.get(\"method\"),\n",
    "            \"ratio\": float(OVERSAMPLING.get(\"ratio\", 1.0)),\n",
    "            \"k_neighbors\": int(OVERSAMPLING.get(\"k_neighbors\", 5)),\n",
    "        },\n",
    "        \"artifacts_csv\": {\n",
    "            \"all\": OUT_ALL,\n",
    "            \"prefix_filtered\": OUT_PREFIXF_PATH(K)\n",
    "        },\n",
    "        \"validation\": {\n",
    "            \"Acc@Oper\": eval_by_k[K][\"Valid@Oper.Acc\"],\n",
    "            \"MacroF1@Oper\": eval_by_k[K][\"Valid@Oper.MacroF1\"],\n",
    "            \"Balanced_t\": eval_by_k[K][\"Balanced.t\"],\n",
    "            \"Balanced_Score\": eval_by_k[K][\"Balanced.Score\"],\n",
    "        }\n",
    "    }\n",
    "    meta_path = META_PATH_K(K)\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\" - Saved model: {model_path}\")\n",
    "    print(f\" - Saved meta : {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c8d3-ff7b-4b85-a4c0-071c0ced0f9e",
   "metadata": {},
   "source": [
    "## 11) 세션 이탈 추론 결과 값 시각화\n",
    "- 검증한 모델을 평가한 결과 값을 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87ed7c-07a0-4fda-a3da-d98105ce0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11  ===\n",
    "\n",
    "LABELS = [0, 1]\n",
    "LABEL_NAMES = {0: \"0 = Drop\", 1: \"1 = Complete\"}\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "    model = trained_models_by_k[K]\n",
    "    t = T_OPER_BY_K[K]\n",
    "\n",
    "    p = model.predict_proba(X_va)[:, 1]\n",
    "    yhat = (p >= t).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_va, yhat, labels=LABELS)\n",
    "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "    acc = accuracy_score(y_va, yhat)\n",
    "    macro_f1 = f1_score(y_va, yhat, average=\"macro\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4.5))\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            correct = (i == j)\n",
    "            ax.add_patch(plt.Rectangle(\n",
    "                (j - 0.5, i - 0.5), 1, 1,\n",
    "                facecolor=(\"green\" if correct else \"red\"),\n",
    "                edgecolor=\"black\", alpha=0.35\n",
    "            ))\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{cm[i, j]}\\n({cm_norm[i, j]:.2f})\",\n",
    "                ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "    ax.set_xticks([0, 1]); ax.set_xticklabels([LABEL_NAMES[0], LABEL_NAMES[1]], rotation=15, ha=\"right\")\n",
    "    ax.set_yticks([0, 1]); ax.set_yticklabels([LABEL_NAMES[0], LABEL_NAMES[1]])\n",
    "    ax.set_xlim(-0.5, 1.5); ax.set_ylim(1.5, -0.5)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(f\"K={K} | @t={t:.3f} — Acc={acc:.4f}, Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=\"green\", edgecolor=\"black\", label=\"Correct (TP/TN)\"),\n",
    "        Patch(facecolor=\"red\", edgecolor=\"black\", label=\"Incorrect (FP/FN)\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_elems, loc=\"upper right\", frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
