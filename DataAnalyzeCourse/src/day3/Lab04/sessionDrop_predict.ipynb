{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25249867-3760-45a1-bb3e-52fec245c9c3",
   "metadata": {},
   "source": [
    "# 세션 이탈\n",
    "### 사용자의 첫 활동부터 K개의 활동까지 분석하여 세션이 완료될지 중간에 이탈할지 추론하는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c10e7-7711-4b02-9e19-b2b23e7bb5ff",
   "metadata": {},
   "source": [
    "# 0) 패키지 설치\n",
    "- 실습에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962590e-2f0c-4b6f-8c91-88dfec598ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec43eb-ccb2-4ded-bdbb-5b9e23f897b9",
   "metadata": {},
   "source": [
    "## 1) 환경 구성\n",
    "- 실습을 원활하게 진행하기 위해 환경을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd433fae-7f93-4f27-bc7e-fd3d540112b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1. 환경 구성 ===\n",
    "import os, re, json, inspect\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from io import StringIO\n",
    "from joblib import load\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix, average_precision_score\n",
    ")\n",
    "from sklearn.utils import check_random_state\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "_imb_ok = True\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except Exception as e:\n",
    "    print(\"[warn] imbalanced-learn를 불러오지 못했습니다. 오버샘플링 비활성화:\", e)\n",
    "    _imb_ok = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = check_random_state(RANDOM_STATE)\n",
    "TOPK_BIGRAMS = 50\n",
    "K_LIST       = [3, 5, 7]\n",
    "TERMINALS_FINAL = {\"/logout\", \"/delete_user\"}\n",
    "\n",
    "OPER_POLICY = \"balanced\" \n",
    "T_OPER_DEFAULT = 0.730\n",
    "OPER_T_OVERRIDE = None\n",
    "\n",
    "# === (신규) Epoch/재학습 노브 ===\n",
    "XGB_EPOCHS = 1200              # XGBoost \"epochs\" == n_estimators\n",
    "EARLY_STOPPING_ROUNDS = 100    # XGBoost 조기종료 라운드\n",
    "LR_MAX_ITER = 8000             # LogisticRegression 반복 상한(유사-epoch)\n",
    "N_RESTARTS = 3                  # 멀티 시드 재학습 횟수\n",
    "SEED_SEQ = [42, 1337, 2027, 7777, 2025][:N_RESTARTS]\n",
    "\n",
    "# 오버샘플링 옵션 - 한 쪽의 데이터가 너무 적을 시 사용\n",
    "OVERSAMPLING = {\n",
    "    \"enable\": False,          # 전역 기본값 ( True = on / False = off )\n",
    "    \"method\": \"smote\",        # \"random\" | \"smote\"\n",
    "    \"ratio\": \"auto\",          # 1.0=완전균형, 0.5=소수/다수=0.5, \"auto\"=자동균형\n",
    "    \"k_neighbors\": 5,         # SMOTE용\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "# K별 오버샘플링 override (전역 기본값을 덮어씀)\n",
    "OVERSAMPLING_BY_K = {\n",
    "    3: {\"enable\": False, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=3 \n",
    "    5: {\"enable\": True, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=5\n",
    "    7: {\"enable\": False, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=7\n",
    "}\n",
    "\n",
    "def _merge_oversampling_cfg(global_cfg: dict, per_k_cfg: dict) -> dict:\n",
    "    cfg = dict(global_cfg)\n",
    "    if per_k_cfg:\n",
    "        cfg.update(per_k_cfg)\n",
    "    return cfg\n",
    "\n",
    "def _normalize_sampling_strategy(val):\n",
    "    \"\"\"imblearn이 받는 sampling_strategy 형태로 정규화.\"\"\"\n",
    "    if callable(val):\n",
    "        return val\n",
    "    if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "        return float(val)  # 예: 0.5\n",
    "    if isinstance(val, dict):\n",
    "        return {k: (int(v) if isinstance(v, (np.integer,)) else int(v) if isinstance(v, bool) else v)\n",
    "                for k, v in val.items()}\n",
    "    if val is None:\n",
    "        return \"auto\"\n",
    "    if isinstance(val, str):\n",
    "        v = val.strip().lower()\n",
    "        if v in {\"auto\", \"minority\", \"not minority\", \"not majority\", \"all\"}:\n",
    "            return v\n",
    "        try:\n",
    "            return float(v)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid sampling_strategy (ratio): {val!r}\")\n",
    "    raise TypeError(f\"Unsupported sampling_strategy type: {type(val)}\")\n",
    "\n",
    "def _build_sampler(cfg: dict):\n",
    "    \"\"\"오버샘플러 생성(파이프라인 내부에서만 사용).\"\"\"\n",
    "    if (not cfg.get(\"enable\")) or (not _imb_ok):\n",
    "        return None\n",
    "    method = str(cfg.get(\"method\", \"random\")).lower()\n",
    "    rs     = cfg.get(\"random_state\", RANDOM_STATE)\n",
    "    ss     = _normalize_sampling_strategy(cfg.get(\"ratio\", \"auto\"))\n",
    "    if method == \"random\":\n",
    "        return RandomOverSampler(sampling_strategy=ss, random_state=rs)\n",
    "    elif method == \"smote\":\n",
    "        return SMOTE(sampling_strategy=ss, random_state=rs, k_neighbors=int(cfg.get(\"k_neighbors\", 5)))\n",
    "    else:\n",
    "        print(f\"[warn] 알 수 없는 오버샘플링 방법: {method}. 비활성화합니다.\")\n",
    "        return None\n",
    "\n",
    "# 입력/출력 경로\n",
    "INPUT_PATH = \"datasets/processed_user_behavior.sorted.csv\"\n",
    "OUT_DIR = \"datasets/sessionDrop\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_ALL = os.path.join(OUT_DIR, \"sessionDrop.features.csv\")\n",
    "def OUT_PREFIXF_PATH(k:int) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"sessionDrop.prefix{k}.features.filtered.csv\")\n",
    "\n",
    "MODEL_OUT_DIR = \"models/sessionDrop\"\n",
    "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
    "def MODEL_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.joblib\")\n",
    "def META_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.meta.json\")\n",
    "\n",
    "FEATURE_COLS = [\"timestamp\",\"current_state\",\"page_depth\",\n",
    "                \"delta_search_count\",\"delta_cart_item_count\",\"gap_sec\"]\n",
    "\n",
    "def make_param_dist(seed: int):\n",
    "    r = check_random_state(seed)\n",
    "    return {\n",
    "        \"n_estimators\":      r.randint(600, 1601, size=20),\n",
    "        \"learning_rate\":     r.choice([0.02, 0.03, 0.05, 0.07, 0.1], size=20),\n",
    "        \"max_depth\":         r.choice([3,4,5,6,7], size=20),\n",
    "        \"min_child_weight\":  r.choice([1,2,3,5], size=20),\n",
    "        \"subsample\":         r.uniform(0.6, 1.0, size=20),\n",
    "        \"colsample_bytree\":  r.uniform(0.6, 1.0, size=20),\n",
    "        \"reg_lambda\":        r.choice([0.5,1.0,1.5,2.0], size=20),\n",
    "        \"reg_alpha\":         r.choice([0.0,0.1,0.3,0.5], size=20),\n",
    "    }\n",
    "\n",
    "print(\"[Done]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db08ee-9994-4374-b02f-ee8b1cddf8c1",
   "metadata": {},
   "source": [
    "## 2) 원본 데이터 로드\n",
    "- 원본 데이터를 로드하고 전체 세션의 개수를 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347123a-97a8-48d9-88c4-339b1e365600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2. 데이터 로드 ===\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "required = [\"session_id\",\"timestamp\",\"current_state\",\"next_state\",\"page_depth\",\"search_count\",\"cart_item_count\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"필수 컬럼 누락: {missing}\")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"session_id\",\"timestamp\"]).copy()\n",
    "df[\"session_id\"] = df[\"session_id\"].astype(str)\n",
    "df = df.sort_values([\"session_id\",\"timestamp\"])\n",
    "\n",
    "def norm_state(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\?.*$\",\"\", s)\n",
    "    if len(s)>1 and s.endswith(\"/\"):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "df[\"current_state\"] = df[\"current_state\"].map(norm_state)\n",
    "df[\"next_state\"]    = df[\"next_state\"].map(norm_state)\n",
    "\n",
    "print(\"Loaded rows:\", len(df), \"sessions:\", df[\"session_id\"].nunique())\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349630be-6cdb-4676-8c95-de9a9ef399a5",
   "metadata": {},
   "source": [
    "## 3) 간격/증분 계산\n",
    "- 세션 내 이벤트 간 시간차와 누적 지표의 증가분을 만들어 이후 피처 집계에 쓰기 좋게 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0f05c-33bd-401a-93b5-4215700d7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3. 간격/증분 생성 ===\n",
    "df[\"gap_sec\"] = (\n",
    "    df.groupby(\"session_id\")[\"timestamp\"]\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    "      .clip(lower=0)\n",
    ")\n",
    "\n",
    "for col in [\"page_depth\",\"search_count\",\"cart_item_count\"]:\n",
    "    dcol = f\"delta_{col}\"\n",
    "    df[dcol] = df.groupby(\"session_id\")[col].diff()\n",
    "    df[dcol] = df[dcol].fillna(df[col]).clip(lower=0)\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8525c12-f745-40ff-aff1-c3981619e684",
   "metadata": {},
   "source": [
    "## 4) Targit Label 생성 & 누수 세션 식별\n",
    "- 실제 결과인 Label을 생성하고, 모델 학습을 위해 첫 활동부터 K 활동 내에 실제 결과 값이 포함된 세션을 식별합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09409371-2846-4c33-b9d0-55114f77dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4. 세션 라벨 + prefix-K 및 누수 세션 식별 ===\n",
    "is_final = df[\"current_state\"].isin(TERMINALS_FINAL) | df[\"next_state\"].isin(TERMINALS_FINAL)\n",
    "labels = (\n",
    "    df.assign(is_final=is_final)\n",
    "      .groupby(\"session_id\", sort=False)[\"is_final\"]\n",
    "      .any()\n",
    "      .astype(\"int8\")\n",
    "      .reset_index(name=\"label\")\n",
    ")\n",
    "\n",
    "prefix_by_k    = {}\n",
    "term_sid_by_k  = {}\n",
    "\n",
    "# 🔧 누수 처리 모드: 'drop' | 'truncate' | 'mask' | 'mask_pad'\n",
    "LEAK_MODE  = \"mask_pad\"\n",
    "PAD_TOKEN  = \"\"     # 패딩/마스킹에 쓸 중립 토큰(빈 문자열이면 대부분의 피처에 영향 최소)\n",
    "PAD_DEPTH  = None   # None=이전값 유지(ffill), 숫자 주면 고정값으로 세팅(예: 0)\n",
    "\n",
    "def _sanitize_prefix_group(g: pd.DataFrame, K: int) -> pd.DataFrame:\n",
    "    \"\"\"prefix-K 그룹에서 터미널 등장 이벤트를 누수 방지 처리.\"\"\"\n",
    "    mask = g[\"current_state\"].isin(TERMINALS_FINAL) | g[\"next_state\"].isin(TERMINALS_FINAL)\n",
    "    if not mask.any():\n",
    "        # 누수 없음\n",
    "        if LEAK_MODE.endswith(\"pad\") and len(g) < K:\n",
    "            # 패딩만 필요하면 패딩\n",
    "            last = g.iloc[-1]\n",
    "            while len(g) < K:\n",
    "                pad = {\n",
    "                    \"session_id\": last[\"session_id\"],\n",
    "                    \"timestamp\":  last[\"timestamp\"],      # 시간 증가 없음\n",
    "                    \"current_state\": PAD_TOKEN,\n",
    "                    \"next_state\":    PAD_TOKEN,\n",
    "                    \"page_depth\":    last[\"page_depth\"] if PAD_DEPTH is None else PAD_DEPTH,\n",
    "                    \"delta_search_count\": 0.0,\n",
    "                    \"delta_cart_item_count\": 0.0,\n",
    "                    \"gap_sec\": 0.0,\n",
    "                }\n",
    "                g = pd.concat([g, pd.DataFrame([pad])], ignore_index=True)\n",
    "        return g\n",
    "\n",
    "    if LEAK_MODE == \"drop\":\n",
    "        # 터미널이 있는 행 제거(세션이 비면 downstream에서 자연 탈락)\n",
    "        g2 = g.loc[~mask].copy()\n",
    "        return g2\n",
    "\n",
    "    if LEAK_MODE == \"truncate\":\n",
    "        # 첫 터미널 등장 이전까지만 사용\n",
    "        first_idx = mask.idxmax()  # True가 처음 나온 index\n",
    "        g2 = g.loc[g.index < first_idx].copy()\n",
    "        return g2\n",
    "\n",
    "    # === 'mask' / 'mask_pad' 공통: 터미널 등장 행을 '중립화' ===\n",
    "    g2 = g.copy()\n",
    "\n",
    "    # 상태 토큰 마스킹\n",
    "    g2.loc[mask, \"current_state\"] = PAD_TOKEN\n",
    "    g2.loc[mask, \"next_state\"]    = PAD_TOKEN\n",
    "\n",
    "    # 델타/간격은 0으로(동작 최소화)\n",
    "    g2.loc[mask, [\"delta_search_count\", \"delta_cart_item_count\", \"gap_sec\"]] = 0.0\n",
    "\n",
    "    # page_depth는 이전값 유지(ffill) 또는 고정값\n",
    "    if PAD_DEPTH is None:\n",
    "        g2[\"page_depth\"] = g2[\"page_depth\"].ffill()\n",
    "        g2.loc[mask & g2[\"page_depth\"].isna(), \"page_depth\"] = 0.0\n",
    "    else:\n",
    "        g2.loc[mask, \"page_depth\"] = float(PAD_DEPTH)\n",
    "\n",
    "    # 시간 신호가 새어 나가지 않도록, 마스킹된 행의 timestamp는 직전과 동일하게\n",
    "    g2.loc[mask, \"timestamp\"] = g2[\"timestamp\"].shift(1).loc[mask].fillna(g2[\"timestamp\"].iloc[0])\n",
    "\n",
    "    # 필요 시 K까지 패딩\n",
    "    if LEAK_MODE == \"mask_pad\" and len(g2) < K:\n",
    "        last = g2.iloc[-1]\n",
    "        while len(g2) < K:\n",
    "            pad = {\n",
    "                \"session_id\": last[\"session_id\"],\n",
    "                \"timestamp\":  last[\"timestamp\"],\n",
    "                \"current_state\": PAD_TOKEN,\n",
    "                \"next_state\":    PAD_TOKEN,\n",
    "                \"page_depth\":    last[\"page_depth\"] if PAD_DEPTH is None else PAD_DEPTH,\n",
    "                \"delta_search_count\": 0.0,\n",
    "                \"delta_cart_item_count\": 0.0,\n",
    "                \"gap_sec\": 0.0,\n",
    "            }\n",
    "            g2 = pd.concat([g2, pd.DataFrame([pad])], ignore_index=True)\n",
    "\n",
    "    return g2\n",
    "\n",
    "for K in K_LIST:\n",
    "    # 원본 prefix-K\n",
    "    prefix_k = (\n",
    "        df.sort_values([\"session_id\",\"timestamp\"])\n",
    "          .groupby(\"session_id\", group_keys=False)\n",
    "          .head(K)\n",
    "    )\n",
    "\n",
    "    # 참고용(통계 출력 용도로만 유지)\n",
    "    term_sid_k = prefix_k.loc[\n",
    "        prefix_k[\"current_state\"].isin(TERMINALS_FINAL) | prefix_k[\"next_state\"].isin(TERMINALS_FINAL),\n",
    "        \"session_id\"\n",
    "    ].unique()\n",
    "\n",
    "    # ✅ 누수 처리 적용된 prefix-K\n",
    "    prefix_clean_k = pd.concat(\n",
    "        (_sanitize_prefix_group(g.copy(), K) for _, g in prefix_k.groupby(\"session_id\", sort=False)),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    prefix_by_k[K]   = prefix_clean_k\n",
    "    term_sid_by_k[K] = term_sid_k  # 통계 출력용 (계속 사용해도 됨)\n",
    "\n",
    "print(f\"[info] total sessions: {df['session_id'].nunique():,}\")\n",
    "for K in K_LIST:\n",
    "    print(f\"[info] K={K}: sessions reaching terminals in first {K} events (detected): {len(term_sid_by_k[K]):,}\")\n",
    "\n",
    "def bigrams(seq):\n",
    "    a, b = tee(seq)\n",
    "    next(b, None)\n",
    "    for x, y in zip(a, b):\n",
    "        yield (x, y)\n",
    "\n",
    "bg_counter = Counter()\n",
    "TERMINALS = TERMINALS_FINAL\n",
    "for _, g in df.groupby(\"session_id\", sort=False):\n",
    "    seq = list(g[\"current_state\"].fillna(\"\").values)\n",
    "    for bg in bigrams(seq):\n",
    "        if (bg[0] in TERMINALS) or (bg[1] in TERMINALS):\n",
    "            continue\n",
    "        bg_counter[bg] += 1\n",
    "\n",
    "top_bigrams = [bg for bg, _ in bg_counter.most_common(TOPK_BIGRAMS)]\n",
    "bigram_to_idx = {bg:i for i,bg in enumerate(top_bigrams)}\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c3a31-b3b0-4784-9355-d7ea120d81ff",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "- 모델을 학습하기 위해 데이터를 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14c11e-f07e-4fc7-b9ce-c673240f595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5. 세션 피처 집계 함수 ===\n",
    "from IPython.display import display\n",
    "\n",
    "def agg_features(g: pd.DataFrame) -> pd.Series:\n",
    "    g = g.sort_values(\"timestamp\")\n",
    "    states = g[\"current_state\"].fillna(\"\").tolist()\n",
    "    gaps   = g[\"gap_sec\"].values\n",
    "\n",
    "    feats = {}\n",
    "    feats[\"n_events\"] = len(g)\n",
    "    feats[\"session_duration_sec\"] = (\n",
    "        (g[\"timestamp\"].iloc[-1] - g[\"timestamp\"].iloc[0]).total_seconds()\n",
    "        if len(g)>1 else 0.0\n",
    "    )\n",
    "    feats[\"events_per_min\"] = feats[\"n_events\"] / max(feats[\"session_duration_sec\"]/60.0, 1e-9)\n",
    "\n",
    "    if len(gaps)>0:\n",
    "        feats[\"gap_mean\"] = float(np.mean(gaps))\n",
    "        feats[\"gap_std\"]  = float(np.std(gaps))\n",
    "        feats[\"gap_p95\"]  = float(np.quantile(gaps, 0.95))\n",
    "    else:\n",
    "        feats[\"gap_mean\"]=feats[\"gap_std\"]=feats[\"gap_p95\"]=0.0\n",
    "\n",
    "    feats[\"page_depth_max\"]   = float(g[\"page_depth\"].max())\n",
    "    feats[\"page_depth_slope\"] = float((g[\"page_depth\"].iloc[-1] - g[\"page_depth\"].iloc[0]) / max(len(g)-1,1))\n",
    "    feats[\"depth_backtracks\"] = int((g[\"page_depth\"].diff()<0).sum())\n",
    "\n",
    "    feats[\"search_delta_sum\"] = float(g[\"delta_search_count\"].sum())\n",
    "    feats[\"search_rate\"]      = feats[\"search_delta_sum\"] / max(feats[\"n_events\"],1)\n",
    "\n",
    "    cart_delta = g[\"delta_cart_item_count\"].clip(lower=0)\n",
    "    feats[\"cart_adds\"]        = float(cart_delta.sum())\n",
    "    feats[\"cart_touch_count\"] = int((cart_delta>0).sum())\n",
    "    feats[\"cart_adds_per_event\"] = feats[\"cart_adds\"] / max(feats[\"n_events\"],1)\n",
    "    feats[\"search_to_cart_ratio\"] = (feats[\"search_delta_sum\"] + 1.0) / (feats[\"cart_adds\"] + 1.0)\n",
    "\n",
    "    first_ts = g[\"timestamp\"].iloc[0]\n",
    "    feats[\"first_event_hour\"] = int(first_ts.hour)\n",
    "    feats[\"is_weekend\"]       = int(first_ts.weekday()>=5)\n",
    "\n",
    "    def idx_of(pred_list, key):\n",
    "        for i, s in enumerate(pred_list):\n",
    "            if key in s: return i\n",
    "        return None\n",
    "    i_search = idx_of(states, \"/search\")\n",
    "    i_cart   = idx_of(states, \"/cart\")\n",
    "    feats[\"time_to_first_search\"] = float((g[\"timestamp\"].iloc[i_search] - first_ts).total_seconds()) if i_search is not None else -1.0\n",
    "    feats[\"time_to_first_cart\"]   = float((g[\"timestamp\"].iloc[i_cart]   - first_ts).total_seconds()) if i_cart   is not None else -1.0\n",
    "\n",
    "    lastk = states[-5:]\n",
    "    joined_lastk = \" \".join(lastk)\n",
    "    feats[\"last5_has_search\"]   = int(\"search\" in joined_lastk)\n",
    "    feats[\"last5_has_cart\"]     = int(\"cart\" in joined_lastk)\n",
    "    feats[\"last5_has_checkout\"] = int(\"checkout\" in joined_lastk)\n",
    "\n",
    "    bg_counts = Counter(list(bigrams(states)))\n",
    "    for bg, idx in bigram_to_idx.items():\n",
    "        feats[f\"bg_{idx}\"] = int(bg_counts.get(bg, 0))\n",
    "\n",
    "    uniq = pd.Series(states).value_counts(normalize=True)\n",
    "    feats[\"unique_states\"] = int(uniq.size)\n",
    "    feats[\"entropy_state\"] = float(-(uniq*np.log(uniq+1e-12)).sum())\n",
    "\n",
    "    st0 = states[0] if states else \"\"\n",
    "    stL = states[-1] if states else \"\"\n",
    "    for name, st in [(\"start\", st0), (\"last\", stL)]:\n",
    "        feats[f\"{name}_is_root\"]      = int(st == \"/\")\n",
    "        feats[f\"{name}_is_search\"]    = int(\"/search\" in st)\n",
    "        feats[f\"{name}_is_products\"]  = int(\"/products\" in st)\n",
    "        feats[f\"{name}_is_cart\"]      = int(\"/cart\" in st)\n",
    "        feats[f\"{name}_is_checkout\"]  = int(\"/checkout\" in st)\n",
    "\n",
    "    all_states = \" \".join(states)\n",
    "    feats[\"hit_search\"]   = int(\"/search\" in all_states)\n",
    "    feats[\"hit_products\"] = int(\"/products\" in all_states)\n",
    "    feats[\"hit_cart\"]     = int(\"/cart\" in all_states)\n",
    "    feats[\"hit_checkout\"] = int(\"/checkout\" in all_states)\n",
    "\n",
    "    return pd.Series(feats)\n",
    "\n",
    "print(\"[Done]\")\n",
    "\n",
    "# --- (교체) 전처리 & Feature Engineering 미리보기 ---\n",
    "from IPython.display import display\n",
    "\n",
    "def preview_engineered(\n",
    "    n_sessions=5, K=None, drop_terminals=True, random=False, seed=42,\n",
    "    include_bigrams=False, topn_bg=None\n",
    "):\n",
    "    \"\"\"\n",
    "    K: None이면 전체 세션 기준(ALL), 정수면 prefix-K 기준\n",
    "    drop_terminals: True면 누수 방지용 terminal 포함 세션 제거\n",
    "    random: True면 무작위 샘플, False면 앞에서부터\n",
    "    include_bigrams: True면 bg_* 컬럼을 포함, False면 숨김\n",
    "    topn_bg: 정수로 주면 bg_* 컬럼 중 앞에서 n개만 노출(예: topn_bg=10)\n",
    "    \"\"\"\n",
    "    if K is None:\n",
    "        df_src = df\n",
    "        term_sids = set()\n",
    "    else:\n",
    "        df_src = prefix_by_k[K]\n",
    "        term_sids = set(term_sid_by_k[K]) if drop_terminals else set()\n",
    "\n",
    "    sids_series = df_src[\"session_id\"].drop_duplicates()\n",
    "    if random:\n",
    "        sids = sids_series.sample(n=min(n_sessions, len(sids_series)), random_state=seed).tolist()\n",
    "    else:\n",
    "        sids = sids_series.head(n_sessions).tolist()\n",
    "\n",
    "    # terminal 포함 세션 제거 옵션\n",
    "    sids = [sid for sid in sids if sid not in term_sids]\n",
    "\n",
    "    preview = (\n",
    "        df_src[df_src[\"session_id\"].isin(sids)]\n",
    "          .groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "          .apply(agg_features)\n",
    "          .reset_index()\n",
    "          .merge(labels, on=\"session_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # bigram 컬럼 처리\n",
    "    bg_cols = [c for c in preview.columns if c.startswith(\"bg_\")]\n",
    "    if not include_bigrams:\n",
    "        # 전부 숨김\n",
    "        preview = preview.drop(columns=bg_cols, errors=\"ignore\")\n",
    "    elif isinstance(topn_bg, int) and topn_bg >= 0:\n",
    "        def _bg_idx(col): \n",
    "            try: return int(col.split(\"_\", 1)[1])\n",
    "            except: return 1_000_000\n",
    "        keep_bg = [c for c in sorted(bg_cols, key=_bg_idx)[:topn_bg]]\n",
    "        drop_bg = [c for c in bg_cols if c not in keep_bg]\n",
    "        preview = preview.drop(columns=drop_bg, errors=\"ignore\")\n",
    "\n",
    "    # 보기 좋게 컬럼 정렬\n",
    "    key_cols = [\"session_id\", \"label\", \"n_events\", \"session_duration_sec\", \"events_per_min\",\n",
    "                \"gap_mean\", \"gap_std\", \"gap_p95\",\n",
    "                \"page_depth_max\", \"page_depth_slope\", \"depth_backtracks\",\n",
    "                \"search_delta_sum\", \"search_rate\",\n",
    "                \"cart_adds\", \"cart_touch_count\", \"cart_adds_per_event\", \"search_to_cart_ratio\",\n",
    "                \"first_event_hour\", \"is_weekend\",\n",
    "                \"time_to_first_search\", \"time_to_first_cart\",\n",
    "                \"last5_has_search\", \"last5_has_cart\", \"last5_has_checkout\",\n",
    "                \"unique_states\", \"entropy_state\",\n",
    "                \"start_is_root\", \"start_is_search\", \"start_is_products\", \"start_is_cart\", \"start_is_checkout\",\n",
    "                \"last_is_root\", \"last_is_search\", \"last_is_products\", \"last_is_cart\", \"last_is_checkout\",\n",
    "                \"hit_search\", \"hit_products\", \"hit_cart\", \"hit_checkout\"]\n",
    "    ordered = [c for c in key_cols if c in preview.columns] + [c for c in preview.columns if c not in key_cols]\n",
    "    preview = preview[ordered]\n",
    "\n",
    "    title = \"ALL\" if K is None else f\"prefix-K={K}\"\n",
    "    bg_count_total = sum(1 for c in preview.columns if c.startswith(\"bg_\"))\n",
    "    print(f\"[Preview] Engineered features ({title}) — {len(preview)} rows\")\n",
    "    display(preview)\n",
    "    print(f\"  - Total columns shown: {len(preview.columns)} (bigrams shown: {bg_count_total})\")\n",
    "    return preview\n",
    "\n",
    "\n",
    "_ = preview_engineered(n_sessions=5, K=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6c2bc-e0cf-4452-bb8a-13d310682ca1",
   "metadata": {},
   "source": [
    "## 6) 전처리 데이터 저장\n",
    "- Feature Engineering 한 데이터를 K별로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79ba5e-6fc4-4cba-83a6-dac260622fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6. 세션 피처 테이블 생성 & 저장 ===\n",
    "X_all = (\n",
    "    df.groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "      .apply(agg_features)\n",
    "      .reset_index()\n",
    ")\n",
    "dataset_all = X_all.merge(labels, on=\"session_id\", how=\"left\")\n",
    "dataset_all.to_csv(OUT_ALL, index=False)\n",
    "\n",
    "datasets_by_k = {}\n",
    "print(\"Saved CSVs:\")\n",
    "print(\" -\", OUT_ALL)\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_prefix_k = (\n",
    "        prefix_by_k[K].groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "                      .apply(agg_features)\n",
    "                      .reset_index()\n",
    "    )\n",
    "    dataset_prefix_k = X_prefix_k.merge(labels, on=\"session_id\", how=\"left\")\n",
    "    dataset_prefix_filt_k = dataset_prefix_k.reset_index(drop=True)\n",
    "    datasets_by_k[K] = dataset_prefix_filt_k\n",
    "    out_path_k = OUT_PREFIXF_PATH(K)\n",
    "    dataset_prefix_filt_k.to_csv(out_path_k, index=False)\n",
    "    print(\" -\", out_path_k, \"| shape:\", tuple(dataset_prefix_filt_k.shape))\n",
    "    \n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0656042-6faa-4caf-a7fd-29fbf36cd4ff",
   "metadata": {},
   "source": [
    "## 7) 학습용 / 검증용 데이터셋 분할\n",
    "- 데이터를 학습용 / 검증용 데이터셋으로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c929dc3-d7bf-49b6-a4d4-b1dc18ca4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7. 학습/검증 분할 (K별) ===\n",
    "splits_by_k = {}\n",
    "for K in K_LIST:\n",
    "    dfp = datasets_by_k[K].copy()\n",
    "    y = dfp[\"label\"].astype(int)\n",
    "    leak_cols = [c for c in dfp.columns if (\"logout\" in c.lower()) or (\"delete\" in c.lower())]\n",
    "    X = dfp.drop(columns=[\"session_id\",\"label\"] + leak_cols, errors=\"ignore\").fillna(0.0)\n",
    "\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE)\n",
    "    splits_by_k[K] = (X_tr, X_va, y_tr, y_va, X.columns.tolist())\n",
    "\n",
    "    print(f\"[K={K}] Shape: {X.shape}  PosRate: {float(y.mean()):.4f}\")\n",
    "    print(f\"[K={K}] Train: {X_tr.shape}  Valid: {X_va.shape}\")\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acc53f-496b-42cd-8122-84eb06647347",
   "metadata": {},
   "source": [
    "## 8) 모델 선택 & 학습\n",
    "- 모델을 선택한 후 학습용 데이터 셋으로 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89db6-9054-4cd1-9e97-813cc5dc8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8. 모델 선택 & 학습  ===\n",
    "use_xgb = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    use_xgb = True\n",
    "except Exception as e:\n",
    "    print(\"[warn] xgboost 불가, LogisticRegression으로 폴백:\", e)\n",
    "\n",
    "# 학습 로그 옵션\n",
    "SHOW_XGB_LOG = False   # True면 부스팅 라운드별 metric을 출력(많이 나옵니다!)\n",
    "USE_TQDM     = True    # True면 tqdm 프로그레스 바 사용\n",
    "\n",
    "# tqdm 안전 임포트\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "# ---------- 고정 모델 빌더 ----------\n",
    "def _fixed_xgb(sampler_enabled: bool, y_tr, seed=RANDOM_STATE):\n",
    "    \"\"\"고정 XGB 분류기(시드/epoch 반영).\"\"\"\n",
    "    pos = int(y_tr.sum()); neg = int(len(y_tr) - pos)\n",
    "    spw = 1.0 if sampler_enabled else (neg / max(pos, 1))\n",
    "    return XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        # eval_metric은 fit kwargs가 아닌 set_params에서 커스텀으로 덮습니다(조기종료 지표)\n",
    "        n_estimators=XGB_EPOCHS,          # ← epoch처럼 제어\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.80,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        scale_pos_weight=spw,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "def _fixed_logreg(sampler_enabled: bool, seed=RANDOM_STATE):\n",
    "    \"\"\"고정 로지스틱(유사-epoch: max_iter 반영).\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    return LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=LR_MAX_ITER,             # ← 반복 상한\n",
    "        class_weight=None if sampler_enabled else \"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "# ---------- 커스텀 평가함수(feval) & XGB 학습 헬퍼 ----------\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "_FEVAL_NUM_THRESHOLDS = 61  # 속도/정확도 타협: 31~101 권장\n",
    "\n",
    "def _feval_macro_f1_balanced(y_true, y_score, sample_weight=None):\n",
    "    \"\"\"sklearn 래퍼 호환: (y_true, y_score[, sample_weight]) -> float\"\"\"\n",
    "    ths = np.linspace(0.01, 0.99, _FEVAL_NUM_THRESHOLDS, dtype=float)\n",
    "    best = 0.0\n",
    "    for t in ths:\n",
    "        y_hat = (y_score >= t).astype(np.int32)\n",
    "        try:\n",
    "            f1 = f1_score(y_true, y_hat, average=\"macro\", sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            f1 = 0.0\n",
    "        if f1 > best:\n",
    "            best = f1\n",
    "    return float(best)\n",
    "\n",
    "def safe_fit_xgb(model, Xtr, ytr, Xva, yva):\n",
    "    \"\"\"Pipeline 또는 단일 모델 모두 지원. XGBClassifier에 eval_set/feval/early_stopping 전달.\n",
    "       - eval_metric: 모델 파라미터(set_params)로 주입\n",
    "       - early_stopping_rounds: fit kwargs\n",
    "       - verbose: SHOW_XGB_LOG에 따라 on/off\n",
    "    \"\"\"\n",
    "    is_pipe = hasattr(model, \"named_steps\") and (\"clf\" in getattr(model, \"named_steps\", {}))\n",
    "    clf = model.named_steps[\"clf\"] if is_pipe else model\n",
    "    prefix = \"clf__\" if is_pipe else \"\"\n",
    "\n",
    "    # (중요) 커스텀 feval은 fit kwargs가 아니라 '모델 파라미터'로 설정\n",
    "    if is_pipe:\n",
    "        model.set_params(**{f\"{prefix}eval_metric\": _feval_macro_f1_balanced})\n",
    "    else:\n",
    "        clf.set_params(eval_metric=_feval_macro_f1_balanced)\n",
    "\n",
    "    # fit kwargs 구성\n",
    "    fit_kwargs = {}\n",
    "    sig = inspect.signature(clf.fit)\n",
    "\n",
    "    # 검증 세트\n",
    "    if \"eval_set\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}eval_set\"] = [(Xva, yva)]\n",
    "\n",
    "    # 조기종료 (버전 호환: callbacks 미사용)\n",
    "    if \"early_stopping_rounds\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}early_stopping_rounds\"] = EARLY_STOPPING_ROUNDS\n",
    "\n",
    "    # 라운드 로그 출력\n",
    "    if \"verbose\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}verbose\"] = bool(SHOW_XGB_LOG)\n",
    "\n",
    "    # 학습\n",
    "    t0 = time.time()\n",
    "    model.fit(Xtr, ytr, **fit_kwargs)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    # 베스트 이터레이션/점수 로깅(가능한 경우)\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    best_iter  = getattr(est, \"best_iteration\", None)\n",
    "    best_score = getattr(est, \"best_score\", None)\n",
    "    if best_iter is None:\n",
    "        best_iter = getattr(est, \"best_ntree_limit\", None)\n",
    "\n",
    "    print(f\"    ↳ fit done in {dt:.2f}s | best_iter={best_iter} | best_score={best_score}\")\n",
    "    return model\n",
    "\n",
    "# ---------- 실제 학습: 후보 모델들을 훈련만 수행 ----------\n",
    "best_by_k = {}                 # 파라미터 요약(메타 저장용)\n",
    "use_pipeline = _imb_ok         # sampler 사용 가능하면 파이프라인\n",
    "trained_candidates_by_k = {}   # ← K별 학습된 모델 리스트(후보군). 선택/평가는 Cell 9에서.\n",
    "\n",
    "SEP = \"-\" * 86  # ← K 구분선\n",
    "\n",
    "_k_iter = tqdm(K_LIST, desc=\"K sweep (train)\") if USE_TQDM else K_LIST\n",
    "\n",
    "for K in _k_iter:\n",
    "    print(f\"\\n{SEP}\\n[TRAIN START] K={K}\\n{SEP}\")\n",
    "\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "\n",
    "    cfg_k = _merge_oversampling_cfg(OVERSAMPLING, OVERSAMPLING_BY_K.get(K, {}))\n",
    "    sampler = _build_sampler(cfg_k)\n",
    "    sampler_enabled = sampler is not None\n",
    "\n",
    "    # 파라미터 요약(대표 시드로 1회 생성)\n",
    "    base_for_summary = _fixed_xgb(sampler_enabled, y_tr, seed=RANDOM_STATE) if use_xgb else _fixed_logreg(sampler_enabled, seed=RANDOM_STATE)\n",
    "    clf_params = base_for_summary.get_params(deep=False)\n",
    "    summary_keys = (\n",
    "        [\"n_estimators\", \"learning_rate\", \"max_depth\", \"min_child_weight\",\n",
    "         \"subsample\", \"colsample_bytree\", \"reg_lambda\", \"reg_alpha\", \"random_state\"]\n",
    "        if use_xgb else\n",
    "        [\"penalty\", \"C\", \"class_weight\", \"solver\", \"max_iter\", \"random_state\"]\n",
    "    )\n",
    "    used_params = {k: clf_params.get(k) for k in summary_keys if k in clf_params}\n",
    "    best_by_k[K] = (None, used_params)\n",
    "    print(f\"[K={K}] Using {'XGBClassifier' if use_xgb else 'LogisticRegression'} Params: {used_params}\")\n",
    "\n",
    "    # 후보 모델 학습\n",
    "    models = []\n",
    "    _seed_iter = tqdm(SEED_SEQ, desc=f\"K={K} train seeds\", leave=False) if USE_TQDM else SEED_SEQ\n",
    "    for seed in _seed_iter:\n",
    "        if use_xgb:\n",
    "            base_clf = _fixed_xgb(sampler_enabled, y_tr, seed=seed)\n",
    "            model_name = \"XGBClassifier\"\n",
    "        else:\n",
    "            base_clf = _fixed_logreg(sampler_enabled, seed=seed)\n",
    "            model_name = \"LogisticRegression\"\n",
    "\n",
    "        if use_pipeline:\n",
    "            steps = []\n",
    "            if sampler_enabled:\n",
    "                steps.append((\"sampler\", sampler))\n",
    "            steps.append((\"clf\", base_clf))\n",
    "            estimator = ImbPipeline(steps)\n",
    "        else:\n",
    "            estimator = base_clf\n",
    "\n",
    "        print(f\"[K={K}] [Seed={seed}] model={model_name} | sampler={'ON' if sampler_enabled else 'OFF'} \"\n",
    "              f\"| n_est={getattr(base_clf, 'n_estimators', 'NA')} | max_iter={getattr(base_clf, 'max_iter', 'NA')}\")\n",
    "\n",
    "        if use_xgb:\n",
    "            estimator = safe_fit_xgb(estimator, X_tr, y_tr, X_va, y_va)\n",
    "        else:\n",
    "            t0 = time.time()\n",
    "            estimator.fit(X_tr, y_tr)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"    ↳ fit done in {dt:.2f}s\")\n",
    "    \n",
    "        models.append(estimator)\n",
    "    \n",
    "        trained_candidates_by_k[K] = models\n",
    "        print(f\"[TRAIN END]   K={K} | candidates={len(models)}\")\n",
    "        print(SEP)\n",
    "\n",
    "print(\"[Done] 트레이닝 완료 — 후보 모델들은 trained_candidates_by_k[K]에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c1596-71dd-4a52-b8df-66df0442493a",
   "metadata": {},
   "source": [
    "## 9) 모델 검증 & 평가\n",
    "- 선택한 모델을 검증용 데이터셋으로 검증하고 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb82746-a643-4eff-9806-653b1ec8b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9. 검증 & 평가  ===\n",
    "\n",
    "# 검증/평가 유틸\n",
    "def balanced_score(y_true, p, t):\n",
    "    y_pred = (p >= t).astype(int)\n",
    "    return 0.5*(accuracy_score(y_true, y_pred) + f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "SHOW_05 = False\n",
    "\n",
    "trained_models_by_k = {}   # ← 최종 선정된 단일 모델(K별)\n",
    "eval_by_k = {}             # ← 평가 요약(메타/리포팅용)\n",
    "T_OPER_BY_K = {}           # ← 운영 임계값(K별)\n",
    "summary_rows = []\n",
    "SEP = \"-\" * 86\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_tr, X_va, y_tr, y_va, feat_names = splits_by_k[K]\n",
    "    models = trained_candidates_by_k[K]  # Cell 8에서 훈련된 후보군\n",
    "\n",
    "    # 각 후보 모델의 검증 확률\n",
    "    p_list = [m.predict_proba(X_va)[:, 1] for m in models]\n",
    "\n",
    "    # ── 임계값 탐색은 (기존 로직 유지) 앙상블 평균 확률 기준으로 수행 ──\n",
    "    p_va_mean = np.mean(np.vstack(p_list), axis=0)\n",
    "    ths = np.linspace(0.01, 0.99, 197)\n",
    "    t_best_ensemble, s_best_ensemble = max(\n",
    "        ((t, balanced_score(y_va, p_va_mean, t)) for t in ths),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "\n",
    "    # 운영 임계값 결정(override > balanced > default)\n",
    "    if isinstance(OPER_T_OVERRIDE, dict) and (K in OPER_T_OVERRIDE):\n",
    "        op_t = float(OPER_T_OVERRIDE[K])\n",
    "    elif OPER_POLICY == \"balanced\":\n",
    "        op_t = float(t_best_ensemble)\n",
    "    else:\n",
    "        op_t = float(T_OPER_DEFAULT)\n",
    "    T_OPER_BY_K[K] = op_t\n",
    "\n",
    "    # ── 최종 단일 모델 선정: 동일 op_t에서 Macro-F1이 가장 높은 모델 ──\n",
    "    single_scores_f1 = [\n",
    "        f1_score(y_va, (p_list[i] >= op_t).astype(int), average=\"macro\")\n",
    "        for i in range(len(models))\n",
    "    ]\n",
    "    best_idx = int(np.argmax(single_scores_f1))\n",
    "    final_model = models[best_idx]\n",
    "    p_va_single = p_list[best_idx]\n",
    "\n",
    "    # ── 출력/요약은 \"최종 단일 모델\" 기준 ──\n",
    "    yhat_oper_single = (p_va_single >= op_t).astype(int)\n",
    "    acc_oper_single  = accuracy_score(y_va, yhat_oper_single)\n",
    "    f1_oper_single   = f1_score(y_va, yhat_oper_single, average=\"macro\")\n",
    "\n",
    "    if SHOW_05:\n",
    "        yhat05 = (p_va_single >= 0.5).astype(int)\n",
    "        print(f\"[K={K}] [Valid(single) @0.50] Acc={accuracy_score(y_va, yhat05):.4f} \"\n",
    "              f\"Macro-F1={f1_score(y_va, yhat05, average='macro'):.4f} \"\n",
    "              f\"PR-AUC={average_precision_score(y_va, p_va_single):.4f}\")\n",
    "        print(confusion_matrix(y_va, yhat05))\n",
    "        print(classification_report(y_va, yhat05, digits=4))\n",
    "\n",
    "    print(f\"[K={K}] [Valid(single) @t={op_t:.3f}] Acc={acc_oper_single:.4f} Macro-F1={f1_oper_single:.4f}\")\n",
    "    print(confusion_matrix(y_va, yhat_oper_single))\n",
    "    print(classification_report(y_va, yhat_oper_single, digits=4))\n",
    "    print(SEP, \"\\n\")\n",
    "\n",
    "    # 저장/메타용 결과 기록 (단일 모델 기준 성능을 기록)\n",
    "    trained_models_by_k[K] = final_model\n",
    "    eval_by_k[K] = {\n",
    "        # 출력과 동일하게 단일 모델 기준 성능을 저장\n",
    "        \"Valid@Oper.Acc\": float(acc_oper_single),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper_single),\n",
    "\n",
    "        # 참고용: 임계값 선택은 앙상블 기반으로 했으므로 별도 보존\n",
    "        \"Balanced.t\": float(t_best_ensemble),\n",
    "        \"Balanced.Score\": float(s_best_ensemble),\n",
    "\n",
    "        \"Oper.t\": float(op_t),\n",
    "        \"features\": feat_names,\n",
    "\n",
    "        # (신규) 재학습 정보\n",
    "        \"n_restarts\": int(N_RESTARTS),\n",
    "        \"seeds\": list(SEED_SEQ),\n",
    "        \"single_best_f1_at_oper\": float(single_scores_f1[best_idx]),\n",
    "        \"selected_model_index\": int(best_idx),\n",
    "    }\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"K\": K,\n",
    "        \"Oper.t\": float(op_t),\n",
    "        # 요약도 단일 모델 기준 값으로 출력\n",
    "        \"Valid@Oper.Acc\": float(acc_oper_single),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper_single),\n",
    "\n",
    "        # 참고용(앙상블 기반 탐색 결과)\n",
    "        \"Balanced.t\": float(t_best_ensemble),\n",
    "        \"Balanced.Score\": float(s_best_ensemble),\n",
    "        \"Restarts\": int(N_RESTARTS),\n",
    "    })\n",
    "\n",
    "print(\"=== Summary (K별, Single Final Model on Valid) ===\")\n",
    "for r in sorted(summary_rows, key=lambda x: x[\"K\"]):\n",
    "    print(f\"K={r['K']} | Oper.t={r['Oper.t']:.3f} | Acc@Oper={r['Valid@Oper.Acc']:.4f} \"\n",
    "          f\"| MacroF1@Oper={r['Valid@Oper.MacroF1']:.4f} | t_bal(ens)={r['Balanced.t']:.3f} \"\n",
    "          f\"| Score_bal(ens)={r['Balanced.Score']:.4f} | Restarts={r['Restarts']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906b71-b29f-409f-95a8-2c6ed4ea0664",
   "metadata": {},
   "source": [
    "## 10) 모델 저장\n",
    "- K별로 모델을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d498f73-a6fd-4760-a2a1-4eb322d1da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10. 모델 & 메타 저장 (K별) ===\n",
    "\n",
    "def to_builtin(v):\n",
    "    if isinstance(v, (np.integer,)):  return int(v)\n",
    "    if isinstance(v, (np.floating,)): return float(v)\n",
    "    if isinstance(v, (np.bool_,)):    return bool(v)\n",
    "    return v\n",
    "\n",
    "def _jsonable_sampling_strategy(val):\n",
    "    \"\"\"meta JSON에 기록할 sampling_strategy 직렬화 헬퍼.\"\"\"\n",
    "    if callable(val):\n",
    "        return f\"<callable:{getattr(val, '__name__', 'anonymous')}>\"\n",
    "    if isinstance(val, (np.integer, int, np.floating, float)):\n",
    "        return float(val)\n",
    "    if isinstance(val, (np.bool_, bool)):\n",
    "        return bool(val)\n",
    "    if isinstance(val, dict):\n",
    "        return {str(k): to_builtin(v) for k, v in val.items()}\n",
    "    return None if val is None else str(val)  # \"auto\"/\"minority\"/\"not majority\"/\"all\" 등\n",
    "\n",
    "def _extract_selected_model_info(model):\n",
    "    \"\"\"파이프라인/단일 모델 공통으로 최종 선정 모델의 핵심 정보를 추출.\"\"\"\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    info = {\n",
    "        \"estimator\": est.__class__.__name__,\n",
    "        \"random_state\": to_builtin(getattr(est, \"random_state\", None)),\n",
    "        \"n_estimators\": to_builtin(getattr(est, \"n_estimators\", None)),\n",
    "        \"max_iter\": to_builtin(getattr(est, \"max_iter\", None)),\n",
    "        \"best_iteration\": to_builtin(getattr(est, \"best_iteration\", getattr(est, \"best_ntree_limit\", None))),\n",
    "        \"best_score\": to_builtin(getattr(est, \"best_score\", None)),\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def _threshold_policy_for_k(K):\n",
    "    \"\"\"Cell 9의 정책을 메타로 명시.\"\"\"\n",
    "    if isinstance(OPER_T_OVERRIDE, dict) and (K in OPER_T_OVERRIDE):\n",
    "        return {\"policy\": \"override\", \"value\": float(OPER_T_OVERRIDE[K])}\n",
    "    elif OPER_POLICY == \"balanced\":\n",
    "        return {\"policy\": \"balanced\", \"value\": float(eval_by_k[K][\"Balanced.t\"])}\n",
    "    else:\n",
    "        return {\"policy\": \"default\", \"value\": float(T_OPER_DEFAULT)}\n",
    "\n",
    "def _prepare_for_dump(model):\n",
    "    \"\"\"XGB의 callable eval_metric이 있으면 문자열로 교체해 피클링 가능하게 만든다.\"\"\"\n",
    "    try:\n",
    "        if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps:\n",
    "            est = model.named_steps[\"clf\"]\n",
    "            if hasattr(est, \"eval_metric\") and callable(getattr(est, \"eval_metric\", None)):\n",
    "                # binary:logistic 기준으로 안전한 문자열 메트릭으로 교체\n",
    "                model.set_params(**{\"clf__eval_metric\": \"logloss\"})\n",
    "        else:\n",
    "            if hasattr(model, \"eval_metric\") and callable(getattr(model, \"eval_metric\", None)):\n",
    "                model.set_params(eval_metric=\"logloss\")\n",
    "    except Exception:\n",
    "        # 실패해도 저장을 시도하게끔 조용히 통과\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "print(\"Saving models & meta...\")\n",
    "for K in K_LIST:\n",
    "    # 최종 선정된 단일 모델과 평가 결과\n",
    "    model = trained_models_by_k[K]\n",
    "    feat_names = eval_by_k[K][\"features\"]\n",
    "    oper_t = float(eval_by_k[K][\"Oper.t\"])\n",
    "\n",
    "    # (요약용) 파라미터 스냅샷 — Cell 8에서 만든 used_params 재활용\n",
    "    best_params_ = best_by_k.get(K, ({}, {}))[1] if isinstance(best_by_k.get(K), tuple) else {}\n",
    "    best_params_clean = {k: to_builtin(v) for k, v in (best_params_ or {}).items()}\n",
    "\n",
    "    # 실제 사용된(전역 + K별 override) 오버샘플링 설정을 기록\n",
    "    eff_os_cfg = _merge_oversampling_cfg(OVERSAMPLING, OVERSAMPLING_BY_K.get(K, {}))\n",
    "    oversampling_enabled = bool(eff_os_cfg.get(\"enable\", False) and _imb_ok)\n",
    "\n",
    "    # (핵심) 덤프 전에 eval_metric의 callable 제거\n",
    "    model_for_dump = _prepare_for_dump(model)\n",
    "\n",
    "    # 모델 저장\n",
    "    model_path = MODEL_PATH_K(K)\n",
    "    dump(model_for_dump, model_path)\n",
    "\n",
    "    # 임계값 정책/선정 모델 정보\n",
    "    th_policy = _threshold_policy_for_k(K)\n",
    "    selected_info = _extract_selected_model_info(model)\n",
    "\n",
    "    # 메타 JSON 구성\n",
    "    meta = {\n",
    "        \"variant\": (\n",
    "            f\"Multi-seed (prefix{K}); leakage-safe; MacroF1-earlystop(feval during train); \"\n",
    "            f\"threshold=balanced_on_ensemble(mean)\"\n",
    "        ),\n",
    "        \"features\": [str(c) for c in feat_names],\n",
    "        \"threshold\": oper_t,\n",
    "        \"threshold_policy\": th_policy,  # override/balanced/default + 값\n",
    "        \"best_params\": best_params_clean,  # 요약(대표 시드 기준)\n",
    "        \"random_state\": int(RANDOM_STATE),\n",
    "\n",
    "        \"preprocess\": {\n",
    "            \"PREFIX_K\": int(K),\n",
    "            \"TOPK_BIGRAMS\": int(TOPK_BIGRAMS),\n",
    "            \"terminals\": sorted(list(TERMINALS_FINAL)),\n",
    "            \"note\": \"prefix-K 내 terminal 등장 세션 제외, terminal 전이 제외 bigram 사전\",\n",
    "        },\n",
    "\n",
    "        \"oversampling\": {\n",
    "            \"enabled\": oversampling_enabled,\n",
    "            \"method\": eff_os_cfg.get(\"method\"),\n",
    "            \"ratio\": _jsonable_sampling_strategy(eff_os_cfg.get(\"ratio\", \"auto\")),\n",
    "            \"k_neighbors\": int(eff_os_cfg.get(\"k_neighbors\", 5)),\n",
    "        },\n",
    "\n",
    "        \"artifacts_csv\": {\n",
    "            \"all\": OUT_ALL,\n",
    "            \"prefix_filtered\": OUT_PREFIXF_PATH(K),\n",
    "        },\n",
    "\n",
    "        \"validation\": {\n",
    "            \"Acc@Oper\": float(eval_by_k[K][\"Valid@Oper.Acc\"]),\n",
    "            \"MacroF1@Oper\": float(eval_by_k[K][\"Valid@Oper.MacroF1\"]),\n",
    "            \"Balanced_t\": float(eval_by_k[K][\"Balanced.t\"]),\n",
    "            \"Balanced_Score\": float(eval_by_k[K][\"Balanced.Score\"]),\n",
    "            \"single_best_f1_at_oper\": float(eval_by_k[K].get(\"single_best_f1_at_oper\", np.nan)),\n",
    "        },\n",
    "\n",
    "        # (신규) 학습/앙상블 정보\n",
    "        \"training\": {\n",
    "            \"n_restarts\": int(eval_by_k[K].get(\"n_restarts\", 1)),\n",
    "            \"seeds\": list(eval_by_k[K].get(\"seeds\", [RANDOM_STATE])),\n",
    "            \"candidate_count\": len(eval_by_k[K].get(\"seeds\", [RANDOM_STATE])),\n",
    "            \"ensemble\": {\"method\": \"mean\", \"threshold_search_grid\": 197},\n",
    "            \"feval\": {\"name\": \"macro_f1_balanced\", \"num_thresholds\": int(globals().get(\"_FEVAL_NUM_THRESHOLDS\", 61))},\n",
    "            \"early_stopping_rounds\": int(EARLY_STOPPING_ROUNDS) if use_xgb else None,\n",
    "            \"xgb_epochs\": int(XGB_EPOCHS) if use_xgb else None,\n",
    "            \"lr_max_iter\": int(LR_MAX_ITER) if not use_xgb else None,\n",
    "            \"selected_model\": selected_info,  # 최종 저장된 단일 모델 정보\n",
    "        },\n",
    "    }\n",
    "\n",
    "    meta_path = META_PATH_K(K)\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\" - Saved model: {model_path}\")\n",
    "    print(f\" - Saved meta : {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c8d3-ff7b-4b85-a4c0-071c0ced0f9e",
   "metadata": {},
   "source": [
    "## 11) 세션 이탈 추론 결과 값 시각화\n",
    "- 검증한 모델을 평가한 결과 값을 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7f739-0340-4eb0-827d-d0cca31ee127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11. 시각화 (K별 최종 단일 모델 @운영 임계값) ===\n",
    "\n",
    "import numpy as np  # <<< 추가: bar 차트용\n",
    "\n",
    "LABELS = [0, 1]\n",
    "LABEL_NAMES = {0: \"0 = Drop\", 1: \"1 = Complete\"}\n",
    "\n",
    "def _prob_single_for_k(K, X_va):\n",
    "    \"\"\"K에 대해 '최종 단일 모델'의 확률 벡터와 표시용 문자열을 반환.\"\"\"\n",
    "    model = trained_models_by_k[K]\n",
    "    # 표시용 모델명/시드\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    model_name = est.__class__.__name__\n",
    "    sel_idx = eval_by_k[K].get(\"selected_model_index\", None)\n",
    "    seeds = eval_by_k[K].get(\"seeds\", None)\n",
    "    if sel_idx is not None and seeds and 0 <= sel_idx < len(seeds):\n",
    "        model_str = f\"{model_name} (seed={seeds[sel_idx]})\"\n",
    "    else:\n",
    "        model_str = model_name\n",
    "\n",
    "    # 확률 예측\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X_va)[:, 1]\n",
    "    else:\n",
    "        # 예외적 상황 대비: decision_function만 있을 경우 시그모이드로 변환\n",
    "        z = model.decision_function(X_va)\n",
    "        p = 1.0 / (1.0 + np.exp(-z))\n",
    "    return p, model_str\n",
    "\n",
    "# <<< 추가: K별 성능 수집용 컨테이너\n",
    "ks, accs, f1s = [], [], []\n",
    "\n",
    "for K in K_LIST:\n",
    "    # 검증 세트 / 임계값\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "    t_oper = T_OPER_BY_K.get(K, eval_by_k[K][\"Oper.t\"])\n",
    "    t_bal  = eval_by_k[K].get(\"Balanced.t\", None)\n",
    "    t_label = (\n",
    "        f\"@t={t_oper:.3f}\"\n",
    "        if (t_bal is None or abs(t_oper - t_bal) < 1e-12)\n",
    "        else f\"@t={t_oper:.3f} (Balanced.t={t_bal:.3f})\"\n",
    "    )\n",
    "\n",
    "    # 단일 최종 모델 확률 및 예측\n",
    "    p, model_str = _prob_single_for_k(K, X_va)\n",
    "    yhat = (p >= t_oper).astype(int)\n",
    "\n",
    "    # 혼동행렬 및 지표\n",
    "    cm = confusion_matrix(y_va, yhat, labels=LABELS)\n",
    "    row_sums = cm.sum(axis=1, keepdims=True).astype(float)\n",
    "    cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums != 0)\n",
    "\n",
    "    acc = accuracy_score(y_va, yhat)\n",
    "    macro_f1 = f1_score(y_va, yhat, average=\"macro\")\n",
    "    n_va = len(y_va)\n",
    "\n",
    "    # <<< 추가: 수집\n",
    "    ks.append(K)\n",
    "    accs.append(acc)\n",
    "    f1s.append(macro_f1)\n",
    "\n",
    "    # 플롯(혼동행렬)\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 5.2))\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            correct = (i == j)\n",
    "            ax.add_patch(plt.Rectangle(\n",
    "                (j - 0.5, i - 0.5), 1, 1,\n",
    "                facecolor=(\"green\" if correct else \"red\"),\n",
    "                edgecolor=\"black\", alpha=0.30\n",
    "            ))\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{cm[i, j]}\\n({cm_norm[i, j]:.2f})\",\n",
    "                ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "    ax.set_xticks([0, 1]); ax.set_xticklabels([LABEL_NAMES[0], LABEL_NAMES[1]], rotation=15, ha=\"right\")\n",
    "    ax.set_yticks([0, 1]); ax.set_yticklabels([LABEL_NAMES[0], LABEL_NAMES[1]])\n",
    "    ax.set_xlim(-0.5, 1.5); ax.set_ylim(1.5, -0.5)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(f\"K={K} | {t_label} — Acc={acc:.4f}, Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=\"green\", edgecolor=\"black\", label=\"Correct (TP/TN)\"),\n",
    "        Patch(facecolor=\"red\", edgecolor=\"black\", label=\"Incorrect (FP/FN)\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_elems, loc=\"upper right\", frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === (추가) K별 ACC vs Macro-F1 막대그래프 ===\n",
    "# X축은 K (예: 3,5,7). ACC=파랑, F1=초록.\n",
    "# 필요 시 K 정렬\n",
    "order = np.argsort(ks)\n",
    "ks_sorted  = [ks[i]  for i in order]\n",
    "accs_sorted = [accs[i] for i in order]\n",
    "f1s_sorted  = [f1s[i] for i in order]\n",
    "\n",
    "x = np.arange(len(ks_sorted))\n",
    "width = 0.36\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "bars_acc = ax.bar(x - width/2, accs_sorted, width, label=\"ACC\", color=\"blue\")     # 파란 막대\n",
    "bars_f1  = ax.bar(x + width/2, f1s_sorted, width, label=\"Macro-F1\", color=\"green\")# 초록 막대\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ks_sorted)  # 예: 3,5,7\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Score (ACC & Macro-F1)\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# 값 라벨 표시\n",
    "ax.bar_label(bars_acc, fmt=\"%.4f\", padding=3)\n",
    "ax.bar_label(bars_f1,  fmt=\"%.4f\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de0a54-f10b-4800-9e29-d6557307c6b3",
   "metadata": {},
   "source": [
    "## 12) 추론 테스트\n",
    "- 검증된 모델에 샘플 데이터를 입력하여 추론 테스트를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c6436-e782-47fb-aba9-829b80775f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference Cell (K=7): paste TSV → load model → align features → predict ===\n",
    "import os, json, io, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ---- Config ----\n",
    "K = 7\n",
    "MODEL_DIR = \"models/sessionDrop\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f\"sessionDrop_model.k{K}.joblib\")\n",
    "META_PATH  = os.path.join(MODEL_DIR, f\"sessionDrop_model.k{K}.meta.json\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "def _sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _load_threshold(default=0.5):\n",
    "    # 1) meta.json → threshold, 2) eval_by_k[K][\"Oper.t\"] (메모리에 있다면), 3) default\n",
    "    try:\n",
    "        with open(META_PATH, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        return float(meta.get(\"threshold\", default))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return float(eval_by_k[K][\"Oper.t\"])  # Notebook 메모리에 남아있을 수 있음\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _load_feature_list():\n",
    "    # meta.json의 학습 시 feature 순서를 되살림(빅그램 포함)\n",
    "    try:\n",
    "        with open(META_PATH, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        feats = meta.get(\"features\", None)\n",
    "        if isinstance(feats, list) and len(feats) > 0:\n",
    "            return [str(c) for c in feats]\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return list(eval_by_k[K][\"features\"])  # 메모리 보조\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _ensure_numeric(df):\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == \"O\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out.fillna(0.0)\n",
    "\n",
    "# ---- Paste your TSV here (session_id \\t <features...> [\\t label]) ----\n",
    "RAW_TSV = \"\"\"0012eec7-d6c0-4eff-b4c4-727e03e46b0c\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t2.0\t1.0\t0.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t0.9556998911095345\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0\n",
    "002071d2-83a3-4cc1-ac33-b6a98353cd13\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.4750763110496952\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "a6d0ab18-6102-42d5-ad36-5c02efec6caf\t7.0\t60.0\t7.0\t8.571428571428571\t20.99562636671296\t41.99999999999996\t7.0\t1.0\t0.0\t1.0\t0.14285714285714285\t0.0\t0.0\t0.0\t2.0\t3.0\t0.0\t60.0\t-1.0\t1.0\t0.0\t0.0\t1.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t4.0\t1.1537419426970903\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0\n",
    "010f7eb5-8c35-4e69-bd9d-c0dacef95068\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t16.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t1.0042424730510766\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "c8d03998-8f4e-4c89-85c9-da22f5bc0e64\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t2.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.5498260458732018\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0\n",
    "c5b8c2db-e7c8-436e-96fd-da273dd148b7\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t2.0\t0.41011631828640904\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0\n",
    "51298911-140e-4357-8b66-f9acf0217700\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t4.0\t0.5\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t6.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t1.0789922078745835\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0\n",
    "504045be-f550-4bd7-bae4-0b736fc61dfb\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t16.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.5498260458732018\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "5032cacd-9fc5-4bc5-90b1-9ac4e6bd3416\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t6.0\t1.7478680974607577\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "fffe1852-ab15-4b57-8aa3-cd0fb92c3ccc\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t1.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t4.0\t1.1537419426970903\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1\n",
    "\"\"\"\n",
    "\n",
    "# ---- Read pasted data (whitespace-separated tolerant) ----\n",
    "df_raw = pd.read_csv(io.StringIO(RAW_TSV.strip()), sep=r\"\\s+\", header=None, engine=\"python\")\n",
    "\n",
    "# ---- Load feature order & model ----\n",
    "feature_list = _load_feature_list()\n",
    "if feature_list is None:\n",
    "    raise RuntimeError(\"학습 시 feature 목록(features)을 찾지 못했습니다. Cell 10에서 생성된 meta JSON이 있는지 확인하세요.\")\n",
    "\n",
    "expected_no_label = 1 + len(feature_list)     # session_id + features\n",
    "expected_with_label = expected_no_label + 1   # + label\n",
    "\n",
    "if df_raw.shape[1] not in (expected_no_label, expected_with_label):\n",
    "    raise RuntimeError(\n",
    "        f\"열 수가 예상과 다릅니다. got={df_raw.shape[1]} expected={expected_no_label} or {expected_with_label} \"\n",
    "        f\"(features={len(feature_list)})\"\n",
    "    )\n",
    "\n",
    "columns = [\"session_id\"] + feature_list\n",
    "has_label = (df_raw.shape[1] == expected_with_label)\n",
    "if has_label:\n",
    "    columns = columns + [\"label\"]\n",
    "df_raw.columns = columns\n",
    "\n",
    "# keep reference label if exists; drop from X\n",
    "y_ref = None\n",
    "if has_label:\n",
    "    y_ref = pd.to_numeric(df_raw[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "X = _ensure_numeric(df_raw[feature_list])\n",
    "\n",
    "# load model\n",
    "loaded_from = None\n",
    "try:\n",
    "    model = load(MODEL_PATH)\n",
    "    loaded_from = f\"[file] {MODEL_PATH}\"\n",
    "except Exception:\n",
    "    # fallback to memory (if you already trained in this kernel)\n",
    "    try:\n",
    "        model = trained_models_by_k[K]\n",
    "        loaded_from = \"[memory] trained_models_by_k\"\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"모델을 찾을 수 없습니다. '{MODEL_PATH}'가 존재하거나, 먼저 학습 셀(Cell 8~10)을 실행해 주세요. err={e}\")\n",
    "\n",
    "t_oper = _load_threshold(default=0.5)\n",
    "\n",
    "print(f\"[info] Using K={K} model from {loaded_from}\")\n",
    "print(f\"[info] threshold (Oper.t) = {t_oper:.3f}\")\n",
    "print(f\"[info] samples = {len(X)}, features = {X.shape[1]}\")\n",
    "\n",
    "# ---- Predict ----\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "else:\n",
    "    z = model.decision_function(X)\n",
    "    proba = _sigmoid(z)\n",
    "pred = (proba >= t_oper).astype(int)\n",
    "\n",
    "# ---- Display with rows + clear separators ----\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "if y_ref is not None:\n",
    "    truth_vals = y_ref.values\n",
    "    mark = np.where(pred == truth_vals, \"✓\", \"✗\")\n",
    "else:\n",
    "    truth_vals = [\"-\"] * len(pred)\n",
    "    mark = [\"\"] * len(pred)\n",
    "\n",
    "headers = [\"case\", \"truth\", \"pred\", \"✓/✗\", \"note\"]\n",
    "rows = [\n",
    "    [df_raw.get(\"session_id\", pd.Series(range(len(pred)))).astype(str).iloc[i],\n",
    "     int(truth_vals[i]) if truth_vals[i] in (0,1) else truth_vals[i],\n",
    "     int(pred[i]),\n",
    "     mark[i],\n",
    "     \"\"]\n",
    "    for i in range(len(pred))\n",
    "]\n",
    "\n",
    "df_view = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "display(HTML(\"<hr style='margin:6px 0;'>\"))\n",
    "display(HTML(\"<h4 style='margin:4px 0;'>Inference Result (K=7)</h4>\"))\n",
    "display(df_view)\n",
    "\n",
    "if y_ref is not None:\n",
    "    acc = accuracy_score(truth_vals, pred)\n",
    "    f1m = f1_score(truth_vals, pred, average=\"macro\")\n",
    "    display(HTML(f\"<div><b>[Done]</b> 정확도(샘플 {len(truth_vals)}개): {acc:.3f}\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
