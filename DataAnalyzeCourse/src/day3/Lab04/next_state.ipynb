{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f882790-7067-438a-b21d-eb00cdd81427",
   "metadata": {},
   "source": [
    "# 다음 상태 예측\n",
    "### 사용자 세션의 현재/이전 상태와 탭형 컨텍스트를 입력으로 받아 LSTM으로 다음 상태를 확률적으로 예측하는 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03391d44-2822-4a06-95b1-d88237df1295",
   "metadata": {},
   "source": [
    "# 0) 패키지 설치\n",
    "- 실습에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0d173-cf62-4287-9a1f-3b9888d04211",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyarrow==15.0.2 fastparquet pandas torch optuna scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346807f-a561-4567-9722-76edc7e0ac4c",
   "metadata": {},
   "source": [
    "# 1) 경로·시드·하이퍼파라미터 설정\n",
    "- 데이터/모델 저장 경로를 만들고, 재현성 시드와 LSTM 학습 하이퍼파라미터, 디바이스를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bfadc-39cb-4ae0-8fe3-9da32f1441ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "import torch\n",
    "\n",
    "# ---- 디렉토리/파일 경로 ----\n",
    "DATA_DIR = Path(\"/home/jovyan/datasets/next_state_pre\")  # 전처리 및 산출물 저장 디렉토리\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = Path(\"/home/jovyan/models/next_state_model\") # 모델 및 하이퍼파라미터 저장 디렉토리\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_PARQUET = Path(\"/home/jovyan/datasets/processed_user_behavior.sorted.parquet\")  # 원천 파케\n",
    "MAP_JSON    = DATA_DIR / \"state_mapping.json\"            # state 인덱스 매핑 저장\n",
    "SPLIT_JSON  = DATA_DIR / \"train_val_sessions.json\"       # train/val 세션 분할 저장\n",
    "PAIR_NPY    = DATA_DIR / \"observed_prev_pairs.npy\"       # 관측된 (prev2, prev1) 쌍 저장\n",
    "BEST_PATH   = MODEL_DIR / \"best_model.pt\"                # 베스트 체크포인트(학습 상태 dict) 저장\n",
    "\n",
    "# ---- 시드/분할 ----\n",
    "SEED = 17\n",
    "VAL_FRAC = 0.2  # 검증 세트 비율\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---- 하이퍼파라미터 ----\n",
    "HYPERPARAMS = {\n",
    "    \"emb_dim\": 192,        # 상태 임베딩 차원\n",
    "    \"pair_dim\": 16,        # (prev2, prev1) 선형투영 차원\n",
    "    \"tab_dim\": 16,         # 표형(tab) 피처 투영 차원: cart/page/idle/search 4D -> 16D\n",
    "    \"hid\": 384,            # LSTM hidden size\n",
    "    \"num_layers\": 2,       # LSTM layer 수\n",
    "    \"dropout\": 0.2,        # LSTM 출력 드롭아웃\n",
    "    \"lr\": 3e-4,            # learning rate\n",
    "    \"weight_decay\": 5e-4,  # AdamW weight decay\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 7,         # Early stopping patience (연속 bad epoch 수)\n",
    "    \"use_amp\": True,       # CUDA 있을 때 자동 혼합정밀도 사용\n",
    "    \"grad_clip_norm\": 1.0, # gradient clipping L2 norm\n",
    "}\n",
    "\n",
    "# ---- 디바이스 ----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DATA_DIR:\", DATA_DIR, \"| device:\", device)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c136f-b252-4d76-81d1-9bce891a13fb",
   "metadata": {},
   "source": [
    "# 2) 데이터 로드·컬럼 체크\n",
    "- processed_user_behavior.sorted.parquet을 읽고 session/state/tab 피처와 라벨 컬럼이 모두 존재하는지 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c578e-f9e3-45c2-8849-18b9ab39522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(RAW_PARQUET)\n",
    "\n",
    "need_cols = [\n",
    "    \"session_id\", \"current_state\", \"next_state\",            # 상태 전이 핵심\n",
    "    \"cart_item_count\",\"page_depth\",\"last_action_elapsed\",\"search_count\"  # 컨텍스트(tab) 피처\n",
    "]\n",
    "miss = [c for c in need_cols if c not in df.columns]\n",
    "assert not miss, f\"필수 컬럼 없음: {miss}\"\n",
    "\n",
    "print(df[need_cols].head(3))\n",
    "print(\"[INFO] rows:\", len(df))\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d61bc-fc29-4ab0-be16-0c057cc2ea76",
   "metadata": {},
   "source": [
    "# 3) 상태→인덱스 매핑 생성/저장\n",
    "- PAD=0, UNK=1 예약 후 상태에 2부터 ID를 부여해 JSON으로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ac90e-132f-403f-83c2-cc0d52cccde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID, UNK_ID = 0, 1\n",
    "\n",
    "# 현재/다음 상태의 전체 유니크 값으로 vocab 구성\n",
    "states_all = pd.Index(df[\"current_state\"].astype(str).unique()).union(\n",
    "    df[\"next_state\"].astype(str).unique()\n",
    ")\n",
    "states_sorted = pd.Index(sorted(states_all))\n",
    "\n",
    "state2idx = {s: i+2 for i, s in enumerate(states_sorted)}  # 2부터 시작\n",
    "idx2state = {i: s for s, i in state2idx.items()}\n",
    "num_states = len(state2idx) + 2  # PAD, UNK 포함한 클래스 수\n",
    "\n",
    "# 매핑 저장 (추론/리포트 단계에서 재사용)\n",
    "with open(MAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"PAD_ID\": PAD_ID, \"UNK_ID\": UNK_ID, \"state2idx\": state2idx}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[SAVE] state mapping ->\", MAP_JSON)\n",
    "print(\"[INFO] num_states:\", num_states)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d354367-d8cd-47ec-a7d2-a203bedeb8d5",
   "metadata": {},
   "source": [
    "# 4) prev1/prev2 생성·관측쌍 저장\n",
    "- 세션별 shift로 이전 상태 인덱스를 만들고, (prev2, prev1) 유니크 쌍을 NPY로 보관합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd22de-a944-4f1b-a5ff-04a3ed2285bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    mp = json.load(f)\n",
    "state2idx = mp[\"state2idx\"]; PAD_ID = mp[\"PAD_ID\"]; UNK_ID = mp[\"UNK_ID\"]\n",
    "\n",
    "def map_state(s):\n",
    "    return state2idx.get(str(s), UNK_ID)\n",
    "\n",
    "# 상태 → 인덱스\n",
    "df[\"current_state_idx\"] = df[\"current_state\"].astype(str).map(map_state).astype(\"int32\")\n",
    "df[\"next_state_idx\"]    = df[\"next_state\"].astype(str).map(map_state).astype(\"int32\")\n",
    "\n",
    "# 세션별로 이전 상태 shift\n",
    "g = df.groupby(\"session_id\")[\"current_state_idx\"]\n",
    "df[\"prev1_idx\"] = g.shift(1).fillna(PAD_ID).astype(\"int32\")\n",
    "df[\"prev2_idx\"] = g.shift(2).fillna(PAD_ID).astype(\"int32\")\n",
    "\n",
    "# 관측된 (prev2, prev1) 유니크 쌍 저장 (분석/디버깅용)\n",
    "pairs = df[[\"prev2_idx\",\"prev1_idx\"]].drop_duplicates().to_numpy(dtype=np.int32)\n",
    "np.save(PAIR_NPY, pairs)\n",
    "print(\"[SAVE] observed prev pairs ->\", PAIR_NPY, pairs.shape)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7cc05-8746-4845-a15b-125c8da6a181",
   "metadata": {},
   "source": [
    "# 5) 세션 단위 train/val 분리\n",
    "- 세션 ID를 셔플해 검증 비율만큼 떼고, 교차 오염 없이 분할 내역을 JSON으로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da19bf3-a55d-4df9-acc6-eec0dc7d7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = df[\"session_id\"].drop_duplicates().to_numpy()\n",
    "np.random.shuffle(sess)\n",
    "n_val = int(len(sess)*VAL_FRAC)\n",
    "val_sessions = set(sess[:n_val])\n",
    "train_sessions = set(sess[n_val:])\n",
    "\n",
    "with open(SPLIT_JSON, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seed\": SEED, \"val_frac\": VAL_FRAC,\n",
    "        \"train_sessions\": list(train_sessions),\n",
    "        \"val_sessions\": list(val_sessions)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"[SAVE] split ->\", SPLIT_JSON, \"| #train:\", len(train_sessions), \" #val:\", len(val_sessions))\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21039719-e175-490a-abd9-75debee44fb7",
   "metadata": {},
   "source": [
    "# 6) 시퀀스 데이터 구성\n",
    "- 각 세션의 마지막 스텝을 제외해 입력(state/pair/tab)과 라벨(next_state)을 T−1 길이로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8cbaa-5149-4ebb-a959-ba34fe18ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_session_sequences(frame):\n",
    "    seqs_x, seqs_prevpair, seqs_tab, seqs_y = [], [], [], []\n",
    "    for sid, sub in frame.sort_values([\"session_id\"]).groupby(\"session_id\"):\n",
    "        x = sub[\"current_state_idx\"].to_numpy(dtype=np.int32)\n",
    "        y = sub[\"next_state_idx\"].to_numpy(dtype=np.int32)\n",
    "        p1 = sub[\"prev1_idx\"].to_numpy(dtype=np.int32)\n",
    "        p2 = sub[\"prev2_idx\"].to_numpy(dtype=np.int32)\n",
    "        tabs = sub[[\"cart_item_count\",\"page_depth\",\"last_action_elapsed\",\"search_count\"]].to_numpy(dtype=np.float32)\n",
    "        if len(x) < 2:\n",
    "            continue\n",
    "        # 마지막 스텝은 다음 상태가 없으므로 제외\n",
    "        seqs_x.append(x[:-1])                                  # [T-1]\n",
    "        seqs_y.append(y[:-1])                                  # [T-1]\n",
    "        seqs_prevpair.append(np.stack([p2[:-1], p1[:-1]], 1))  # [T-1, 2]\n",
    "        seqs_tab.append(tabs[:-1])                             # [T-1, 4]\n",
    "    return seqs_x, seqs_prevpair, seqs_tab, seqs_y\n",
    "\n",
    "train_x, train_p, train_t, train_y = build_session_sequences(df[df[\"session_id\"].isin(train_sessions)])\n",
    "val_x,   val_p,   val_t,   val_y   = build_session_sequences(df[df[\"session_id\"].isin(val_sessions)])\n",
    "\n",
    "print(\"[INFO] #train seq:\", len(train_x), \" | #val seq:\", len(val_x))\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a9490-28cb-4d40-bfe1-348f9cf1836b",
   "metadata": {},
   "source": [
    "# 7) X,y 저장\n",
    "- 테이블 형태의 피처/라벨을 Parquet·CSV로 저장해 디버깅/재현에 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac843e-7efe-4a8a-9030-22f2dc834904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TAB_FEATS = [\n",
    "    \"current_state_idx\",\"prev1_idx\",\"prev2_idx\",  # 상태 기반 피처\n",
    "    \"cart_item_count\",\"page_depth\",\"last_action_elapsed\",\"search_count\"  # tab 피처\n",
    "]\n",
    "LABEL = \"next_state_idx\"\n",
    "\n",
    "# dtype 캐스팅 (파일 사이즈/일관성)\n",
    "int_cols   = [\"current_state_idx\",\"prev1_idx\",\"prev2_idx\",\"cart_item_count\",\"page_depth\",\"search_count\"]\n",
    "float_cols = [\"last_action_elapsed\"]\n",
    "\n",
    "def _cast_types(df_):\n",
    "    for c in int_cols:\n",
    "        df_[c] = df_[c].astype(\"int32\")\n",
    "    for c in float_cols:\n",
    "        df_[c] = df_[c].astype(\"float32\")\n",
    "    return df_\n",
    "\n",
    "X_train = df[df[\"session_id\"].isin(train_sessions)][TAB_FEATS].reset_index(drop=True).pipe(_cast_types)\n",
    "y_train = df[df[\"session_id\"].isin(train_sessions)][LABEL].reset_index(drop=True).astype(\"int32\")\n",
    "\n",
    "X_val   = df[df[\"session_id\"].isin(val_sessions)][TAB_FEATS].reset_index(drop=True).pipe(_cast_types)\n",
    "y_val   = df[df[\"session_id\"].isin(val_sessions)][LABEL].reset_index(drop=True).astype(\"int32\")\n",
    "\n",
    "# Parquet/CSV 저장\n",
    "(X_train).to_parquet(DATA_DIR/\"X_train.parquet\", index=False)\n",
    "(y_train.to_frame(name=\"y\")).to_parquet(DATA_DIR/\"y_train.parquet\", index=False)\n",
    "(X_val).to_parquet(DATA_DIR/\"X_val.parquet\", index=False)\n",
    "(y_val.to_frame(name=\"y\")).to_parquet(DATA_DIR/\"y_val.parquet\", index=False)\n",
    "\n",
    "X_train.to_csv(DATA_DIR/\"X_train.csv\", index=False)\n",
    "y_train.to_frame(name=\"y\").to_csv(DATA_DIR/\"y_train.csv\", index=False)\n",
    "X_val.to_csv(DATA_DIR/\"X_val.csv\", index=False)\n",
    "y_val.to_frame(name=\"y\").to_csv(DATA_DIR/\"y_val.csv\", index=False)\n",
    "\n",
    "print(\"[SAVE] flattened X,y ->\", DATA_DIR)\n",
    "print(\"  X_train:\", X_train.shape, \" | y_train:\", y_train.shape)\n",
    "print(\"  X_val  :\", X_val.shape,   \" | y_val  :\", y_val.shape)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfee812-2e89-46f0-9ade-d483f8933142",
   "metadata": {},
   "source": [
    "# 8) Dataset/Collate/DataLoader 정의\n",
    "- 가변 길이를 배치 최대 길이에 패딩하는 collate로 x/pair/tab/y/mask 텐서를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedf661-0209-4c4b-9b19-6a8fb39440c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NextStateSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xs, pairs, tabs, ys):\n",
    "        self.xs = xs; self.pairs = pairs; self.tabs = tabs; self.ys = ys\n",
    "    def __len__(self): return len(self.xs)\n",
    "    def __getitem__(self, i): return self.xs[i], self.pairs[i], self.tabs[i], self.ys[i]\n",
    "\n",
    "def pad_collate(batch, pad_id=PAD_ID):\n",
    "    xs, ps, ts, ys = zip(*batch)\n",
    "    L = max(len(x) for x in xs)   # 배치 내 최대 길이\n",
    "    B = len(xs)                   # 배치 크기\n",
    "\n",
    "    # 패딩 버퍼 생성\n",
    "    x_pad = np.full((B, L), pad_id, dtype=np.int64)\n",
    "    y_pad = np.full((B, L), pad_id, dtype=np.int64)\n",
    "    mask  = np.zeros((B, L), dtype=np.bool_)      # 유효 토큰 마스크\n",
    "    pair_pad = np.full((B, L, 2), pad_id, dtype=np.int64)\n",
    "    tab_pad  = np.zeros((B, L, 4), dtype=np.float32)\n",
    "\n",
    "    # 각 샘플을 왼쪽 정렬로 채우기\n",
    "    for i,(x,p,t,y) in enumerate(zip(xs,ps,ts,ys)):\n",
    "        l = len(x)\n",
    "        x_pad[i,:l] = x; y_pad[i,:l] = y; mask[i,:l] = True\n",
    "        pair_pad[i,:l,:] = p; tab_pad[i,:l,:] = t\n",
    "\n",
    "    return (torch.from_numpy(x_pad),\n",
    "            torch.from_numpy(pair_pad),\n",
    "            torch.from_numpy(tab_pad),\n",
    "            torch.from_numpy(y_pad),\n",
    "            torch.from_numpy(mask))\n",
    "\n",
    "# DataLoader\n",
    "train_ds = NextStateSeqDataset(train_x, train_p, train_t, train_y)\n",
    "val_ds   = NextStateSeqDataset(val_x,   val_p,   val_t,   val_y)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=HYPERPARAMS[\"batch_size\"], shuffle=True,  collate_fn=pad_collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=HYPERPARAMS[\"batch_size\"], shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "# 샘플 배치 형태 확인\n",
    "xb, pb, tb, yb, mb = next(iter(train_loader))\n",
    "print(\"[INFO] batch shapes:\", xb.shape, pb.shape, tb.shape, yb.shape, mb.shape)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df2698-477b-4b64-aca4-219b766047bc",
   "metadata": {},
   "source": [
    "# 9) LSTM 분류기 정의\n",
    "- 상태 임베딩+pair 투영+tab 투영을 concat해 LSTM→Dropout→Linear로 다음 상태 로짓을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da745fa0-4eba-4213-b20e-4923f87fac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=192, pair_dim=16, tab_dim=16,\n",
    "                 hid=384, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_classes = vocab_size\n",
    "\n",
    "        # 상태 임베딩 (PAD는 0으로 고정)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
    "\n",
    "        # (prev2, prev1) → pair_dim\n",
    "        self.pair_proj = nn.Linear(2, pair_dim)\n",
    "\n",
    "        # tab 피처(4D) → tab_dim\n",
    "        self.tab_proj  = nn.Linear(4, tab_dim)\n",
    "\n",
    "        # LSTM 입력 차원 = emb_dim + pair_dim + tab_dim\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim + pair_dim + tab_dim,\n",
    "                            hidden_size=hid, num_layers=num_layers,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(hid, vocab_size)\n",
    "\n",
    "    def forward(self, x_state, pair, x_tab, mask):\n",
    "        xe = self.emb(x_state)             # [B,T,E]\n",
    "        pe = self.pair_proj(pair.float())  # [B,T,P]\n",
    "        te = self.tab_proj(x_tab.float())  # [B,T,Td]\n",
    "        inp = torch.cat([xe, pe, te], dim=-1)\n",
    "        out, _ = self.lstm(inp)            # [B,T,H]\n",
    "        out = self.drop(out)\n",
    "        return self.head(out)              # [B,T,C]\n",
    "\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7482f6-8ef3-4dbe-b492-5eb7f0a62860",
   "metadata": {},
   "source": [
    "# 10) 클래스 가중치 산출\n",
    "- 라벨 분포의 inverse-frequency 기반 가중치를 계산해 MAX_WEIGHT로 클리핑 후 리포트/저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25f9e7-759f-4289-8e96-1cd4bf9b5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "MAX_WEIGHT = 1.5  # 상한 클리핑 (희소 클래스 가중치를 너무 크게 만들지 않기 위해)\n",
    "\n",
    "# 라벨 분포\n",
    "label_counts = Counter(df[\"next_state_idx\"].tolist())\n",
    "total = sum(label_counts.values())\n",
    "\n",
    "# 가중치 계산 (raw -> clip)\n",
    "weights = np.ones(num_states, dtype=np.float32)\n",
    "raw_weights = np.ones(num_states, dtype=np.float32)\n",
    "\n",
    "for k, v in label_counts.items():\n",
    "    raw = total / (len(label_counts) * v)  # inverse freq 기반\n",
    "    raw_weights[k] = raw\n",
    "    weights[k] = raw\n",
    "\n",
    "weights = np.clip(weights, None, MAX_WEIGHT)\n",
    "class_weights = torch.tensor(weights, device=device)  # 손실 함수에 전달\n",
    "\n",
    "# per-class 테이블 (PAD/UNK 제외, 희소 클래스부터 보기 쉽도록 support↑ 정렬)\n",
    "rows = []\n",
    "for idx, support in sorted(label_counts.items(), key=lambda x: x[1]):\n",
    "    if idx in (PAD_ID, UNK_ID):\n",
    "        continue\n",
    "    label = idx2state.get(int(idx), f\"UNKNOWN_{idx}\")\n",
    "    rows.append({\n",
    "        \"class_id\": int(idx),\n",
    "        \"label\": label,\n",
    "        \"support\": int(support),\n",
    "        \"raw_weight\": float(raw_weights[idx]),\n",
    "        \"clipped_weight\": float(weights[idx]),\n",
    "    })\n",
    "\n",
    "df_class_weight = pd.DataFrame(rows).sort_values([\"support\",\"label\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"[INFO] class_weights ready (clipped @ {MAX_WEIGHT})  | classes={len(df_class_weight)}\")\n",
    "print(df_class_weight.to_string(index=False))\n",
    "\n",
    "# CSV 저장 (추후 비교/대시보드용)\n",
    "out_csv = DATA_DIR / \"class_weights_report.csv\"\n",
    "df_class_weight.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"[SAVE] per-class weights ->\", out_csv)\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae56b15-e772-44ac-9bd0-fcf0d101459c",
   "metadata": {},
   "source": [
    "# 11) 학습·평가 루틴 구현\n",
    "- AMP/GradClip/가중치/마스크를 반영해 train_one_epoch와 eval_epoch로 top1·top3·macroF1을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46c194-9988-4694-82d1-f9a87f7099a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, num_classes, class_weights=None,\n",
    "                    use_amp=True, grad_clip_norm=1.0, scaler=None):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
    "    correct = 0; total = 0; last_loss = 0.0\n",
    "\n",
    "    for x, pair, tab, y, mask in loader:\n",
    "        x, pair, tab, y, mask = x.to(device), pair.to(device), tab.to(device), y.to(device), mask.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp and scaler is not None:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                logits = model(x, pair, tab, mask)                      # [B,T,C]\n",
    "                loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip_norm:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            logits = model(x, pair, tab, mask)\n",
    "            loss = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "            loss.backward()\n",
    "            if grad_clip_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        last_loss = float(loss.item())\n",
    "        with torch.no_grad():\n",
    "            pred  = logits.argmax(-1)                # [B,T]\n",
    "            valid = (mask & (y != PAD_ID))           # 유효 타임스텝\n",
    "            correct += (pred[valid] == y[valid]).sum().item()\n",
    "            total   += valid.sum().item()\n",
    "\n",
    "    return {\"loss\": last_loss, \"acc\": correct / max(total, 1)}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, num_classes, class_weights=None):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, weight=class_weights)\n",
    "    total_valid=0; top1=0; top3=0; sum_loss=0; n_batches=0\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    for x, pair, tab, y, mask in loader:\n",
    "        x, pair, tab, y, mask = x.to(device), pair.to(device), tab.to(device), y.to(device), mask.to(device)\n",
    "        logits = model(x, pair, tab, mask)\n",
    "        loss   = loss_fn(logits.view(-1, num_classes), y.view(-1))\n",
    "        sum_loss += float(loss.item()); n_batches += 1\n",
    "\n",
    "        prob = logits.softmax(-1)                    # [B,T,C]\n",
    "        pred = prob.argmax(-1)                       # [B,T]\n",
    "\n",
    "        valid = (mask & (y != PAD_ID))\n",
    "        top1 += (pred[valid] == y[valid]).sum().item()\n",
    "        total_valid += valid.sum().item()\n",
    "\n",
    "        _, topk_idx = prob.topk(3, dim=-1)          # [B,T,3]\n",
    "        top3 += (topk_idx[valid].eq(y[valid].unsqueeze(-1))).any(dim=-1).sum().item()\n",
    "\n",
    "        all_preds.extend(pred[valid].cpu().tolist())\n",
    "        all_trues.extend(y[valid].cpu().tolist())\n",
    "\n",
    "    macro_f1 = f1_score(all_trues, all_preds, average=\"macro\") if all_trues else 0.0\n",
    "    return {\"loss\": sum_loss/max(n_batches,1), \"top1\": top1/max(total_valid,1),\n",
    "            \"top3\": top3/max(total_valid,1), \"macro_f1\": macro_f1}\n",
    "\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391efe1-7633-45a7-808f-43656aa02dbe",
   "metadata": {},
   "source": [
    "# 12) Optuna 하이퍼파라미터 튜닝\n",
    "- 검증 top1을 최대화하는 하이퍼파라미터를 탐색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519d4ff-2607-45e5-8de0-2b4e1991a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    # ---- 탐색 범위(필요 시 조정) ----\n",
    "    emb_dim    = trial.suggest_int(\"emb_dim\", 128, 256, step=32)\n",
    "    pair_dim   = trial.suggest_int(\"pair_dim\", 8, 24, step=8)\n",
    "    tab_dim    = trial.suggest_int(\"tab_dim\", 8, 32, step=8)\n",
    "    hid        = trial.suggest_int(\"hid\", 256, 384, step=64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n",
    "    dropout    = trial.suggest_float(\"dropout\", 0.2, 0.4)\n",
    "    lr         = trial.suggest_float(\"lr\", 5e-4, 2e-3, log=True)\n",
    "    wd         = trial.suggest_float(\"weight_decay\", 1e-5, 5e-4, log=True)\n",
    "\n",
    "    # ---- 모델/옵티마/AMP ----\n",
    "    m = LSTMClassifier(num_states, emb_dim, pair_dim, tab_dim, hid, num_layers, dropout).to(device)\n",
    "    opt = AdamW(m.parameters(), lr=lr, weight_decay=wd)\n",
    "    scaler_tmp = GradScaler(\"cuda\", enabled=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]))\n",
    "\n",
    "    # ---- 1 epoch quick fit ----\n",
    "    train_one_epoch(m, train_loader, opt, device, num_states, class_weights=class_weights,\n",
    "                    use_amp=(device==\"cuda\" and HYPERPARAMS[\"use_amp\"]),\n",
    "                    grad_clip_norm=HYPERPARAMS[\"grad_clip_norm\"], scaler=scaler_tmp)\n",
    "\n",
    "    # ---- 검증 ----\n",
    "    ev = eval_epoch(m, val_loader, device, num_states, class_weights=class_weights)\n",
    "    print(f\"[Trial {trial.number:02d}] top1={ev['top1']*100:6.2f}% | top3={ev['top3']*100:6.2f}% | F1={ev['macro_f1']:.3f}\")\n",
    "\n",
    "    # 로그용 부가 지표 기록\n",
    "    trial.set_user_attr(\"top3\", ev[\"top3\"])\n",
    "    trial.set_user_attr(\"macro_f1\", ev[\"macro_f1\"])\n",
    "    return ev[\"top1\"]\n",
    "\n",
    "# ---- 튜닝 실행 ----\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"fix_final_next_state_v1\")\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# ---- 결과 요약 ----\n",
    "best_trial = study.best_trial\n",
    "print(\"\\n[BEST RESULT]\")\n",
    "print(f\" top1    : {best_trial.value*100:.2f}%\")\n",
    "print(f\" top3    : {best_trial.user_attrs.get('top3')*100:.2f}%\")\n",
    "print(f\" macroF1 : {best_trial.user_attrs.get('macro_f1'):.3f}\")\n",
    "print(\" params  :\", best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afe425-7cb0-43dd-ad2f-c84e90c34b5f",
   "metadata": {},
   "source": [
    "# 13) 최종 학습·체크포인트 저장\n",
    "- 최적 하이퍼 파라미터를 적용해 ReduceLROnPlateau+EarlyStopping으로 훈련하고 best 모델을 pt로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184fa870-747b-4062-9a9d-a8d747fbf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "HYPERPARAMS.update({\n",
    "    \"emb_dim\":      best_params[\"emb_dim\"],\n",
    "    \"pair_dim\":     best_params[\"pair_dim\"],\n",
    "    \"tab_dim\":      best_params[\"tab_dim\"],\n",
    "    \"hid\":          best_params[\"hid\"],\n",
    "    \"num_layers\":   best_params[\"num_layers\"],\n",
    "    \"dropout\":      best_params[\"dropout\"],\n",
    "    \"lr\":           best_params[\"lr\"],\n",
    "    \"weight_decay\": best_params[\"weight_decay\"],\n",
    "})\n",
    "print(\"[INFO] HYPERPARAMS updated with Optuna best:\", HYPERPARAMS)\n",
    "\n",
    "# 하이퍼 저장\n",
    "BEST_JSON = (Path(BEST_PATH).parent / \"best_hparams.json\")\n",
    "with open(BEST_JSON, \"w\") as f:\n",
    "    json.dump({\"optuna_best\": best_params, \"hparams\": HYPERPARAMS}, f, indent=2)\n",
    "print(f\"[SAVE] best_hparams -> {BEST_JSON}\")\n",
    "\n",
    "# 학습 설정\n",
    "USE_AMP   = (device == \"cuda\") and bool(HYPERPARAMS.get(\"use_amp\", True))\n",
    "EPOCHS    = int(HYPERPARAMS.get(\"epochs\", 50))\n",
    "PATIENCE  = int(HYPERPARAMS.get(\"patience\", 7))\n",
    "GRAD_CLIP = float(HYPERPARAMS.get(\"grad_clip_norm\", 1.0))\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=num_states,\n",
    "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
    "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
    "    tab_dim=HYPERPARAMS[\"tab_dim\"],\n",
    "    hid=HYPERPARAMS[\"hid\"],\n",
    "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
    "    dropout=HYPERPARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=HYPERPARAMS[\"lr\"], weight_decay=HYPERPARAMS[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "scaler    = GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "best, bad = None, 0\n",
    "for epoch in range(EPOCHS):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                         num_classes=num_states, class_weights=class_weights,\n",
    "                         use_amp=USE_AMP, grad_clip_norm=GRAD_CLIP, scaler=scaler)\n",
    "    ev = eval_epoch(model, val_loader, device, num_states, class_weights=class_weights)\n",
    "    scheduler.step(ev[\"top1\"])\n",
    "\n",
    "    print(f\"[E{epoch:02d}] train loss={tr['loss']:.4f} acc={tr['acc']:.4f} | \"\n",
    "          f\"val loss={ev['loss']:.4f} top1={ev['top1']:.4f} top3={ev['top3']:.4f} macroF1={ev['macro_f1']:.4f}\")\n",
    "\n",
    "    if (best is None) or (ev[\"top1\"] > best[\"top1\"]):\n",
    "        best, bad = {\"epoch\": epoch, **ev}, 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"metrics\": ev,\n",
    "            \"num_classes\": num_states,\n",
    "            \"pad_id\": PAD_ID,\n",
    "            \"state2idx\": state2idx,\n",
    "            \"idx2state\": idx2state,\n",
    "            \"hparams\": HYPERPARAMS,\n",
    "        }, str(BEST_PATH))\n",
    "        print(f\"[SAVE] best -> {BEST_PATH}  (epoch={epoch}, top1={ev['top1']:.4f})\")\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= PATIENCE:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "print(\"\\n[BEST FINAL]\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e8497-eeb5-4a5e-acab-846e741208d2",
   "metadata": {},
   "source": [
    "# 14) per-class 지표 생성\n",
    "- 체크포인트로 검증셋 예측을 수집해 class별 support/top1/top3/precision/recall/F1을 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeb5c1-2014-4a86-bf2a-82400089a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "\n",
    "# 체크포인트 로드\n",
    "ckpt = torch.load(str(BEST_PATH), map_location=device)\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=ckpt.get(\"num_classes\", num_states),\n",
    "    emb_dim=HYPERPARAMS[\"emb_dim\"],\n",
    "    pair_dim=HYPERPARAMS[\"pair_dim\"],\n",
    "    tab_dim=HYPERPARAMS[\"tab_dim\"],\n",
    "    hid=HYPERPARAMS[\"hid\"],\n",
    "    num_layers=HYPERPARAMS[\"num_layers\"],\n",
    "    dropout=HYPERPARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# val 예측 수집\n",
    "all_true, all_pred, all_top3 = [], [], []\n",
    "with torch.no_grad():\n",
    "    for x, pair, tab, y, mask in val_loader:\n",
    "        x, pair, tab, y, mask = x.to(device), pair.to(device), tab.to(device), y.to(device), mask.to(device)\n",
    "        logits = model(x, pair, tab, mask)\n",
    "        prob   = logits.softmax(-1)\n",
    "        pred   = prob.argmax(-1)\n",
    "        _, topk_idx = prob.topk(3, dim=-1)\n",
    "\n",
    "        valid = (mask & (y != PAD_ID))\n",
    "        all_true.extend(y[valid].detach().cpu().tolist())\n",
    "        all_pred.extend(pred[valid].detach().cpu().tolist())\n",
    "        all_top3.extend(topk_idx[valid].detach().cpu().tolist())  # 각 항목 길이=3\n",
    "\n",
    "# per-class 지표\n",
    "labels = sorted(set(all_true))\n",
    "prec, rec, f1, support = precision_recall_fscore_support(all_true, all_pred, labels=labels, zero_division=0)\n",
    "\n",
    "# top1/top3 per class\n",
    "correct_per_cls = defaultdict(int); total_per_cls = defaultdict(int)\n",
    "for t, p in zip(all_true, all_pred):\n",
    "    total_per_cls[t] += 1\n",
    "    if t == p:\n",
    "        correct_per_cls[t] += 1\n",
    "top1_acc_per_cls = {c: (correct_per_cls[c] / total_per_cls[c]) if total_per_cls[c] > 0 else 0.0 for c in labels}\n",
    "\n",
    "hit3_per_cls = defaultdict(int)\n",
    "for t, top3 in zip(all_true, all_top3):\n",
    "    if int(t) in list(map(int, top3)):\n",
    "        hit3_per_cls[t] += 1\n",
    "top3_hit_per_cls = {c: (hit3_per_cls[c] / total_per_cls[c]) if total_per_cls[c] > 0 else 0.0 for c in labels}\n",
    "\n",
    "# DataFrame 생성\n",
    "rows = []\n",
    "for i, c in enumerate(labels):\n",
    "    state_name = idx2state.get(int(c), f\"UNKNOWN_{c}\")\n",
    "    rows.append({\n",
    "        \"class_id\": int(c),\n",
    "        \"label\": state_name,\n",
    "        \"support\": int(support[i]),\n",
    "        \"top1_acc\": float(top1_acc_per_cls.get(c, 0.0)),\n",
    "        \"top3_hit_rate\": float(top3_hit_per_cls.get(c, 0.0)),\n",
    "        \"precision\": float(prec[i]),\n",
    "        \"recall\": float(rec[i]),\n",
    "        \"f1\": float(f1[i]),\n",
    "    })\n",
    "\n",
    "df_cls = pd.DataFrame(rows).sort_values([\"support\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 요약\n",
    "print(df_cls.head(10))\n",
    "print(\"\\n[SUMMARY]\")\n",
    "print(\"macroF1 (val):\", df_cls[\"f1\"].mean().round(4))\n",
    "print(\"weighted F1 (val):\", (df_cls[\"f1\"] * df_cls[\"support\"] / df_cls[\"support\"].sum()).sum().round(4))\n",
    "print(\"[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c84a5-6c66-4676-af8e-b44e1ee2d76b",
   "metadata": {},
   "source": [
    "# 15) per-class 표 스타일링 표시\n",
    "- df_cls에 bar/gradient/format을 적용해 support·정확도·F1을 시각적으로 강조합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581e548-221e-49aa-a606-c6daa7b4ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "styled = (\n",
    "    df_cls.style\n",
    "    .bar(subset=[\"support\"], color=\"#d0e0f0\")                       # support를 bar 느낌으로\n",
    "    .background_gradient(subset=[\"top1_acc\",\"top3_hit_rate\",\"f1\"],  # 핵심 지표 색상 강조\n",
    "                         cmap=\"Blues\")\n",
    "    .format({\n",
    "        \"top1_acc\": \"{:.2%}\",\n",
    "        \"top3_hit_rate\": \"{:.2%}\",\n",
    "        \"precision\": \"{:.3f}\",\n",
    "        \"recall\": \"{:.3f}\",\n",
    "        \"f1\": \"{:.3f}\",\n",
    "    })\n",
    ")\n",
    "\n",
    "display(styled.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a56665-ae00-4033-87fa-13dc73c0f10f",
   "metadata": {},
   "source": [
    "# 16) 혼동행렬 시각화\n",
    "- 실제×예측 카운트를 표시하고 밝기 기반 텍스트 색 전환·대각선(정답) 강조 테두리를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e06a7-7132-43d3-94c1-503731c01cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if \"all_true\" not in globals() or \"all_pred\" not in globals():\n",
    "    raise RuntimeError(\"all_true/all_pred가 없습니다. 먼저 Cell 13(클래스별 리포트 생성)을 실행하세요.\")\n",
    "\n",
    "cls_ids   = labels\n",
    "cls_names = [idx2state.get(int(c), f\"UNK_{c}\") for c in cls_ids]\n",
    "\n",
    "cm = confusion_matrix(all_true, all_pred, labels=cls_ids)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, len(cls_ids)*0.6), max(6, len(cls_ids)*0.6)))\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")  # 기본 colormap (viridis 등)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Confusion Matrix (Counts)\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_xticks(np.arange(len(cls_ids))); ax.set_yticks(np.arange(len(cls_ids)))\n",
    "ax.set_xticklabels(cls_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(cls_names)\n",
    "\n",
    "# === 값의 밝기에 따른 텍스트 색상 자동 전환 ===\n",
    "# imshow의 norm을 사용 (값→0~1 정규화). 밝기가 0.5 넘으면 배경이 밝다고 보고 검은색 텍스트 사용.\n",
    "norm = getattr(im, \"norm\", None)\n",
    "if norm is None:\n",
    "    from matplotlib import colors\n",
    "    norm = colors.Normalize(vmin=float(np.min(cm)), vmax=float(np.max(cm)))\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        val = cm[i, j]\n",
    "        bright = norm(val) > 0.5  # True면 밝은 셀(검은 글자), False면 어두운 셀(흰 글자)\n",
    "        txt_color = \"black\" if bright else \"white\"\n",
    "        ax.text(j, i, f\"{val:d}\", ha=\"center\", va=\"center\", fontsize=8, color=txt_color)\n",
    "\n",
    "# === 대각선(정답) 셀만 테두리 강조 ===\n",
    "for k in range(len(cls_ids)):\n",
    "    rect = Rectangle((k - 0.5, k - 0.5), 1, 1,\n",
    "                     fill=False, edgecolor=\"red\", linewidth=2, zorder=3)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd2669-ca73-499c-92b8-329e317ac43f",
   "metadata": {},
   "source": [
    "# 17) Top-1 정확도 막대그래프\n",
    "- support 내림차순으로 정렬된 클래스별 top1 정확도를 bar 차트로 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e4bd3-04d5-4ae2-aaa3-679937e3153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"df_cls\" not in globals():\n",
    "    raise RuntimeError(\"df_cls가 없습니다. 먼저 Cell 13을 실행하세요.\")\n",
    "\n",
    "order = df_cls.sort_values(\"support\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(max(8, len(order)*0.5), 5))\n",
    "plt.bar(order[\"label\"], order[\"top1_acc\"])\n",
    "plt.title(\"Per-Class Top-1 Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(DATA_DIR / \"per_class_top1.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e048d98-b00d-463a-bc41-8fc25a656c0a",
   "metadata": {},
   "source": [
    "# 18) 오류율(1−Top1) 막대그래프\n",
    "- 클래스별 상대적 오분류 정도를 error rate로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36584119-0550-4ce3-9d7c-c56c28bd65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "order = df_cls.sort_values(\"support\", ascending=False)\n",
    "error_rate = 1.0 - order[\"top1_acc\"]\n",
    "\n",
    "plt.figure(figsize=(max(8, len(order)*0.5), 5))\n",
    "plt.bar(order[\"label\"], error_rate)\n",
    "plt.title(\"Per-Class Error Rate (1 − Top-1)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(DATA_DIR / \"per_class_error.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
