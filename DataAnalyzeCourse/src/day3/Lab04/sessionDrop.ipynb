{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25249867-3760-45a1-bb3e-52fec245c9c3",
   "metadata": {},
   "source": [
    "# ì„¸ì…˜ ì´íƒˆ\n",
    "### ì‚¬ìš©ìì˜ ì²« í™œë™ë¶€í„° Kê°œì˜ í™œë™ê¹Œì§€ ë¶„ì„í•˜ì—¬ ì„¸ì…˜ì´ ì™„ë£Œë ì§€ ì¤‘ê°„ì— ì´íƒˆí• ì§€ ì¶”ë¡ í•˜ëŠ” ì‹¤ìŠµì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c10e7-7711-4b02-9e19-b2b23e7bb5ff",
   "metadata": {},
   "source": [
    "# 0) íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "- ì‹¤ìŠµì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962590e-2f0c-4b6f-8c91-88dfec598ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec43eb-ccb2-4ded-bdbb-5b9e23f897b9",
   "metadata": {},
   "source": [
    "## 1) í™˜ê²½ êµ¬ì„±\n",
    "- ì‹¤ìŠµì„ ì›í™œí•˜ê²Œ ì§„í–‰í•˜ê¸° ìœ„í•´ í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd433fae-7f93-4f27-bc7e-fd3d540112b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1. í™˜ê²½ êµ¬ì„± ===\n",
    "import os, re, json, inspect\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from io import StringIO\n",
    "from joblib import load\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix, average_precision_score\n",
    ")\n",
    "from sklearn.utils import check_random_state\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "_imb_ok = True\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except Exception as e:\n",
    "    print(\"[warn] imbalanced-learnë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜¤ë²„ìƒ˜í”Œë§ ë¹„í™œì„±í™”:\", e)\n",
    "    _imb_ok = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = check_random_state(RANDOM_STATE)\n",
    "TOPK_BIGRAMS = 50\n",
    "K_LIST       = [3, 5, 7]\n",
    "TERMINALS_FINAL = {\"/logout\", \"/delete_user\"}\n",
    "\n",
    "OPER_POLICY = \"balanced\" \n",
    "T_OPER_DEFAULT = 0.730\n",
    "OPER_T_OVERRIDE = None\n",
    "\n",
    "# === (ì‹ ê·œ) Epoch/ì¬í•™ìŠµ ë…¸ë¸Œ ===\n",
    "XGB_EPOCHS = 1200              # XGBoost \"epochs\" == n_estimators\n",
    "EARLY_STOPPING_ROUNDS = 100    # XGBoost ì¡°ê¸°ì¢…ë£Œ ë¼ìš´ë“œ\n",
    "LR_MAX_ITER = 8000             # LogisticRegression ë°˜ë³µ ìƒí•œ(ìœ ì‚¬-epoch)\n",
    "N_RESTARTS = 3                  # ë©€í‹° ì‹œë“œ ì¬í•™ìŠµ íšŸìˆ˜\n",
    "SEED_SEQ = [42, 1337, 2027, 7777, 2025][:N_RESTARTS]\n",
    "\n",
    "# ì˜¤ë²„ìƒ˜í”Œë§ ì˜µì…˜ - í•œ ìª½ì˜ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì„ ì‹œ ì‚¬ìš©\n",
    "OVERSAMPLING = {\n",
    "    \"enable\": False,          # ì „ì—­ ê¸°ë³¸ê°’ ( True = on / False = off )\n",
    "    \"method\": \"smote\",        # \"random\" | \"smote\"\n",
    "    \"ratio\": \"auto\",          # 1.0=ì™„ì „ê· í˜•, 0.5=ì†Œìˆ˜/ë‹¤ìˆ˜=0.5, \"auto\"=ìë™ê· í˜•\n",
    "    \"k_neighbors\": 5,         # SMOTEìš©\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "# Kë³„ ì˜¤ë²„ìƒ˜í”Œë§ override (ì „ì—­ ê¸°ë³¸ê°’ì„ ë®ì–´ì”€)\n",
    "OVERSAMPLING_BY_K = {\n",
    "    3: {\"enable\": False, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=3 \n",
    "    5: {\"enable\": True, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=5\n",
    "    7: {\"enable\": False, \"method\": \"smote\", \"ratio\": \"auto\", \"k_neighbors\": 5}, # K=7\n",
    "}\n",
    "\n",
    "def _merge_oversampling_cfg(global_cfg: dict, per_k_cfg: dict) -> dict:\n",
    "    cfg = dict(global_cfg)\n",
    "    if per_k_cfg:\n",
    "        cfg.update(per_k_cfg)\n",
    "    return cfg\n",
    "\n",
    "def _normalize_sampling_strategy(val):\n",
    "    \"\"\"imblearnì´ ë°›ëŠ” sampling_strategy í˜•íƒœë¡œ ì •ê·œí™”.\"\"\"\n",
    "    if callable(val):\n",
    "        return val\n",
    "    if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "        return float(val)  # ì˜ˆ: 0.5\n",
    "    if isinstance(val, dict):\n",
    "        return {k: (int(v) if isinstance(v, (np.integer,)) else int(v) if isinstance(v, bool) else v)\n",
    "                for k, v in val.items()}\n",
    "    if val is None:\n",
    "        return \"auto\"\n",
    "    if isinstance(val, str):\n",
    "        v = val.strip().lower()\n",
    "        if v in {\"auto\", \"minority\", \"not minority\", \"not majority\", \"all\"}:\n",
    "            return v\n",
    "        try:\n",
    "            return float(v)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid sampling_strategy (ratio): {val!r}\")\n",
    "    raise TypeError(f\"Unsupported sampling_strategy type: {type(val)}\")\n",
    "\n",
    "def _build_sampler(cfg: dict):\n",
    "    \"\"\"ì˜¤ë²„ìƒ˜í”ŒëŸ¬ ìƒì„±(íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œë§Œ ì‚¬ìš©).\"\"\"\n",
    "    if (not cfg.get(\"enable\")) or (not _imb_ok):\n",
    "        return None\n",
    "    method = str(cfg.get(\"method\", \"random\")).lower()\n",
    "    rs     = cfg.get(\"random_state\", RANDOM_STATE)\n",
    "    ss     = _normalize_sampling_strategy(cfg.get(\"ratio\", \"auto\"))\n",
    "    if method == \"random\":\n",
    "        return RandomOverSampler(sampling_strategy=ss, random_state=rs)\n",
    "    elif method == \"smote\":\n",
    "        return SMOTE(sampling_strategy=ss, random_state=rs, k_neighbors=int(cfg.get(\"k_neighbors\", 5)))\n",
    "    else:\n",
    "        print(f\"[warn] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë²„ìƒ˜í”Œë§ ë°©ë²•: {method}. ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "# ì…ë ¥/ì¶œë ¥ ê²½ë¡œ\n",
    "INPUT_PATH = \"datasets/processed_user_behavior.sorted.csv\"\n",
    "OUT_DIR = \"datasets/sessionDrop\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_ALL = os.path.join(OUT_DIR, \"sessionDrop.features.csv\")\n",
    "def OUT_PREFIXF_PATH(k:int) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"sessionDrop.prefix{k}.features.filtered.csv\")\n",
    "\n",
    "MODEL_OUT_DIR = \"models/sessionDrop\"\n",
    "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
    "def MODEL_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.joblib\")\n",
    "def META_PATH_K(k:int) -> str:\n",
    "    return os.path.join(MODEL_OUT_DIR, f\"sessionDrop_model.k{k}.meta.json\")\n",
    "\n",
    "FEATURE_COLS = [\"timestamp\",\"current_state\",\"page_depth\",\n",
    "                \"delta_search_count\",\"delta_cart_item_count\",\"gap_sec\"]\n",
    "\n",
    "def make_param_dist(seed: int):\n",
    "    r = check_random_state(seed)\n",
    "    return {\n",
    "        \"n_estimators\":      r.randint(600, 1601, size=20),\n",
    "        \"learning_rate\":     r.choice([0.02, 0.03, 0.05, 0.07, 0.1], size=20),\n",
    "        \"max_depth\":         r.choice([3,4,5,6,7], size=20),\n",
    "        \"min_child_weight\":  r.choice([1,2,3,5], size=20),\n",
    "        \"subsample\":         r.uniform(0.6, 1.0, size=20),\n",
    "        \"colsample_bytree\":  r.uniform(0.6, 1.0, size=20),\n",
    "        \"reg_lambda\":        r.choice([0.5,1.0,1.5,2.0], size=20),\n",
    "        \"reg_alpha\":         r.choice([0.0,0.1,0.3,0.5], size=20),\n",
    "    }\n",
    "\n",
    "print(\"[Done]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db08ee-9994-4374-b02f-ee8b1cddf8c1",
   "metadata": {},
   "source": [
    "## 2) ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "- ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²´ ì„¸ì…˜ì˜ ê°œìˆ˜ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347123a-97a8-48d9-88c4-339b1e365600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2. ë°ì´í„° ë¡œë“œ ===\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "required = [\"session_id\",\"timestamp\",\"current_state\",\"next_state\",\"page_depth\",\"search_count\",\"cart_item_count\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}\")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"session_id\",\"timestamp\"]).copy()\n",
    "df[\"session_id\"] = df[\"session_id\"].astype(str)\n",
    "df = df.sort_values([\"session_id\",\"timestamp\"])\n",
    "\n",
    "def norm_state(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\?.*$\",\"\", s)\n",
    "    if len(s)>1 and s.endswith(\"/\"):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "df[\"current_state\"] = df[\"current_state\"].map(norm_state)\n",
    "df[\"next_state\"]    = df[\"next_state\"].map(norm_state)\n",
    "\n",
    "print(\"Loaded rows:\", len(df), \"sessions:\", df[\"session_id\"].nunique())\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349630be-6cdb-4676-8c95-de9a9ef399a5",
   "metadata": {},
   "source": [
    "## 3) ê°„ê²©/ì¦ë¶„ ê³„ì‚°\n",
    "- ì„¸ì…˜ ë‚´ ì´ë²¤íŠ¸ ê°„ ì‹œê°„ì°¨ì™€ ëˆ„ì  ì§€í‘œì˜ ì¦ê°€ë¶„ì„ ë§Œë“¤ì–´ ì´í›„ í”¼ì²˜ ì§‘ê³„ì— ì“°ê¸° ì¢‹ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0f05c-33bd-401a-93b5-4215700d7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3. ê°„ê²©/ì¦ë¶„ ìƒì„± ===\n",
    "df[\"gap_sec\"] = (\n",
    "    df.groupby(\"session_id\")[\"timestamp\"]\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    "      .clip(lower=0)\n",
    ")\n",
    "\n",
    "for col in [\"page_depth\",\"search_count\",\"cart_item_count\"]:\n",
    "    dcol = f\"delta_{col}\"\n",
    "    df[dcol] = df.groupby(\"session_id\")[col].diff()\n",
    "    df[dcol] = df[dcol].fillna(df[col]).clip(lower=0)\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8525c12-f745-40ff-aff1-c3981619e684",
   "metadata": {},
   "source": [
    "## 4) Targit Label ìƒì„± & ëˆ„ìˆ˜ ì„¸ì…˜ ì‹ë³„\n",
    "- ì‹¤ì œ ê²°ê³¼ì¸ Labelì„ ìƒì„±í•˜ê³ , ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì²« í™œë™ë¶€í„° K í™œë™ ë‚´ì— ì‹¤ì œ ê²°ê³¼ ê°’ì´ í¬í•¨ëœ ì„¸ì…˜ì„ ì‹ë³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09409371-2846-4c33-b9d0-55114f77dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4. ì„¸ì…˜ ë¼ë²¨ + prefix-K ë° ëˆ„ìˆ˜ ì„¸ì…˜ ì‹ë³„ ===\n",
    "is_final = df[\"current_state\"].isin(TERMINALS_FINAL) | df[\"next_state\"].isin(TERMINALS_FINAL)\n",
    "labels = (\n",
    "    df.assign(is_final=is_final)\n",
    "      .groupby(\"session_id\", sort=False)[\"is_final\"]\n",
    "      .any()\n",
    "      .astype(\"int8\")\n",
    "      .reset_index(name=\"label\")\n",
    ")\n",
    "\n",
    "prefix_by_k    = {}\n",
    "term_sid_by_k  = {}\n",
    "\n",
    "# ğŸ”§ ëˆ„ìˆ˜ ì²˜ë¦¬ ëª¨ë“œ: 'drop' | 'truncate' | 'mask' | 'mask_pad'\n",
    "LEAK_MODE  = \"mask_pad\"\n",
    "PAD_TOKEN  = \"\"     # íŒ¨ë”©/ë§ˆìŠ¤í‚¹ì— ì“¸ ì¤‘ë¦½ í† í°(ë¹ˆ ë¬¸ìì—´ì´ë©´ ëŒ€ë¶€ë¶„ì˜ í”¼ì²˜ì— ì˜í–¥ ìµœì†Œ)\n",
    "PAD_DEPTH  = None   # None=ì´ì „ê°’ ìœ ì§€(ffill), ìˆ«ì ì£¼ë©´ ê³ ì •ê°’ìœ¼ë¡œ ì„¸íŒ…(ì˜ˆ: 0)\n",
    "\n",
    "def _sanitize_prefix_group(g: pd.DataFrame, K: int) -> pd.DataFrame:\n",
    "    \"\"\"prefix-K ê·¸ë£¹ì—ì„œ í„°ë¯¸ë„ ë“±ì¥ ì´ë²¤íŠ¸ë¥¼ ëˆ„ìˆ˜ ë°©ì§€ ì²˜ë¦¬.\"\"\"\n",
    "    mask = g[\"current_state\"].isin(TERMINALS_FINAL) | g[\"next_state\"].isin(TERMINALS_FINAL)\n",
    "    if not mask.any():\n",
    "        # ëˆ„ìˆ˜ ì—†ìŒ\n",
    "        if LEAK_MODE.endswith(\"pad\") and len(g) < K:\n",
    "            # íŒ¨ë”©ë§Œ í•„ìš”í•˜ë©´ íŒ¨ë”©\n",
    "            last = g.iloc[-1]\n",
    "            while len(g) < K:\n",
    "                pad = {\n",
    "                    \"session_id\": last[\"session_id\"],\n",
    "                    \"timestamp\":  last[\"timestamp\"],      # ì‹œê°„ ì¦ê°€ ì—†ìŒ\n",
    "                    \"current_state\": PAD_TOKEN,\n",
    "                    \"next_state\":    PAD_TOKEN,\n",
    "                    \"page_depth\":    last[\"page_depth\"] if PAD_DEPTH is None else PAD_DEPTH,\n",
    "                    \"delta_search_count\": 0.0,\n",
    "                    \"delta_cart_item_count\": 0.0,\n",
    "                    \"gap_sec\": 0.0,\n",
    "                }\n",
    "                g = pd.concat([g, pd.DataFrame([pad])], ignore_index=True)\n",
    "        return g\n",
    "\n",
    "    if LEAK_MODE == \"drop\":\n",
    "        # í„°ë¯¸ë„ì´ ìˆëŠ” í–‰ ì œê±°(ì„¸ì…˜ì´ ë¹„ë©´ downstreamì—ì„œ ìì—° íƒˆë½)\n",
    "        g2 = g.loc[~mask].copy()\n",
    "        return g2\n",
    "\n",
    "    if LEAK_MODE == \"truncate\":\n",
    "        # ì²« í„°ë¯¸ë„ ë“±ì¥ ì´ì „ê¹Œì§€ë§Œ ì‚¬ìš©\n",
    "        first_idx = mask.idxmax()  # Trueê°€ ì²˜ìŒ ë‚˜ì˜¨ index\n",
    "        g2 = g.loc[g.index < first_idx].copy()\n",
    "        return g2\n",
    "\n",
    "    # === 'mask' / 'mask_pad' ê³µí†µ: í„°ë¯¸ë„ ë“±ì¥ í–‰ì„ 'ì¤‘ë¦½í™”' ===\n",
    "    g2 = g.copy()\n",
    "\n",
    "    # ìƒíƒœ í† í° ë§ˆìŠ¤í‚¹\n",
    "    g2.loc[mask, \"current_state\"] = PAD_TOKEN\n",
    "    g2.loc[mask, \"next_state\"]    = PAD_TOKEN\n",
    "\n",
    "    # ë¸íƒ€/ê°„ê²©ì€ 0ìœ¼ë¡œ(ë™ì‘ ìµœì†Œí™”)\n",
    "    g2.loc[mask, [\"delta_search_count\", \"delta_cart_item_count\", \"gap_sec\"]] = 0.0\n",
    "\n",
    "    # page_depthëŠ” ì´ì „ê°’ ìœ ì§€(ffill) ë˜ëŠ” ê³ ì •ê°’\n",
    "    if PAD_DEPTH is None:\n",
    "        g2[\"page_depth\"] = g2[\"page_depth\"].ffill()\n",
    "        g2.loc[mask & g2[\"page_depth\"].isna(), \"page_depth\"] = 0.0\n",
    "    else:\n",
    "        g2.loc[mask, \"page_depth\"] = float(PAD_DEPTH)\n",
    "\n",
    "    # ì‹œê°„ ì‹ í˜¸ê°€ ìƒˆì–´ ë‚˜ê°€ì§€ ì•Šë„ë¡, ë§ˆìŠ¤í‚¹ëœ í–‰ì˜ timestampëŠ” ì§ì „ê³¼ ë™ì¼í•˜ê²Œ\n",
    "    g2.loc[mask, \"timestamp\"] = g2[\"timestamp\"].shift(1).loc[mask].fillna(g2[\"timestamp\"].iloc[0])\n",
    "\n",
    "    # í•„ìš” ì‹œ Kê¹Œì§€ íŒ¨ë”©\n",
    "    if LEAK_MODE == \"mask_pad\" and len(g2) < K:\n",
    "        last = g2.iloc[-1]\n",
    "        while len(g2) < K:\n",
    "            pad = {\n",
    "                \"session_id\": last[\"session_id\"],\n",
    "                \"timestamp\":  last[\"timestamp\"],\n",
    "                \"current_state\": PAD_TOKEN,\n",
    "                \"next_state\":    PAD_TOKEN,\n",
    "                \"page_depth\":    last[\"page_depth\"] if PAD_DEPTH is None else PAD_DEPTH,\n",
    "                \"delta_search_count\": 0.0,\n",
    "                \"delta_cart_item_count\": 0.0,\n",
    "                \"gap_sec\": 0.0,\n",
    "            }\n",
    "            g2 = pd.concat([g2, pd.DataFrame([pad])], ignore_index=True)\n",
    "\n",
    "    return g2\n",
    "\n",
    "for K in K_LIST:\n",
    "    # ì›ë³¸ prefix-K\n",
    "    prefix_k = (\n",
    "        df.sort_values([\"session_id\",\"timestamp\"])\n",
    "          .groupby(\"session_id\", group_keys=False)\n",
    "          .head(K)\n",
    "    )\n",
    "\n",
    "    # ì°¸ê³ ìš©(í†µê³„ ì¶œë ¥ ìš©ë„ë¡œë§Œ ìœ ì§€)\n",
    "    term_sid_k = prefix_k.loc[\n",
    "        prefix_k[\"current_state\"].isin(TERMINALS_FINAL) | prefix_k[\"next_state\"].isin(TERMINALS_FINAL),\n",
    "        \"session_id\"\n",
    "    ].unique()\n",
    "\n",
    "    # âœ… ëˆ„ìˆ˜ ì²˜ë¦¬ ì ìš©ëœ prefix-K\n",
    "    prefix_clean_k = pd.concat(\n",
    "        (_sanitize_prefix_group(g.copy(), K) for _, g in prefix_k.groupby(\"session_id\", sort=False)),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    prefix_by_k[K]   = prefix_clean_k\n",
    "    term_sid_by_k[K] = term_sid_k  # í†µê³„ ì¶œë ¥ìš© (ê³„ì† ì‚¬ìš©í•´ë„ ë¨)\n",
    "\n",
    "print(f\"[info] total sessions: {df['session_id'].nunique():,}\")\n",
    "for K in K_LIST:\n",
    "    print(f\"[info] K={K}: sessions reaching terminals in first {K} events (detected): {len(term_sid_by_k[K]):,}\")\n",
    "\n",
    "def bigrams(seq):\n",
    "    a, b = tee(seq)\n",
    "    next(b, None)\n",
    "    for x, y in zip(a, b):\n",
    "        yield (x, y)\n",
    "\n",
    "bg_counter = Counter()\n",
    "TERMINALS = TERMINALS_FINAL\n",
    "for _, g in df.groupby(\"session_id\", sort=False):\n",
    "    seq = list(g[\"current_state\"].fillna(\"\").values)\n",
    "    for bg in bigrams(seq):\n",
    "        if (bg[0] in TERMINALS) or (bg[1] in TERMINALS):\n",
    "            continue\n",
    "        bg_counter[bg] += 1\n",
    "\n",
    "top_bigrams = [bg for bg, _ in bg_counter.most_common(TOPK_BIGRAMS)]\n",
    "bigram_to_idx = {bg:i for i,bg in enumerate(top_bigrams)}\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c3a31-b3b0-4784-9355-d7ea120d81ff",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "- ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14c11e-f07e-4fc7-b9ce-c673240f595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5. ì„¸ì…˜ í”¼ì²˜ ì§‘ê³„ í•¨ìˆ˜ ===\n",
    "from IPython.display import display\n",
    "\n",
    "def agg_features(g: pd.DataFrame) -> pd.Series:\n",
    "    g = g.sort_values(\"timestamp\")\n",
    "    states = g[\"current_state\"].fillna(\"\").tolist()\n",
    "    gaps   = g[\"gap_sec\"].values\n",
    "\n",
    "    feats = {}\n",
    "    feats[\"n_events\"] = len(g)\n",
    "    feats[\"session_duration_sec\"] = (\n",
    "        (g[\"timestamp\"].iloc[-1] - g[\"timestamp\"].iloc[0]).total_seconds()\n",
    "        if len(g)>1 else 0.0\n",
    "    )\n",
    "    feats[\"events_per_min\"] = feats[\"n_events\"] / max(feats[\"session_duration_sec\"]/60.0, 1e-9)\n",
    "\n",
    "    if len(gaps)>0:\n",
    "        feats[\"gap_mean\"] = float(np.mean(gaps))\n",
    "        feats[\"gap_std\"]  = float(np.std(gaps))\n",
    "        feats[\"gap_p95\"]  = float(np.quantile(gaps, 0.95))\n",
    "    else:\n",
    "        feats[\"gap_mean\"]=feats[\"gap_std\"]=feats[\"gap_p95\"]=0.0\n",
    "\n",
    "    feats[\"page_depth_max\"]   = float(g[\"page_depth\"].max())\n",
    "    feats[\"page_depth_slope\"] = float((g[\"page_depth\"].iloc[-1] - g[\"page_depth\"].iloc[0]) / max(len(g)-1,1))\n",
    "    feats[\"depth_backtracks\"] = int((g[\"page_depth\"].diff()<0).sum())\n",
    "\n",
    "    feats[\"search_delta_sum\"] = float(g[\"delta_search_count\"].sum())\n",
    "    feats[\"search_rate\"]      = feats[\"search_delta_sum\"] / max(feats[\"n_events\"],1)\n",
    "\n",
    "    cart_delta = g[\"delta_cart_item_count\"].clip(lower=0)\n",
    "    feats[\"cart_adds\"]        = float(cart_delta.sum())\n",
    "    feats[\"cart_touch_count\"] = int((cart_delta>0).sum())\n",
    "    feats[\"cart_adds_per_event\"] = feats[\"cart_adds\"] / max(feats[\"n_events\"],1)\n",
    "    feats[\"search_to_cart_ratio\"] = (feats[\"search_delta_sum\"] + 1.0) / (feats[\"cart_adds\"] + 1.0)\n",
    "\n",
    "    first_ts = g[\"timestamp\"].iloc[0]\n",
    "    feats[\"first_event_hour\"] = int(first_ts.hour)\n",
    "    feats[\"is_weekend\"]       = int(first_ts.weekday()>=5)\n",
    "\n",
    "    def idx_of(pred_list, key):\n",
    "        for i, s in enumerate(pred_list):\n",
    "            if key in s: return i\n",
    "        return None\n",
    "    i_search = idx_of(states, \"/search\")\n",
    "    i_cart   = idx_of(states, \"/cart\")\n",
    "    feats[\"time_to_first_search\"] = float((g[\"timestamp\"].iloc[i_search] - first_ts).total_seconds()) if i_search is not None else -1.0\n",
    "    feats[\"time_to_first_cart\"]   = float((g[\"timestamp\"].iloc[i_cart]   - first_ts).total_seconds()) if i_cart   is not None else -1.0\n",
    "\n",
    "    lastk = states[-5:]\n",
    "    joined_lastk = \" \".join(lastk)\n",
    "    feats[\"last5_has_search\"]   = int(\"search\" in joined_lastk)\n",
    "    feats[\"last5_has_cart\"]     = int(\"cart\" in joined_lastk)\n",
    "    feats[\"last5_has_checkout\"] = int(\"checkout\" in joined_lastk)\n",
    "\n",
    "    bg_counts = Counter(list(bigrams(states)))\n",
    "    for bg, idx in bigram_to_idx.items():\n",
    "        feats[f\"bg_{idx}\"] = int(bg_counts.get(bg, 0))\n",
    "\n",
    "    uniq = pd.Series(states).value_counts(normalize=True)\n",
    "    feats[\"unique_states\"] = int(uniq.size)\n",
    "    feats[\"entropy_state\"] = float(-(uniq*np.log(uniq+1e-12)).sum())\n",
    "\n",
    "    st0 = states[0] if states else \"\"\n",
    "    stL = states[-1] if states else \"\"\n",
    "    for name, st in [(\"start\", st0), (\"last\", stL)]:\n",
    "        feats[f\"{name}_is_root\"]      = int(st == \"/\")\n",
    "        feats[f\"{name}_is_search\"]    = int(\"/search\" in st)\n",
    "        feats[f\"{name}_is_products\"]  = int(\"/products\" in st)\n",
    "        feats[f\"{name}_is_cart\"]      = int(\"/cart\" in st)\n",
    "        feats[f\"{name}_is_checkout\"]  = int(\"/checkout\" in st)\n",
    "\n",
    "    all_states = \" \".join(states)\n",
    "    feats[\"hit_search\"]   = int(\"/search\" in all_states)\n",
    "    feats[\"hit_products\"] = int(\"/products\" in all_states)\n",
    "    feats[\"hit_cart\"]     = int(\"/cart\" in all_states)\n",
    "    feats[\"hit_checkout\"] = int(\"/checkout\" in all_states)\n",
    "\n",
    "    return pd.Series(feats)\n",
    "\n",
    "print(\"[Done]\")\n",
    "\n",
    "# --- (êµì²´) ì „ì²˜ë¦¬ & Feature Engineering ë¯¸ë¦¬ë³´ê¸° ---\n",
    "from IPython.display import display\n",
    "\n",
    "def preview_engineered(\n",
    "    n_sessions=5, K=None, drop_terminals=True, random=False, seed=42,\n",
    "    include_bigrams=False, topn_bg=None\n",
    "):\n",
    "    \"\"\"\n",
    "    K: Noneì´ë©´ ì „ì²´ ì„¸ì…˜ ê¸°ì¤€(ALL), ì •ìˆ˜ë©´ prefix-K ê¸°ì¤€\n",
    "    drop_terminals: Trueë©´ ëˆ„ìˆ˜ ë°©ì§€ìš© terminal í¬í•¨ ì„¸ì…˜ ì œê±°\n",
    "    random: Trueë©´ ë¬´ì‘ìœ„ ìƒ˜í”Œ, Falseë©´ ì•ì—ì„œë¶€í„°\n",
    "    include_bigrams: Trueë©´ bg_* ì»¬ëŸ¼ì„ í¬í•¨, Falseë©´ ìˆ¨ê¹€\n",
    "    topn_bg: ì •ìˆ˜ë¡œ ì£¼ë©´ bg_* ì»¬ëŸ¼ ì¤‘ ì•ì—ì„œ nê°œë§Œ ë…¸ì¶œ(ì˜ˆ: topn_bg=10)\n",
    "    \"\"\"\n",
    "    if K is None:\n",
    "        df_src = df\n",
    "        term_sids = set()\n",
    "    else:\n",
    "        df_src = prefix_by_k[K]\n",
    "        term_sids = set(term_sid_by_k[K]) if drop_terminals else set()\n",
    "\n",
    "    sids_series = df_src[\"session_id\"].drop_duplicates()\n",
    "    if random:\n",
    "        sids = sids_series.sample(n=min(n_sessions, len(sids_series)), random_state=seed).tolist()\n",
    "    else:\n",
    "        sids = sids_series.head(n_sessions).tolist()\n",
    "\n",
    "    # terminal í¬í•¨ ì„¸ì…˜ ì œê±° ì˜µì…˜\n",
    "    sids = [sid for sid in sids if sid not in term_sids]\n",
    "\n",
    "    preview = (\n",
    "        df_src[df_src[\"session_id\"].isin(sids)]\n",
    "          .groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "          .apply(agg_features)\n",
    "          .reset_index()\n",
    "          .merge(labels, on=\"session_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # bigram ì»¬ëŸ¼ ì²˜ë¦¬\n",
    "    bg_cols = [c for c in preview.columns if c.startswith(\"bg_\")]\n",
    "    if not include_bigrams:\n",
    "        # ì „ë¶€ ìˆ¨ê¹€\n",
    "        preview = preview.drop(columns=bg_cols, errors=\"ignore\")\n",
    "    elif isinstance(topn_bg, int) and topn_bg >= 0:\n",
    "        def _bg_idx(col): \n",
    "            try: return int(col.split(\"_\", 1)[1])\n",
    "            except: return 1_000_000\n",
    "        keep_bg = [c for c in sorted(bg_cols, key=_bg_idx)[:topn_bg]]\n",
    "        drop_bg = [c for c in bg_cols if c not in keep_bg]\n",
    "        preview = preview.drop(columns=drop_bg, errors=\"ignore\")\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì»¬ëŸ¼ ì •ë ¬\n",
    "    key_cols = [\"session_id\", \"label\", \"n_events\", \"session_duration_sec\", \"events_per_min\",\n",
    "                \"gap_mean\", \"gap_std\", \"gap_p95\",\n",
    "                \"page_depth_max\", \"page_depth_slope\", \"depth_backtracks\",\n",
    "                \"search_delta_sum\", \"search_rate\",\n",
    "                \"cart_adds\", \"cart_touch_count\", \"cart_adds_per_event\", \"search_to_cart_ratio\",\n",
    "                \"first_event_hour\", \"is_weekend\",\n",
    "                \"time_to_first_search\", \"time_to_first_cart\",\n",
    "                \"last5_has_search\", \"last5_has_cart\", \"last5_has_checkout\",\n",
    "                \"unique_states\", \"entropy_state\",\n",
    "                \"start_is_root\", \"start_is_search\", \"start_is_products\", \"start_is_cart\", \"start_is_checkout\",\n",
    "                \"last_is_root\", \"last_is_search\", \"last_is_products\", \"last_is_cart\", \"last_is_checkout\",\n",
    "                \"hit_search\", \"hit_products\", \"hit_cart\", \"hit_checkout\"]\n",
    "    ordered = [c for c in key_cols if c in preview.columns] + [c for c in preview.columns if c not in key_cols]\n",
    "    preview = preview[ordered]\n",
    "\n",
    "    title = \"ALL\" if K is None else f\"prefix-K={K}\"\n",
    "    bg_count_total = sum(1 for c in preview.columns if c.startswith(\"bg_\"))\n",
    "    print(f\"[Preview] Engineered features ({title}) â€” {len(preview)} rows\")\n",
    "    display(preview)\n",
    "    print(f\"  - Total columns shown: {len(preview.columns)} (bigrams shown: {bg_count_total})\")\n",
    "    return preview\n",
    "\n",
    "\n",
    "_ = preview_engineered(n_sessions=5, K=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6c2bc-e0cf-4452-bb8a-13d310682ca1",
   "metadata": {},
   "source": [
    "## 6) ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥\n",
    "- Feature Engineering í•œ ë°ì´í„°ë¥¼ Kë³„ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79ba5e-6fc4-4cba-83a6-dac260622fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6. ì„¸ì…˜ í”¼ì²˜ í…Œì´ë¸” ìƒì„± & ì €ì¥ ===\n",
    "X_all = (\n",
    "    df.groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "      .apply(agg_features)\n",
    "      .reset_index()\n",
    ")\n",
    "dataset_all = X_all.merge(labels, on=\"session_id\", how=\"left\")\n",
    "dataset_all.to_csv(OUT_ALL, index=False)\n",
    "\n",
    "datasets_by_k = {}\n",
    "print(\"Saved CSVs:\")\n",
    "print(\" -\", OUT_ALL)\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_prefix_k = (\n",
    "        prefix_by_k[K].groupby(\"session_id\", sort=False)[FEATURE_COLS]\n",
    "                      .apply(agg_features)\n",
    "                      .reset_index()\n",
    "    )\n",
    "    dataset_prefix_k = X_prefix_k.merge(labels, on=\"session_id\", how=\"left\")\n",
    "    dataset_prefix_filt_k = dataset_prefix_k.reset_index(drop=True)\n",
    "    datasets_by_k[K] = dataset_prefix_filt_k\n",
    "    out_path_k = OUT_PREFIXF_PATH(K)\n",
    "    dataset_prefix_filt_k.to_csv(out_path_k, index=False)\n",
    "    print(\" -\", out_path_k, \"| shape:\", tuple(dataset_prefix_filt_k.shape))\n",
    "    \n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0656042-6faa-4caf-a7fd-29fbf36cd4ff",
   "metadata": {},
   "source": [
    "## 7) í•™ìŠµìš© / ê²€ì¦ìš© ë°ì´í„°ì…‹ ë¶„í• \n",
    "- ë°ì´í„°ë¥¼ í•™ìŠµìš© / ê²€ì¦ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c929dc3-d7bf-49b6-a4d4-b1dc18ca4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7. í•™ìŠµ/ê²€ì¦ ë¶„í•  (Kë³„) ===\n",
    "splits_by_k = {}\n",
    "for K in K_LIST:\n",
    "    dfp = datasets_by_k[K].copy()\n",
    "    y = dfp[\"label\"].astype(int)\n",
    "    leak_cols = [c for c in dfp.columns if (\"logout\" in c.lower()) or (\"delete\" in c.lower())]\n",
    "    X = dfp.drop(columns=[\"session_id\",\"label\"] + leak_cols, errors=\"ignore\").fillna(0.0)\n",
    "\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE)\n",
    "    splits_by_k[K] = (X_tr, X_va, y_tr, y_va, X.columns.tolist())\n",
    "\n",
    "    print(f\"[K={K}] Shape: {X.shape}  PosRate: {float(y.mean()):.4f}\")\n",
    "    print(f\"[K={K}] Train: {X_tr.shape}  Valid: {X_va.shape}\")\n",
    "\n",
    "print(\"[Done]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acc53f-496b-42cd-8122-84eb06647347",
   "metadata": {},
   "source": [
    "## 8) ëª¨ë¸ ì„ íƒ & í•™ìŠµ\n",
    "- ëª¨ë¸ì„ ì„ íƒí•œ í›„ í•™ìŠµìš© ë°ì´í„° ì…‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89db6-9054-4cd1-9e97-813cc5dc8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8. ëª¨ë¸ ì„ íƒ & í•™ìŠµ  ===\n",
    "use_xgb = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    use_xgb = True\n",
    "except Exception as e:\n",
    "    print(\"[warn] xgboost ë¶ˆê°€, LogisticRegressionìœ¼ë¡œ í´ë°±:\", e)\n",
    "\n",
    "# í•™ìŠµ ë¡œê·¸ ì˜µì…˜\n",
    "SHOW_XGB_LOG = False   # Trueë©´ ë¶€ìŠ¤íŒ… ë¼ìš´ë“œë³„ metricì„ ì¶œë ¥(ë§ì´ ë‚˜ì˜µë‹ˆë‹¤!)\n",
    "USE_TQDM     = True    # Trueë©´ tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì‚¬ìš©\n",
    "\n",
    "# tqdm ì•ˆì „ ì„í¬íŠ¸\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "# ---------- ê³ ì • ëª¨ë¸ ë¹Œë” ----------\n",
    "def _fixed_xgb(sampler_enabled: bool, y_tr, seed=RANDOM_STATE):\n",
    "    \"\"\"ê³ ì • XGB ë¶„ë¥˜ê¸°(ì‹œë“œ/epoch ë°˜ì˜).\"\"\"\n",
    "    pos = int(y_tr.sum()); neg = int(len(y_tr) - pos)\n",
    "    spw = 1.0 if sampler_enabled else (neg / max(pos, 1))\n",
    "    return XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        # eval_metricì€ fit kwargsê°€ ì•„ë‹Œ set_paramsì—ì„œ ì»¤ìŠ¤í…€ìœ¼ë¡œ ë®ìŠµë‹ˆë‹¤(ì¡°ê¸°ì¢…ë£Œ ì§€í‘œ)\n",
    "        n_estimators=XGB_EPOCHS,          # â† epochì²˜ëŸ¼ ì œì–´\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.80,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.1,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "        scale_pos_weight=spw,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "def _fixed_logreg(sampler_enabled: bool, seed=RANDOM_STATE):\n",
    "    \"\"\"ê³ ì • ë¡œì§€ìŠ¤í‹±(ìœ ì‚¬-epoch: max_iter ë°˜ì˜).\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    return LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=LR_MAX_ITER,             # â† ë°˜ë³µ ìƒí•œ\n",
    "        class_weight=None if sampler_enabled else \"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "# ---------- ì»¤ìŠ¤í…€ í‰ê°€í•¨ìˆ˜(feval) & XGB í•™ìŠµ í—¬í¼ ----------\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "_FEVAL_NUM_THRESHOLDS = 61  # ì†ë„/ì •í™•ë„ íƒ€í˜‘: 31~101 ê¶Œì¥\n",
    "\n",
    "def _feval_macro_f1_balanced(y_true, y_score, sample_weight=None):\n",
    "    \"\"\"sklearn ë˜í¼ í˜¸í™˜: (y_true, y_score[, sample_weight]) -> float\"\"\"\n",
    "    ths = np.linspace(0.01, 0.99, _FEVAL_NUM_THRESHOLDS, dtype=float)\n",
    "    best = 0.0\n",
    "    for t in ths:\n",
    "        y_hat = (y_score >= t).astype(np.int32)\n",
    "        try:\n",
    "            f1 = f1_score(y_true, y_hat, average=\"macro\", sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            f1 = 0.0\n",
    "        if f1 > best:\n",
    "            best = f1\n",
    "    return float(best)\n",
    "\n",
    "def safe_fit_xgb(model, Xtr, ytr, Xva, yva):\n",
    "    \"\"\"Pipeline ë˜ëŠ” ë‹¨ì¼ ëª¨ë¸ ëª¨ë‘ ì§€ì›. XGBClassifierì— eval_set/feval/early_stopping ì „ë‹¬.\n",
    "       - eval_metric: ëª¨ë¸ íŒŒë¼ë¯¸í„°(set_params)ë¡œ ì£¼ì…\n",
    "       - early_stopping_rounds: fit kwargs\n",
    "       - verbose: SHOW_XGB_LOGì— ë”°ë¼ on/off\n",
    "    \"\"\"\n",
    "    is_pipe = hasattr(model, \"named_steps\") and (\"clf\" in getattr(model, \"named_steps\", {}))\n",
    "    clf = model.named_steps[\"clf\"] if is_pipe else model\n",
    "    prefix = \"clf__\" if is_pipe else \"\"\n",
    "\n",
    "    # (ì¤‘ìš”) ì»¤ìŠ¤í…€ fevalì€ fit kwargsê°€ ì•„ë‹ˆë¼ 'ëª¨ë¸ íŒŒë¼ë¯¸í„°'ë¡œ ì„¤ì •\n",
    "    if is_pipe:\n",
    "        model.set_params(**{f\"{prefix}eval_metric\": _feval_macro_f1_balanced})\n",
    "    else:\n",
    "        clf.set_params(eval_metric=_feval_macro_f1_balanced)\n",
    "\n",
    "    # fit kwargs êµ¬ì„±\n",
    "    fit_kwargs = {}\n",
    "    sig = inspect.signature(clf.fit)\n",
    "\n",
    "    # ê²€ì¦ ì„¸íŠ¸\n",
    "    if \"eval_set\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}eval_set\"] = [(Xva, yva)]\n",
    "\n",
    "    # ì¡°ê¸°ì¢…ë£Œ (ë²„ì „ í˜¸í™˜: callbacks ë¯¸ì‚¬ìš©)\n",
    "    if \"early_stopping_rounds\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}early_stopping_rounds\"] = EARLY_STOPPING_ROUNDS\n",
    "\n",
    "    # ë¼ìš´ë“œ ë¡œê·¸ ì¶œë ¥\n",
    "    if \"verbose\" in sig.parameters:\n",
    "        fit_kwargs[f\"{prefix}verbose\"] = bool(SHOW_XGB_LOG)\n",
    "\n",
    "    # í•™ìŠµ\n",
    "    t0 = time.time()\n",
    "    model.fit(Xtr, ytr, **fit_kwargs)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    # ë² ìŠ¤íŠ¸ ì´í„°ë ˆì´ì…˜/ì ìˆ˜ ë¡œê¹…(ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    best_iter  = getattr(est, \"best_iteration\", None)\n",
    "    best_score = getattr(est, \"best_score\", None)\n",
    "    if best_iter is None:\n",
    "        best_iter = getattr(est, \"best_ntree_limit\", None)\n",
    "\n",
    "    print(f\"    â†³ fit done in {dt:.2f}s | best_iter={best_iter} | best_score={best_score}\")\n",
    "    return model\n",
    "\n",
    "# ---------- ì‹¤ì œ í•™ìŠµ: í›„ë³´ ëª¨ë¸ë“¤ì„ í›ˆë ¨ë§Œ ìˆ˜í–‰ ----------\n",
    "best_by_k = {}                 # íŒŒë¼ë¯¸í„° ìš”ì•½(ë©”íƒ€ ì €ì¥ìš©)\n",
    "use_pipeline = _imb_ok         # sampler ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ íŒŒì´í”„ë¼ì¸\n",
    "trained_candidates_by_k = {}   # â† Kë³„ í•™ìŠµëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸(í›„ë³´êµ°). ì„ íƒ/í‰ê°€ëŠ” Cell 9ì—ì„œ.\n",
    "\n",
    "SEP = \"-\" * 86  # â† K êµ¬ë¶„ì„ \n",
    "\n",
    "_k_iter = tqdm(K_LIST, desc=\"K sweep (train)\") if USE_TQDM else K_LIST\n",
    "\n",
    "for K in _k_iter:\n",
    "    print(f\"\\n{SEP}\\n[TRAIN START] K={K}\\n{SEP}\")\n",
    "\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "\n",
    "    cfg_k = _merge_oversampling_cfg(OVERSAMPLING, OVERSAMPLING_BY_K.get(K, {}))\n",
    "    sampler = _build_sampler(cfg_k)\n",
    "    sampler_enabled = sampler is not None\n",
    "\n",
    "    # íŒŒë¼ë¯¸í„° ìš”ì•½(ëŒ€í‘œ ì‹œë“œë¡œ 1íšŒ ìƒì„±)\n",
    "    base_for_summary = _fixed_xgb(sampler_enabled, y_tr, seed=RANDOM_STATE) if use_xgb else _fixed_logreg(sampler_enabled, seed=RANDOM_STATE)\n",
    "    clf_params = base_for_summary.get_params(deep=False)\n",
    "    summary_keys = (\n",
    "        [\"n_estimators\", \"learning_rate\", \"max_depth\", \"min_child_weight\",\n",
    "         \"subsample\", \"colsample_bytree\", \"reg_lambda\", \"reg_alpha\", \"random_state\"]\n",
    "        if use_xgb else\n",
    "        [\"penalty\", \"C\", \"class_weight\", \"solver\", \"max_iter\", \"random_state\"]\n",
    "    )\n",
    "    used_params = {k: clf_params.get(k) for k in summary_keys if k in clf_params}\n",
    "    best_by_k[K] = (None, used_params)\n",
    "    print(f\"[K={K}] Using {'XGBClassifier' if use_xgb else 'LogisticRegression'} Params: {used_params}\")\n",
    "\n",
    "    # í›„ë³´ ëª¨ë¸ í•™ìŠµ\n",
    "    models = []\n",
    "    _seed_iter = tqdm(SEED_SEQ, desc=f\"K={K} train seeds\", leave=False) if USE_TQDM else SEED_SEQ\n",
    "    for seed in _seed_iter:\n",
    "        if use_xgb:\n",
    "            base_clf = _fixed_xgb(sampler_enabled, y_tr, seed=seed)\n",
    "            model_name = \"XGBClassifier\"\n",
    "        else:\n",
    "            base_clf = _fixed_logreg(sampler_enabled, seed=seed)\n",
    "            model_name = \"LogisticRegression\"\n",
    "\n",
    "        if use_pipeline:\n",
    "            steps = []\n",
    "            if sampler_enabled:\n",
    "                steps.append((\"sampler\", sampler))\n",
    "            steps.append((\"clf\", base_clf))\n",
    "            estimator = ImbPipeline(steps)\n",
    "        else:\n",
    "            estimator = base_clf\n",
    "\n",
    "        print(f\"[K={K}] [Seed={seed}] model={model_name} | sampler={'ON' if sampler_enabled else 'OFF'} \"\n",
    "              f\"| n_est={getattr(base_clf, 'n_estimators', 'NA')} | max_iter={getattr(base_clf, 'max_iter', 'NA')}\")\n",
    "\n",
    "        if use_xgb:\n",
    "            estimator = safe_fit_xgb(estimator, X_tr, y_tr, X_va, y_va)\n",
    "        else:\n",
    "            t0 = time.time()\n",
    "            estimator.fit(X_tr, y_tr)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"    â†³ fit done in {dt:.2f}s\")\n",
    "    \n",
    "        models.append(estimator)\n",
    "    \n",
    "        trained_candidates_by_k[K] = models\n",
    "        print(f\"[TRAIN END]   K={K} | candidates={len(models)}\")\n",
    "        print(SEP)\n",
    "\n",
    "print(\"[Done] íŠ¸ë ˆì´ë‹ ì™„ë£Œ â€” í›„ë³´ ëª¨ë¸ë“¤ì€ trained_candidates_by_k[K]ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c1596-71dd-4a52-b8df-66df0442493a",
   "metadata": {},
   "source": [
    "## 9) ëª¨ë¸ ê²€ì¦ & í‰ê°€\n",
    "- ì„ íƒí•œ ëª¨ë¸ì„ ê²€ì¦ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ì¦í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb82746-a643-4eff-9806-653b1ec8b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9. ê²€ì¦ & í‰ê°€  ===\n",
    "\n",
    "# ê²€ì¦/í‰ê°€ ìœ í‹¸\n",
    "def balanced_score(y_true, p, t):\n",
    "    y_pred = (p >= t).astype(int)\n",
    "    return 0.5*(accuracy_score(y_true, y_pred) + f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "SHOW_05 = False\n",
    "\n",
    "trained_models_by_k = {}   # â† ìµœì¢… ì„ ì •ëœ ë‹¨ì¼ ëª¨ë¸(Kë³„)\n",
    "eval_by_k = {}             # â† í‰ê°€ ìš”ì•½(ë©”íƒ€/ë¦¬í¬íŒ…ìš©)\n",
    "T_OPER_BY_K = {}           # â† ìš´ì˜ ì„ê³„ê°’(Kë³„)\n",
    "summary_rows = []\n",
    "SEP = \"-\" * 86\n",
    "\n",
    "for K in K_LIST:\n",
    "    X_tr, X_va, y_tr, y_va, feat_names = splits_by_k[K]\n",
    "    models = trained_candidates_by_k[K]  # Cell 8ì—ì„œ í›ˆë ¨ëœ í›„ë³´êµ°\n",
    "\n",
    "    # ê° í›„ë³´ ëª¨ë¸ì˜ ê²€ì¦ í™•ë¥ \n",
    "    p_list = [m.predict_proba(X_va)[:, 1] for m in models]\n",
    "\n",
    "    # â”€â”€ ì„ê³„ê°’ íƒìƒ‰ì€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ì•™ìƒë¸” í‰ê·  í™•ë¥  ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰ â”€â”€\n",
    "    p_va_mean = np.mean(np.vstack(p_list), axis=0)\n",
    "    ths = np.linspace(0.01, 0.99, 197)\n",
    "    t_best_ensemble, s_best_ensemble = max(\n",
    "        ((t, balanced_score(y_va, p_va_mean, t)) for t in ths),\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "\n",
    "    # ìš´ì˜ ì„ê³„ê°’ ê²°ì •(override > balanced > default)\n",
    "    if isinstance(OPER_T_OVERRIDE, dict) and (K in OPER_T_OVERRIDE):\n",
    "        op_t = float(OPER_T_OVERRIDE[K])\n",
    "    elif OPER_POLICY == \"balanced\":\n",
    "        op_t = float(t_best_ensemble)\n",
    "    else:\n",
    "        op_t = float(T_OPER_DEFAULT)\n",
    "    T_OPER_BY_K[K] = op_t\n",
    "\n",
    "    # â”€â”€ ìµœì¢… ë‹¨ì¼ ëª¨ë¸ ì„ ì •: ë™ì¼ op_tì—ì„œ Macro-F1ì´ ê°€ì¥ ë†’ì€ ëª¨ë¸ â”€â”€\n",
    "    single_scores_f1 = [\n",
    "        f1_score(y_va, (p_list[i] >= op_t).astype(int), average=\"macro\")\n",
    "        for i in range(len(models))\n",
    "    ]\n",
    "    best_idx = int(np.argmax(single_scores_f1))\n",
    "    final_model = models[best_idx]\n",
    "    p_va_single = p_list[best_idx]\n",
    "\n",
    "    # â”€â”€ ì¶œë ¥/ìš”ì•½ì€ \"ìµœì¢… ë‹¨ì¼ ëª¨ë¸\" ê¸°ì¤€ â”€â”€\n",
    "    yhat_oper_single = (p_va_single >= op_t).astype(int)\n",
    "    acc_oper_single  = accuracy_score(y_va, yhat_oper_single)\n",
    "    f1_oper_single   = f1_score(y_va, yhat_oper_single, average=\"macro\")\n",
    "\n",
    "    if SHOW_05:\n",
    "        yhat05 = (p_va_single >= 0.5).astype(int)\n",
    "        print(f\"[K={K}] [Valid(single) @0.50] Acc={accuracy_score(y_va, yhat05):.4f} \"\n",
    "              f\"Macro-F1={f1_score(y_va, yhat05, average='macro'):.4f} \"\n",
    "              f\"PR-AUC={average_precision_score(y_va, p_va_single):.4f}\")\n",
    "        print(confusion_matrix(y_va, yhat05))\n",
    "        print(classification_report(y_va, yhat05, digits=4))\n",
    "\n",
    "    print(f\"[K={K}] [Valid(single) @t={op_t:.3f}] Acc={acc_oper_single:.4f} Macro-F1={f1_oper_single:.4f}\")\n",
    "    print(confusion_matrix(y_va, yhat_oper_single))\n",
    "    print(classification_report(y_va, yhat_oper_single, digits=4))\n",
    "    print(SEP, \"\\n\")\n",
    "\n",
    "    # ì €ì¥/ë©”íƒ€ìš© ê²°ê³¼ ê¸°ë¡ (ë‹¨ì¼ ëª¨ë¸ ê¸°ì¤€ ì„±ëŠ¥ì„ ê¸°ë¡)\n",
    "    trained_models_by_k[K] = final_model\n",
    "    eval_by_k[K] = {\n",
    "        # ì¶œë ¥ê³¼ ë™ì¼í•˜ê²Œ ë‹¨ì¼ ëª¨ë¸ ê¸°ì¤€ ì„±ëŠ¥ì„ ì €ì¥\n",
    "        \"Valid@Oper.Acc\": float(acc_oper_single),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper_single),\n",
    "\n",
    "        # ì°¸ê³ ìš©: ì„ê³„ê°’ ì„ íƒì€ ì•™ìƒë¸” ê¸°ë°˜ìœ¼ë¡œ í–ˆìœ¼ë¯€ë¡œ ë³„ë„ ë³´ì¡´\n",
    "        \"Balanced.t\": float(t_best_ensemble),\n",
    "        \"Balanced.Score\": float(s_best_ensemble),\n",
    "\n",
    "        \"Oper.t\": float(op_t),\n",
    "        \"features\": feat_names,\n",
    "\n",
    "        # (ì‹ ê·œ) ì¬í•™ìŠµ ì •ë³´\n",
    "        \"n_restarts\": int(N_RESTARTS),\n",
    "        \"seeds\": list(SEED_SEQ),\n",
    "        \"single_best_f1_at_oper\": float(single_scores_f1[best_idx]),\n",
    "        \"selected_model_index\": int(best_idx),\n",
    "    }\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"K\": K,\n",
    "        \"Oper.t\": float(op_t),\n",
    "        # ìš”ì•½ë„ ë‹¨ì¼ ëª¨ë¸ ê¸°ì¤€ ê°’ìœ¼ë¡œ ì¶œë ¥\n",
    "        \"Valid@Oper.Acc\": float(acc_oper_single),\n",
    "        \"Valid@Oper.MacroF1\": float(f1_oper_single),\n",
    "\n",
    "        # ì°¸ê³ ìš©(ì•™ìƒë¸” ê¸°ë°˜ íƒìƒ‰ ê²°ê³¼)\n",
    "        \"Balanced.t\": float(t_best_ensemble),\n",
    "        \"Balanced.Score\": float(s_best_ensemble),\n",
    "        \"Restarts\": int(N_RESTARTS),\n",
    "    })\n",
    "\n",
    "print(\"=== Summary (Kë³„, Single Final Model on Valid) ===\")\n",
    "for r in sorted(summary_rows, key=lambda x: x[\"K\"]):\n",
    "    print(f\"K={r['K']} | Oper.t={r['Oper.t']:.3f} | Acc@Oper={r['Valid@Oper.Acc']:.4f} \"\n",
    "          f\"| MacroF1@Oper={r['Valid@Oper.MacroF1']:.4f} | t_bal(ens)={r['Balanced.t']:.3f} \"\n",
    "          f\"| Score_bal(ens)={r['Balanced.Score']:.4f} | Restarts={r['Restarts']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906b71-b29f-409f-95a8-2c6ed4ea0664",
   "metadata": {},
   "source": [
    "## 10) ëª¨ë¸ ì €ì¥\n",
    "- Kë³„ë¡œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d498f73-a6fd-4760-a2a1-4eb322d1da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10. ëª¨ë¸ & ë©”íƒ€ ì €ì¥ (Kë³„) ===\n",
    "\n",
    "def to_builtin(v):\n",
    "    if isinstance(v, (np.integer,)):  return int(v)\n",
    "    if isinstance(v, (np.floating,)): return float(v)\n",
    "    if isinstance(v, (np.bool_,)):    return bool(v)\n",
    "    return v\n",
    "\n",
    "def _jsonable_sampling_strategy(val):\n",
    "    \"\"\"meta JSONì— ê¸°ë¡í•  sampling_strategy ì§ë ¬í™” í—¬í¼.\"\"\"\n",
    "    if callable(val):\n",
    "        return f\"<callable:{getattr(val, '__name__', 'anonymous')}>\"\n",
    "    if isinstance(val, (np.integer, int, np.floating, float)):\n",
    "        return float(val)\n",
    "    if isinstance(val, (np.bool_, bool)):\n",
    "        return bool(val)\n",
    "    if isinstance(val, dict):\n",
    "        return {str(k): to_builtin(v) for k, v in val.items()}\n",
    "    return None if val is None else str(val)  # \"auto\"/\"minority\"/\"not majority\"/\"all\" ë“±\n",
    "\n",
    "def _extract_selected_model_info(model):\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸/ë‹¨ì¼ ëª¨ë¸ ê³µí†µìœ¼ë¡œ ìµœì¢… ì„ ì • ëª¨ë¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œ.\"\"\"\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    info = {\n",
    "        \"estimator\": est.__class__.__name__,\n",
    "        \"random_state\": to_builtin(getattr(est, \"random_state\", None)),\n",
    "        \"n_estimators\": to_builtin(getattr(est, \"n_estimators\", None)),\n",
    "        \"max_iter\": to_builtin(getattr(est, \"max_iter\", None)),\n",
    "        \"best_iteration\": to_builtin(getattr(est, \"best_iteration\", getattr(est, \"best_ntree_limit\", None))),\n",
    "        \"best_score\": to_builtin(getattr(est, \"best_score\", None)),\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def _threshold_policy_for_k(K):\n",
    "    \"\"\"Cell 9ì˜ ì •ì±…ì„ ë©”íƒ€ë¡œ ëª…ì‹œ.\"\"\"\n",
    "    if isinstance(OPER_T_OVERRIDE, dict) and (K in OPER_T_OVERRIDE):\n",
    "        return {\"policy\": \"override\", \"value\": float(OPER_T_OVERRIDE[K])}\n",
    "    elif OPER_POLICY == \"balanced\":\n",
    "        return {\"policy\": \"balanced\", \"value\": float(eval_by_k[K][\"Balanced.t\"])}\n",
    "    else:\n",
    "        return {\"policy\": \"default\", \"value\": float(T_OPER_DEFAULT)}\n",
    "\n",
    "def _prepare_for_dump(model):\n",
    "    \"\"\"XGBì˜ callable eval_metricì´ ìˆìœ¼ë©´ ë¬¸ìì—´ë¡œ êµì²´í•´ í”¼í´ë§ ê°€ëŠ¥í•˜ê²Œ ë§Œë“ ë‹¤.\"\"\"\n",
    "    try:\n",
    "        if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps:\n",
    "            est = model.named_steps[\"clf\"]\n",
    "            if hasattr(est, \"eval_metric\") and callable(getattr(est, \"eval_metric\", None)):\n",
    "                # binary:logistic ê¸°ì¤€ìœ¼ë¡œ ì•ˆì „í•œ ë¬¸ìì—´ ë©”íŠ¸ë¦­ìœ¼ë¡œ êµì²´\n",
    "                model.set_params(**{\"clf__eval_metric\": \"logloss\"})\n",
    "        else:\n",
    "            if hasattr(model, \"eval_metric\") and callable(getattr(model, \"eval_metric\", None)):\n",
    "                model.set_params(eval_metric=\"logloss\")\n",
    "    except Exception:\n",
    "        # ì‹¤íŒ¨í•´ë„ ì €ì¥ì„ ì‹œë„í•˜ê²Œë” ì¡°ìš©íˆ í†µê³¼\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "print(\"Saving models & meta...\")\n",
    "for K in K_LIST:\n",
    "    # ìµœì¢… ì„ ì •ëœ ë‹¨ì¼ ëª¨ë¸ê³¼ í‰ê°€ ê²°ê³¼\n",
    "    model = trained_models_by_k[K]\n",
    "    feat_names = eval_by_k[K][\"features\"]\n",
    "    oper_t = float(eval_by_k[K][\"Oper.t\"])\n",
    "\n",
    "    # (ìš”ì•½ìš©) íŒŒë¼ë¯¸í„° ìŠ¤ëƒ…ìƒ· â€” Cell 8ì—ì„œ ë§Œë“  used_params ì¬í™œìš©\n",
    "    best_params_ = best_by_k.get(K, ({}, {}))[1] if isinstance(best_by_k.get(K), tuple) else {}\n",
    "    best_params_clean = {k: to_builtin(v) for k, v in (best_params_ or {}).items()}\n",
    "\n",
    "    # ì‹¤ì œ ì‚¬ìš©ëœ(ì „ì—­ + Kë³„ override) ì˜¤ë²„ìƒ˜í”Œë§ ì„¤ì •ì„ ê¸°ë¡\n",
    "    eff_os_cfg = _merge_oversampling_cfg(OVERSAMPLING, OVERSAMPLING_BY_K.get(K, {}))\n",
    "    oversampling_enabled = bool(eff_os_cfg.get(\"enable\", False) and _imb_ok)\n",
    "\n",
    "    # (í•µì‹¬) ë¤í”„ ì „ì— eval_metricì˜ callable ì œê±°\n",
    "    model_for_dump = _prepare_for_dump(model)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model_path = MODEL_PATH_K(K)\n",
    "    dump(model_for_dump, model_path)\n",
    "\n",
    "    # ì„ê³„ê°’ ì •ì±…/ì„ ì • ëª¨ë¸ ì •ë³´\n",
    "    th_policy = _threshold_policy_for_k(K)\n",
    "    selected_info = _extract_selected_model_info(model)\n",
    "\n",
    "    # ë©”íƒ€ JSON êµ¬ì„±\n",
    "    meta = {\n",
    "        \"variant\": (\n",
    "            f\"Multi-seed (prefix{K}); leakage-safe; MacroF1-earlystop(feval during train); \"\n",
    "            f\"threshold=balanced_on_ensemble(mean)\"\n",
    "        ),\n",
    "        \"features\": [str(c) for c in feat_names],\n",
    "        \"threshold\": oper_t,\n",
    "        \"threshold_policy\": th_policy,  # override/balanced/default + ê°’\n",
    "        \"best_params\": best_params_clean,  # ìš”ì•½(ëŒ€í‘œ ì‹œë“œ ê¸°ì¤€)\n",
    "        \"random_state\": int(RANDOM_STATE),\n",
    "\n",
    "        \"preprocess\": {\n",
    "            \"PREFIX_K\": int(K),\n",
    "            \"TOPK_BIGRAMS\": int(TOPK_BIGRAMS),\n",
    "            \"terminals\": sorted(list(TERMINALS_FINAL)),\n",
    "            \"note\": \"prefix-K ë‚´ terminal ë“±ì¥ ì„¸ì…˜ ì œì™¸, terminal ì „ì´ ì œì™¸ bigram ì‚¬ì „\",\n",
    "        },\n",
    "\n",
    "        \"oversampling\": {\n",
    "            \"enabled\": oversampling_enabled,\n",
    "            \"method\": eff_os_cfg.get(\"method\"),\n",
    "            \"ratio\": _jsonable_sampling_strategy(eff_os_cfg.get(\"ratio\", \"auto\")),\n",
    "            \"k_neighbors\": int(eff_os_cfg.get(\"k_neighbors\", 5)),\n",
    "        },\n",
    "\n",
    "        \"artifacts_csv\": {\n",
    "            \"all\": OUT_ALL,\n",
    "            \"prefix_filtered\": OUT_PREFIXF_PATH(K),\n",
    "        },\n",
    "\n",
    "        \"validation\": {\n",
    "            \"Acc@Oper\": float(eval_by_k[K][\"Valid@Oper.Acc\"]),\n",
    "            \"MacroF1@Oper\": float(eval_by_k[K][\"Valid@Oper.MacroF1\"]),\n",
    "            \"Balanced_t\": float(eval_by_k[K][\"Balanced.t\"]),\n",
    "            \"Balanced_Score\": float(eval_by_k[K][\"Balanced.Score\"]),\n",
    "            \"single_best_f1_at_oper\": float(eval_by_k[K].get(\"single_best_f1_at_oper\", np.nan)),\n",
    "        },\n",
    "\n",
    "        # (ì‹ ê·œ) í•™ìŠµ/ì•™ìƒë¸” ì •ë³´\n",
    "        \"training\": {\n",
    "            \"n_restarts\": int(eval_by_k[K].get(\"n_restarts\", 1)),\n",
    "            \"seeds\": list(eval_by_k[K].get(\"seeds\", [RANDOM_STATE])),\n",
    "            \"candidate_count\": len(eval_by_k[K].get(\"seeds\", [RANDOM_STATE])),\n",
    "            \"ensemble\": {\"method\": \"mean\", \"threshold_search_grid\": 197},\n",
    "            \"feval\": {\"name\": \"macro_f1_balanced\", \"num_thresholds\": int(globals().get(\"_FEVAL_NUM_THRESHOLDS\", 61))},\n",
    "            \"early_stopping_rounds\": int(EARLY_STOPPING_ROUNDS) if use_xgb else None,\n",
    "            \"xgb_epochs\": int(XGB_EPOCHS) if use_xgb else None,\n",
    "            \"lr_max_iter\": int(LR_MAX_ITER) if not use_xgb else None,\n",
    "            \"selected_model\": selected_info,  # ìµœì¢… ì €ì¥ëœ ë‹¨ì¼ ëª¨ë¸ ì •ë³´\n",
    "        },\n",
    "    }\n",
    "\n",
    "    meta_path = META_PATH_K(K)\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\" - Saved model: {model_path}\")\n",
    "    print(f\" - Saved meta : {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684c8d3-ff7b-4b85-a4c0-071c0ced0f9e",
   "metadata": {},
   "source": [
    "## 11) ì„¸ì…˜ ì´íƒˆ ì¶”ë¡  ê²°ê³¼ ê°’ ì‹œê°í™”\n",
    "- ê²€ì¦í•œ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼ ê°’ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7f739-0340-4eb0-827d-d0cca31ee127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11. ì‹œê°í™” (Kë³„ ìµœì¢… ë‹¨ì¼ ëª¨ë¸ @ìš´ì˜ ì„ê³„ê°’) ===\n",
    "\n",
    "import numpy as np  # <<< ì¶”ê°€: bar ì°¨íŠ¸ìš©\n",
    "\n",
    "LABELS = [0, 1]\n",
    "LABEL_NAMES = {0: \"0 = Drop\", 1: \"1 = Complete\"}\n",
    "\n",
    "def _prob_single_for_k(K, X_va):\n",
    "    \"\"\"Kì— ëŒ€í•´ 'ìµœì¢… ë‹¨ì¼ ëª¨ë¸'ì˜ í™•ë¥  ë²¡í„°ì™€ í‘œì‹œìš© ë¬¸ìì—´ì„ ë°˜í™˜.\"\"\"\n",
    "    model = trained_models_by_k[K]\n",
    "    # í‘œì‹œìš© ëª¨ë¸ëª…/ì‹œë“œ\n",
    "    est = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps else model\n",
    "    model_name = est.__class__.__name__\n",
    "    sel_idx = eval_by_k[K].get(\"selected_model_index\", None)\n",
    "    seeds = eval_by_k[K].get(\"seeds\", None)\n",
    "    if sel_idx is not None and seeds and 0 <= sel_idx < len(seeds):\n",
    "        model_str = f\"{model_name} (seed={seeds[sel_idx]})\"\n",
    "    else:\n",
    "        model_str = model_name\n",
    "\n",
    "    # í™•ë¥  ì˜ˆì¸¡\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X_va)[:, 1]\n",
    "    else:\n",
    "        # ì˜ˆì™¸ì  ìƒí™© ëŒ€ë¹„: decision_functionë§Œ ìˆì„ ê²½ìš° ì‹œê·¸ëª¨ì´ë“œë¡œ ë³€í™˜\n",
    "        z = model.decision_function(X_va)\n",
    "        p = 1.0 / (1.0 + np.exp(-z))\n",
    "    return p, model_str\n",
    "\n",
    "# <<< ì¶”ê°€: Kë³„ ì„±ëŠ¥ ìˆ˜ì§‘ìš© ì»¨í…Œì´ë„ˆ\n",
    "ks, accs, f1s = [], [], []\n",
    "\n",
    "for K in K_LIST:\n",
    "    # ê²€ì¦ ì„¸íŠ¸ / ì„ê³„ê°’\n",
    "    X_tr, X_va, y_tr, y_va, _ = splits_by_k[K]\n",
    "    t_oper = T_OPER_BY_K.get(K, eval_by_k[K][\"Oper.t\"])\n",
    "    t_bal  = eval_by_k[K].get(\"Balanced.t\", None)\n",
    "    t_label = (\n",
    "        f\"@t={t_oper:.3f}\"\n",
    "        if (t_bal is None or abs(t_oper - t_bal) < 1e-12)\n",
    "        else f\"@t={t_oper:.3f} (Balanced.t={t_bal:.3f})\"\n",
    "    )\n",
    "\n",
    "    # ë‹¨ì¼ ìµœì¢… ëª¨ë¸ í™•ë¥  ë° ì˜ˆì¸¡\n",
    "    p, model_str = _prob_single_for_k(K, X_va)\n",
    "    yhat = (p >= t_oper).astype(int)\n",
    "\n",
    "    # í˜¼ë™í–‰ë ¬ ë° ì§€í‘œ\n",
    "    cm = confusion_matrix(y_va, yhat, labels=LABELS)\n",
    "    row_sums = cm.sum(axis=1, keepdims=True).astype(float)\n",
    "    cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums != 0)\n",
    "\n",
    "    acc = accuracy_score(y_va, yhat)\n",
    "    macro_f1 = f1_score(y_va, yhat, average=\"macro\")\n",
    "    n_va = len(y_va)\n",
    "\n",
    "    # <<< ì¶”ê°€: ìˆ˜ì§‘\n",
    "    ks.append(K)\n",
    "    accs.append(acc)\n",
    "    f1s.append(macro_f1)\n",
    "\n",
    "    # í”Œë¡¯(í˜¼ë™í–‰ë ¬)\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 5.2))\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            correct = (i == j)\n",
    "            ax.add_patch(plt.Rectangle(\n",
    "                (j - 0.5, i - 0.5), 1, 1,\n",
    "                facecolor=(\"green\" if correct else \"red\"),\n",
    "                edgecolor=\"black\", alpha=0.30\n",
    "            ))\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{cm[i, j]}\\n({cm_norm[i, j]:.2f})\",\n",
    "                ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "    ax.set_xticks([0, 1]); ax.set_xticklabels([LABEL_NAMES[0], LABEL_NAMES[1]], rotation=15, ha=\"right\")\n",
    "    ax.set_yticks([0, 1]); ax.set_yticklabels([LABEL_NAMES[0], LABEL_NAMES[1]])\n",
    "    ax.set_xlim(-0.5, 1.5); ax.set_ylim(1.5, -0.5)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(f\"K={K} | {t_label} â€” Acc={acc:.4f}, Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=\"green\", edgecolor=\"black\", label=\"Correct (TP/TN)\"),\n",
    "        Patch(facecolor=\"red\", edgecolor=\"black\", label=\"Incorrect (FP/FN)\")\n",
    "    ]\n",
    "    ax.legend(handles=legend_elems, loc=\"upper right\", frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === (ì¶”ê°€) Kë³„ ACC vs Macro-F1 ë§‰ëŒ€ê·¸ë˜í”„ ===\n",
    "# Xì¶•ì€ K (ì˜ˆ: 3,5,7). ACC=íŒŒë‘, F1=ì´ˆë¡.\n",
    "# í•„ìš” ì‹œ K ì •ë ¬\n",
    "order = np.argsort(ks)\n",
    "ks_sorted  = [ks[i]  for i in order]\n",
    "accs_sorted = [accs[i] for i in order]\n",
    "f1s_sorted  = [f1s[i] for i in order]\n",
    "\n",
    "x = np.arange(len(ks_sorted))\n",
    "width = 0.36\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.6))\n",
    "bars_acc = ax.bar(x - width/2, accs_sorted, width, label=\"ACC\", color=\"blue\")     # íŒŒë€ ë§‰ëŒ€\n",
    "bars_f1  = ax.bar(x + width/2, f1s_sorted, width, label=\"Macro-F1\", color=\"green\")# ì´ˆë¡ ë§‰ëŒ€\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ks_sorted)  # ì˜ˆ: 3,5,7\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Score (ACC & Macro-F1)\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# ê°’ ë¼ë²¨ í‘œì‹œ\n",
    "ax.bar_label(bars_acc, fmt=\"%.4f\", padding=3)\n",
    "ax.bar_label(bars_f1,  fmt=\"%.4f\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de0a54-f10b-4800-9e29-d6557307c6b3",
   "metadata": {},
   "source": [
    "## 12) ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "- ê²€ì¦ëœ ëª¨ë¸ì— ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c6436-e782-47fb-aba9-829b80775f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference Cell (K=7): paste TSV â†’ load model â†’ align features â†’ predict ===\n",
    "import os, json, io, numpy as np, pandas as pd\n",
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ---- Config ----\n",
    "K = 7\n",
    "MODEL_DIR = \"models/sessionDrop\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f\"sessionDrop_model.k{K}.joblib\")\n",
    "META_PATH  = os.path.join(MODEL_DIR, f\"sessionDrop_model.k{K}.meta.json\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "def _sigmoid(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _load_threshold(default=0.5):\n",
    "    # 1) meta.json â†’ threshold, 2) eval_by_k[K][\"Oper.t\"] (ë©”ëª¨ë¦¬ì— ìˆë‹¤ë©´), 3) default\n",
    "    try:\n",
    "        with open(META_PATH, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        return float(meta.get(\"threshold\", default))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return float(eval_by_k[K][\"Oper.t\"])  # Notebook ë©”ëª¨ë¦¬ì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _load_feature_list():\n",
    "    # meta.jsonì˜ í•™ìŠµ ì‹œ feature ìˆœì„œë¥¼ ë˜ì‚´ë¦¼(ë¹…ê·¸ë¨ í¬í•¨)\n",
    "    try:\n",
    "        with open(META_PATH, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        feats = meta.get(\"features\", None)\n",
    "        if isinstance(feats, list) and len(feats) > 0:\n",
    "            return [str(c) for c in feats]\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return list(eval_by_k[K][\"features\"])  # ë©”ëª¨ë¦¬ ë³´ì¡°\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _ensure_numeric(df):\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == \"O\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out.fillna(0.0)\n",
    "\n",
    "# ---- Paste your TSV here (session_id \\t <features...> [\\t label]) ----\n",
    "RAW_TSV = \"\"\"0012eec7-d6c0-4eff-b4c4-727e03e46b0c\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t2.0\t1.0\t0.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t0.9556998911095345\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0\n",
    "002071d2-83a3-4cc1-ac33-b6a98353cd13\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.4750763110496952\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "a6d0ab18-6102-42d5-ad36-5c02efec6caf\t7.0\t60.0\t7.0\t8.571428571428571\t20.99562636671296\t41.99999999999996\t7.0\t1.0\t0.0\t1.0\t0.14285714285714285\t0.0\t0.0\t0.0\t2.0\t3.0\t0.0\t60.0\t-1.0\t1.0\t0.0\t0.0\t1.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t4.0\t1.1537419426970903\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0\n",
    "010f7eb5-8c35-4e69-bd9d-c0dacef95068\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t16.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t1.0042424730510766\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "c8d03998-8f4e-4c89-85c9-da22f5bc0e64\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t2.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.5498260458732018\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0\n",
    "c5b8c2db-e7c8-436e-96fd-da273dd148b7\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t2.0\t0.41011631828640904\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0\n",
    "51298911-140e-4357-8b66-f9acf0217700\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t4.0\t0.5\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t6.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t1.0789922078745835\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0\n",
    "504045be-f550-4bd7-bae4-0b736fc61dfb\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t16.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t5.0\t1.5498260458732018\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "5032cacd-9fc5-4bc5-90b1-9ac4e6bd3416\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t6.0\t1.7478680974607577\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t1\n",
    "fffe1852-ab15-4b57-8aa3-cd0fb92c3ccc\t7.0\t0.0\t7000000000.0\t0.0\t0.0\t0.0\t7.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t17.0\t0.0\t-1.0\t-1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t1.0\t0.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t4.0\t1.1537419426970903\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1\n",
    "\"\"\"\n",
    "\n",
    "# ---- Read pasted data (whitespace-separated tolerant) ----\n",
    "df_raw = pd.read_csv(io.StringIO(RAW_TSV.strip()), sep=r\"\\s+\", header=None, engine=\"python\")\n",
    "\n",
    "# ---- Load feature order & model ----\n",
    "feature_list = _load_feature_list()\n",
    "if feature_list is None:\n",
    "    raise RuntimeError(\"í•™ìŠµ ì‹œ feature ëª©ë¡(features)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Cell 10ì—ì„œ ìƒì„±ëœ meta JSONì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "expected_no_label = 1 + len(feature_list)     # session_id + features\n",
    "expected_with_label = expected_no_label + 1   # + label\n",
    "\n",
    "if df_raw.shape[1] not in (expected_no_label, expected_with_label):\n",
    "    raise RuntimeError(\n",
    "        f\"ì—´ ìˆ˜ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. got={df_raw.shape[1]} expected={expected_no_label} or {expected_with_label} \"\n",
    "        f\"(features={len(feature_list)})\"\n",
    "    )\n",
    "\n",
    "columns = [\"session_id\"] + feature_list\n",
    "has_label = (df_raw.shape[1] == expected_with_label)\n",
    "if has_label:\n",
    "    columns = columns + [\"label\"]\n",
    "df_raw.columns = columns\n",
    "\n",
    "# keep reference label if exists; drop from X\n",
    "y_ref = None\n",
    "if has_label:\n",
    "    y_ref = pd.to_numeric(df_raw[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "X = _ensure_numeric(df_raw[feature_list])\n",
    "\n",
    "# load model\n",
    "loaded_from = None\n",
    "try:\n",
    "    model = load(MODEL_PATH)\n",
    "    loaded_from = f\"[file] {MODEL_PATH}\"\n",
    "except Exception:\n",
    "    # fallback to memory (if you already trained in this kernel)\n",
    "    try:\n",
    "        model = trained_models_by_k[K]\n",
    "        loaded_from = \"[memory] trained_models_by_k\"\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{MODEL_PATH}'ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ë¨¼ì € í•™ìŠµ ì…€(Cell 8~10)ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”. err={e}\")\n",
    "\n",
    "t_oper = _load_threshold(default=0.5)\n",
    "\n",
    "print(f\"[info] Using K={K} model from {loaded_from}\")\n",
    "print(f\"[info] threshold (Oper.t) = {t_oper:.3f}\")\n",
    "print(f\"[info] samples = {len(X)}, features = {X.shape[1]}\")\n",
    "\n",
    "# ---- Predict ----\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "else:\n",
    "    z = model.decision_function(X)\n",
    "    proba = _sigmoid(z)\n",
    "pred = (proba >= t_oper).astype(int)\n",
    "\n",
    "# ---- Display with rows + clear separators ----\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "if y_ref is not None:\n",
    "    truth_vals = y_ref.values\n",
    "    mark = np.where(pred == truth_vals, \"âœ“\", \"âœ—\")\n",
    "else:\n",
    "    truth_vals = [\"-\"] * len(pred)\n",
    "    mark = [\"\"] * len(pred)\n",
    "\n",
    "headers = [\"case\", \"truth\", \"pred\", \"âœ“/âœ—\", \"note\"]\n",
    "rows = [\n",
    "    [df_raw.get(\"session_id\", pd.Series(range(len(pred)))).astype(str).iloc[i],\n",
    "     int(truth_vals[i]) if truth_vals[i] in (0,1) else truth_vals[i],\n",
    "     int(pred[i]),\n",
    "     mark[i],\n",
    "     \"\"]\n",
    "    for i in range(len(pred))\n",
    "]\n",
    "\n",
    "df_view = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "display(HTML(\"<hr style='margin:6px 0;'>\"))\n",
    "display(HTML(\"<h4 style='margin:4px 0;'>Inference Result (K=7)</h4>\"))\n",
    "display(df_view)\n",
    "\n",
    "if y_ref is not None:\n",
    "    acc = accuracy_score(truth_vals, pred)\n",
    "    f1m = f1_score(truth_vals, pred, average=\"macro\")\n",
    "    display(HTML(f\"<div><b>[Done]</b> ì •í™•ë„(ìƒ˜í”Œ {len(truth_vals)}ê°œ): {acc:.3f}\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
